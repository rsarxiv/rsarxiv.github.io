<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PaperWeekly</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://rsarxiv.github.io/"/>
  <updated>2017-01-11T20:26:11.000Z</updated>
  <id>http://rsarxiv.github.io/</id>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PaperWeekly ç¬¬äºŒåä¸€æœŸ</title>
    <link href="http://rsarxiv.github.io/2017/01/11/PaperWeekly-%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B8%80%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2017/01/11/PaperWeekly-ç¬¬äºŒåä¸€æœŸ/</id>
    <published>2017-01-11T20:00:23.000Z</published>
    <updated>2017-01-11T20:26:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•"><a href="#å¼•" class="headerlink" title="å¼•"></a>å¼•</h1><p>å¤šä¿¡æ¯èåˆæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¶‹åŠ¿ï¼Œå°¤å…¶æ˜¯å¯¹äºè®­ç»ƒæ•°æ®ç¼ºä¹çš„ä»»åŠ¡æ¥è¯´ï¼Œå¦‚ä½•èå…¥å…¶ä»–ç›¸å…³ä¿¡æ¯æ¥æé«˜æœ¬ä»»åŠ¡çš„å‡†ç¡®ç‡æ˜¯ä¸€ä¸ªéå¸¸å€¼å¾—ç ”ç©¶çš„é—®é¢˜ã€‚æœºå™¨ç¿»è¯‘æ˜¯ä¸€ä¸ªçƒ­é—¨çš„ç ”ç©¶é¢†åŸŸï¼Œéšç€è®­ç»ƒæ•°æ®è§„æ¨¡åœ°å¢åŠ ï¼Œå„ç§NNæ¨¡å‹çš„æ•ˆæœä¹Ÿå–å¾—äº†çªç ´çš„è¿›å±•ï¼Œgoogleå’Œç™¾åº¦å‡å·²éƒ¨ç½²ä¸Šçº¿NMTç³»ç»Ÿï¼›èåˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€æ–‡æœ¬ç­‰å„ç§æ¨¡æ€æ•°æ®çš„å¤šæ¨¡æ€ç ”ç©¶ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸çƒ­é—¨çš„ç ”ç©¶æ–¹å‘ï¼Œæœ¬æœŸPaperWeeklyå°†ä¸ºå¤§å®¶å¸¦æ¥NMTå’Œå¤šæ¨¡æ€äº¤å‰ç ”ç©¶çš„paperè§£è¯»ï¼Œå…±3ç¯‡paperï¼š</p>
<p>1ã€Attention-based Multimodal Neural Machine Translation, 2016<br>2ã€Multimodal Attention for Neural Machine Translation, 2016<br>3ã€Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot, 2016</p>
<h1 id="Attention-based-Multimodal-Neural-Machine-Translation"><a href="#Attention-based-Multimodal-Neural-Machine-Translation" class="headerlink" title="Attention-based Multimodal Neural Machine Translation"></a><a href="https://www.aclweb.org/anthology/W/W16/W16-2360.pdf" target="_blank" rel="external">Attention-based Multimodal Neural Machine Translation</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean Oh, Chris Dyer</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>CMU</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Visual Features, Attention, Multimodal NMT</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ACL 2016</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>å¤šæ¨¡æ€ç¥ç»æœºå™¨ç¿»è¯‘ï¼Œåœ¨ä¼ ç»Ÿçš„seq2seqç¿»è¯‘æ¨¡å‹ä¸Šï¼Œåˆ©ç”¨å›¾åƒç‰¹å¾ä¿¡æ¯å¸®åŠ©æé«˜æœºå™¨ç¿»è¯‘çš„ç»“æœ</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>åœ¨WMT16çš„å¤šæ¨¡æ€ç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘æ–°ä»»åŠ¡ä¸Šçš„å·¥ä½œã€‚<br>æå‡ºäº†3ç§å¦‚ä½•å°†visual featureåŠ å…¥åˆ°seq2seqç½‘ç»œä¸­çš„encoderï¼Œä»è€Œä½¿å¾—decoderæ›´å¥½çš„attentionåˆ°ä¸å›¾åƒï¼Œè¯­ä¹‰ç›¸å…³éƒ¨åˆ†çš„æ¨¡å‹ï¼š global visual featureï¼Œ regional visual featureï¼Œparalle threads.</p>
<p><img src="media/global_visual.png" alt="global_visua"></p>
<p>global visualï¼š ç›´æ¥å°†VGGä¸­çš„fc7æŠ½å‡ºçš„featureåŠ å…¥åˆ°encoderçš„first step(head)æˆ–è€…æ˜¯last step(tail)</p>
<p><img src="media/region_visual.png" alt="region_visua"></p>
<p>regional visualï¼š å…ˆç”¨R-CNNæŠ½å‡ºregion boxçš„ä¿¡æ¯ï¼Œå†ç”¨VGGå¾—åˆ°fc7çš„ç‰¹å¾ï¼Œå°†top4å¯¹åº”çš„region featureï¼Œä»¥åŠglobal visual featureåˆ†åˆ«ä½œä¸ºæ¯ä¸€ä¸ªstepè¾“å…¥åˆ°encoderä¸­</p>
<p><img src="media/parallel_threads.png" alt="parallel_threads"></p>
<p>parallel threads: ä¸regional visualç›¸å¯¹åº”çš„æ˜¯ï¼Œæ¯ä¸ªthreadåªåˆ©ç”¨ä¸€ä¸ªregion boxçš„featureï¼Œå’Œglobal visualä¸€æ ·çš„ç½‘ç»œï¼Œå°†top 4å¯¹åº”çš„4 threadså’Œgloabl threadä¸€èµ·åšaverage poolingï¼Œæ¯ä¸ªtheradçš„å‚æ•°å…±äº«; attentionåˆ™å¯¹åº”æ‰€æœ‰threadsä¸­çš„æ‰€æœ‰hidden states</p>
<p>åŒæ—¶æœ¬æ–‡è¿˜æå‡ºäº†ä¸‰ç§rescoring translationçš„ç»“æœçš„æ–¹æ³•ï¼Œ ç”¨ 1ï¼‰language model 2ï¼‰bilingual autoencoder 3ï¼‰bilingual dictionaryåˆ†åˆ«æ¥æŒ‘é€‰translationçš„å¥å­ï¼Œå‘ç°bilingual dictionaryæ¥åˆ é€‰ç¿»è¯‘çš„å¥å­æ•ˆæœæœ€å¥½</p>
<h2 id="èµ„æº"><a href="#èµ„æº" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>æ•°æ®é›†ï¼š WMT2016 (En-Ge)<br>å›¾åƒç‰¹å¾æå–ï¼š VGGï¼Œ R-CNN</p>
<h2 id="å®éªŒç»“æœ"><a href="#å®éªŒç»“æœ" class="headerlink" title="å®éªŒç»“æœ"></a>å®éªŒç»“æœ</h2><p>åœ¨En-Geçš„ç»“æœå¦‚å›¾ï¼š<br><img src="media/en-ge.png" alt="en-ge"></p>
<h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>NMTï¼š Kalchbrenner and Blunsom 2013<br>Attention NMTï¼š Bahdanau 2014<br>Joint Space Learningï¼š Zhang 2014ï¼ŒSu 2015ï¼ŒKiros 2014<br>å¤šæ¨¡æ€ä¸Šç›¸å…³å·¥ä½œç›®å‰å¹¶æ²¡æœ‰å¾ˆå¤šï¼Œå€¼å¾—å¿«é€Ÿå…¥æ‰‹</p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å›¾åƒå’Œæ–‡æœ¬ç»“åˆçš„ç¥ç»ç½‘ç»œç¿»è¯‘æ¨¡å‹ï¼Œéå¸¸è‡ªç„¶çš„å°†å›¾åƒç‰¹å¾åŠ å…¥åˆ°seq2seqæ¨¡å‹çš„encoderéƒ¨åˆ†ï¼Œä½¿decoderä¸ä»…èƒ½å¤Ÿattentionåœ¨æ–‡æœ¬ä¸Šï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿfocusåˆ°å›¾åƒä¸Š(globalæˆ–è€…region)ï¼›å¹¶ä¸”æ¨¡å‹çš„è®¾è®¡æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰åŠ å…¥å¤ªå¤šå¤æ‚çš„æ¨¡å—ã€‚<br>ä¸è¿‡åªæ˜¯ç®€å•çš„å°†å›¾åƒçš„ç‰¹å¾ä½œä¸ºseqä¸­çš„ä¸€ä¸ªstepï¼Œå¹¶æ²¡æœ‰è€ƒè™‘æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ç›¸å…³å…³ç³»ï¼Œå¦‚joint spaceï¼Œç›¸ä¿¡åŠ å…¥joint learingä¼šæœ‰æå‡ã€‚</p>
<h2 id="å®Œæˆäººä¿¡æ¯"><a href="#å®Œæˆäººä¿¡æ¯" class="headerlink" title="å®Œæˆäººä¿¡æ¯"></a>å®Œæˆäººä¿¡æ¯</h2><p>Lijun Wu from SYSU.</p>
<h1 id="Multimodal-Attention-for-Neural-Machine-Translation"><a href="#Multimodal-Attention-for-Neural-Machine-Translation" class="headerlink" title="Multimodal Attention for Neural Machine Translation"></a><a href="https://arxiv.org/abs/1609.03976" target="_blank" rel="external">Multimodal Attention for Neural Machine Translation</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Ozan Caglayan, LoÃ¯c Barrault, Fethi Bougares</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>University of Le Mans, Galatasaray University</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>NMT, Attention</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv 2016.09</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ç»™å®šå›¾ç‰‡å’Œæºè¯­è¨€æè¿°çš„æƒ…å†µä¸‹ï¼ŒåŸºäºattentionæœºåˆ¶,ç”Ÿæˆç›®æ ‡è¯­è¨€çš„å›¾ç‰‡æè¿°ã€‚</p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ¨¡å‹æœ‰ä¸¤ä¸ªencoderï¼Œä¸€ä¸ªæ˜¯textual encoder,æ˜¯ä¸€ä¸ªåŒå‘GRUï¼Œç”¨äºè·å–æºè¯­è¨€æ–‡æœ¬çš„å‘é‡è¡¨ç¤º$A^{txt} = {a^{txt}_1,a^{txt}_2,â€¦}$ï¼Œå¦å¤–ä¸€ä¸ªæ˜¯visual encoder,ä½¿ç”¨çš„æ˜¯ç°æˆç”±ImageNetæ•°æ®é›†è®­å¥½çš„ResNet-50ç½‘ç»œï¼Œç”¨äºè·å–å›¾ç‰‡çš„å‘é‡è¡¨ç¤ºã€‚$A^{im} = {a^{im}_1,a^{im}_2,â€¦}$. Decoderéƒ¨åˆ†ï¼Œæ˜¯ä¸¤å±‚çš„stakced GRU,å…ˆç”¨attentionæ–¹å¼ï¼Œåˆ†åˆ«è·å–æ–‡æœ¬éƒ¨åˆ†å’Œå›¾åƒéƒ¨åˆ†çš„contextå‘é‡$c^{txt}$å’Œ$c^{im}$,ç„¶åå°†ä¸¤ä¸ªå‘é‡concatåœ¨ä¸€èµ·ï¼Œä½œä¸ºæ–°çš„context å‘é‡$c$ã€‚<br>å¦‚å›¾ï¼š</p>
<p><img src="media/mul_attention.jpg" alt="mul_attention"></p>
<p>è¿™æ ·decoderéƒ¨åˆ†çš„è§£ç ç¿»è¯‘çš„æ—¶å€™ï¼Œä¸ä»…å¯ä»¥è€ƒè™‘åˆ°æºè¯­è¨€çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘åˆ°åŸå§‹å›¾ç‰‡çš„ä¿¡æ¯ã€‚</p>
<h2 id="èµ„æº-1"><a href="#èµ„æº-1" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p><a href="https://github.com/elliottd/GroundedTranslation" target="_blank" rel="external">IAPRTC-12 dataset for English and German</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-1"><a href="#ç›¸å…³å·¥ä½œ-1" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>2014å¹´Bahdanauçš„Neural Machine Translation by Jointly Learning to Align and Translateï¼Œä½¿NMTè¶…è¿‡äº†ä¼ ç»Ÿçš„PBMTï¼Œåæ¥çš„NMTè®ºæ–‡åŸºæœ¬éƒ½æ˜¯åœ¨è¿™ä¸ªæ–‡ç« åŸºç¡€ä¸Šè¿›è¡Œçš„æ”¹è¿›ã€‚<br>2015å¹´Elliottçš„å·¥ä½œMulti-language image description with neural sequence models. ä¹Ÿæ˜¯åœ¨ç»™å®šæºè¯­è¨€å’Œå›¾ç‰‡çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆç›®æ ‡è¯­è¨€ã€‚ä¸è¿‡å¹¶æ²¡æœ‰ä½¿ç”¨attentionæœºåˆ¶ã€‚</p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¯¥æ–‡ç« çš„åˆ›æ–°ä¹‹å¤„ï¼Œåœ¨äºå¯¹å›¾ç‰‡æè¿°æ–‡å­—è¿›è¡Œç¿»è¯‘çš„æ—¶å€™ï¼Œè€ƒè™‘åˆ°äº†å›¾ç‰‡æœ¬èº«çš„ç‰¹å¾ä¿¡æ¯å¹¶å¼•å…¥attentionæœºåˆ¶ã€‚åœ¨æºè¯­è¨€æ–‡æœ¬ç”Ÿæˆå‡ºé”™çš„æƒ…å†µä¸‹ï¼Œå› ä¸ºæœ‰å›¾ç‰‡ä¿¡æ¯å‚è€ƒï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œå¯ä»¥å‡è½»è¿™ç§é”™è¯¯å¸¦æ¥çš„å½±å“ã€‚ä¸è¿‡æ–‡ç« å¹¶æ²¡æœ‰åˆ©ç”¨å¤–éƒ¨è‹±å¾·å¹³è¡Œè¯­æ–™ï¼Œè¿™å¯ä»¥è€ƒè™‘ä½œä¸ºåé¢çš„æ”¹è¿›æ–¹å‘ã€‚</p>
<h2 id="å®Œæˆäººä¿¡æ¯-1"><a href="#å®Œæˆäººä¿¡æ¯-1" class="headerlink" title="å®Œæˆäººä¿¡æ¯"></a>å®Œæˆäººä¿¡æ¯</h2><p>xiaose@mail.ustc.edu.cn<br>ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</p>
<h1 id="Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot"><a href="#Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot" class="headerlink" title="Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot"></a><a href="https://arxiv.org/pdf/1611.04503.pdf" target="_blank" rel="external">Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Hideki Nakayamaï¼ŒNoriki Nishida</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>The University of Tokyo</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>pivot, multimodal, NMT</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016.11</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>åœ¨æ²¡æœ‰å¹³è¡Œè¯­æ–™çš„æƒ…å†µä¸‹ï¼Œç”¨imageå½“ä½œpivotæ¥å®ç°æœºå™¨ç¿»è¯‘</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ•´ä½“ä¸Šè®²ï¼Œæ¨¡å‹åˆ†æˆä¸¤éƒ¨åˆ†ã€‚ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¤šæ¨¡æ€embeddingï¼Œé‡‡ç”¨pairwise ranking lossæ¥å®šä¹‰æŸå¤±å‡½æ•°ï¼›ç¬¬äºŒéƒ¨åˆ†æ˜¯ç”¨RNNæ¥å®ç°çš„decoder,è·Ÿimage captioné‡Œé¢çš„decoderç±»ä¼¼ã€‚å¯¹è¿™ä¸ªé—®é¢˜æ¥è¯´ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®åŒ…æ‹¬$i^{s}$ï¼šæºç«¯çš„å›¾ç‰‡ï¼Œ$d^{s}$ï¼šæºç«¯å›¾ç‰‡å¯¹åº”çš„å¥å­æè¿°ï¼›$i^{t}$ï¼šç›®æ ‡ç«¯çš„å›¾ç‰‡ï¼Œ$d^{t<br>}$ï¼šç›®æ ‡ç«¯å›¾ç‰‡å¯¹åº”çš„å¥å­æè¿°ï¼Œå’Œæºç«¯ç”¨çš„ä¸ä¸€æ ·çš„è¯­è¨€ã€‚æ–‡ä¸­æå‡ºäº†2ä¸ªæ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š&lt;div<br><img src="media/21-1-1.png" alt="21-1"></p>
<p>æ¨¡å‹1çš„å¤šæ¨¡æ€ç«¯åŒ…æ‹¬äº†å›¾ç‰‡çš„encoderå’Œæºå¥å­çš„encoderã€‚å›¾ç‰‡encoderå¯ä»¥å¯¹æºå›¾ç‰‡å’Œç›®æ ‡å›¾ç‰‡é€šç”¨ã€‚å¤šæ¨¡æ€ç«¯ç”¨$i^{s}$,$d^{s}$è¿›è¡Œè®­ç»ƒï¼ŒæŸå¤±å‡½æ•°ä¸ºï¼š</p>
<p><img src="media/21-2.png" alt="21-2"></p>
<p>$E^{v}$è¡¨ç¤ºå›¾ç‰‡çš„encoder(æ¯”å¦‚ç”¨VGG-16æå–å›¾ç‰‡çš„feature), $E^{s}$è¡¨ç¤ºæºå¥å­çš„encoder(æ¯”å¦‚ç”¨RNN)ï¼Œ$d^{s}_{ng}$è¡¨ç¤ºå’Œæºç«¯å›¾ç‰‡ä¸ç›¸å…³çš„æè¿°ã€‚Decoderç«¯ç”¨$i^{t}$,$d^{t}$è¿›è¡Œè®­ç»ƒï¼ŒæŸå¤±å‡½æ•°ä¸ºæ ‡å‡†çš„ cross-entropy lossï¼ˆç§°ä½œå›¾ç‰‡æŸå¤±):</p>
<p><img src="media/21-3.png" alt="21-3"></p>
<p>æ¨¡å‹2æ¯”æ¨¡å‹1æ›´å¤æ‚ä¸€ç‚¹ã€‚åœ¨æºç«¯å¢åŠ äº†ä¸€ä¸ªç›®æ ‡å¥å­æè¿°çš„encoderã€‚å› æ­¤ï¼Œåœ¨å¤šæ¨¡æ€embeddingçš„å­¦ä¹ ä¸­ï¼ŒæŸå¤±å‡½æ•°å¢åŠ äº†ç›®æ ‡å›¾ç‰‡å’Œç›®æ ‡å›¾ç‰‡æè¿°çš„pairwise ranking loss.</p>
<p><img src="media/21-4.png" alt="21-4"></p>
<p>åœ¨decoderçš„å­¦ä¹ ä¸­ï¼Œæ¨¡å‹2é™¤äº†å‰é¢çš„å…¬å¼2å®šä¹‰çš„å›¾ç‰‡æŸå¤±å¤–ï¼Œè¿˜å¢åŠ äº†ç›®æ ‡æè¿°çš„reconstruction lossï¼Œå³ä»å¤šæ¨¡æ€ç«¯è¾“å…¥ç›®æ ‡æè¿°ï¼Œå¸Œæœ›é€šè¿‡embeddingå’Œdecoderé‡å»ºè¿™ä¸ªç›®æ ‡æè¿°ã€‚<br><img src="media/21-5.png" alt="21-5"></p>
<h2 id="èµ„æº-2"><a href="#èµ„æº-2" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>ä¸¤ä¸ªMultilingual image-descriptionçš„æ•°æ®é›†ï¼šIAPR-TC12ï¼ˆåŒ…å«2ä¸‡å›¾ç‰‡ä»¥åŠè‹±è¯­å’Œå¾·è¯­çš„æè¿°ï¼‰å’Œ Multi30Kï¼ˆåŒ…å«3ä¸‡å›¾ç‰‡ä»¥åŠè‹±è¯­å’Œå¾·è¯­çš„æè¿°)</p>
<h2 id="ç›¸å…³å·¥ä½œ-2"><a href="#ç›¸å…³å·¥ä½œ-2" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>å¯¹äºæ²¡æœ‰å¹³è¡Œè¯­æ–™çš„æœºå™¨ç¿»è¯‘ï¼Œå¤šæ•°æ–‡ç« æ˜¯ç”¨æŸç§å¸¸è§è¯­è¨€ä½œä¸ºpivotï¼Œæ¯”å¦‚â€œNeural Machine Translation with Pivot Languagesâ€, ç”¨è‹±è¯­ä½œä¸ºè¥¿ç­ç‰™è¯­æ³•è¯­ä»¥åŠå¾·è¯­æ³•è¯­ä¹‹é—´çš„pivotã€‚ç¼ºç‚¹æ˜¯ç¿»è¯‘çš„æ—¶å€™è¿˜æ˜¯è¦ç»è¿‡pivoté‚£ä¸€æ­¥ã€‚ å¦å¤–ï¼Œè¿˜è¦ä¸€äº›å·¥ä½œæ˜¯ç”¨ä¸€ä¸ªæ¨¡å‹å®ç°many to manyçš„ç¿»è¯‘ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ²¡æœ‰å¹³è¡Œè¯­æ–™çš„è¯­è¨€å¯¹ä¹Ÿèƒ½ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œç¿»è¯‘ã€‚ä¸éœ€è¦ç»è¿‡pivoté‚£ä¸ªä¸­é—´å±‚ï¼Œä½†æ˜¯æ•ˆæœä¸€èˆ¬ä¼šå·®ä¸€ç‚¹ã€‚æ¯”å¦‚â€œGoogleâ€™s Multilingual Neural Machine Translation Systemâ€è¿™ç¯‡æ–‡ç« ã€‚</p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¿™ç¯‡æ–‡ç« çš„æ€è·¯å¾ˆæ–°é¢–ï¼Œè€ƒè™‘ç”¨å›¾ç‰‡æ¥ä½œä¸ºpivotï¼Œå®ç°æ²¡æœ‰å¹³è¡Œè¯­æ–™çš„è¯­è¨€å¯¹ä¹‹é—´çš„ç¿»è¯‘ã€‚è®­ç»ƒå®Œæˆåå¯ä»¥ç›´æ¥ä»æºè¯­è¨€åˆ°ç›®æ ‡è¯­è¨€è¿›è¡Œç¿»è¯‘ï¼Œä¸éœ€è¦ç»è¿‡å›¾ç‰‡ã€‚ä½†æ˜¯æ­£å¦‚æ–‡ä¸­æåˆ°çš„ï¼Œè¿™ç§æ–¹æ³•è·Ÿæœ‰è¯­æ–™è®­ç»ƒå‡ºæ¥çš„ç¿»è¯‘æ•ˆæœæ¯”èµ·æ¥è¿˜æ˜¯å·®å¾ˆå¤šï¼Œå¹¶ä¸”ç¿»è¯‘çš„å¥å­éƒ½æ¯”è¾ƒçŸ­ã€‚å¦å¤–ï¼Œå¯¹ä¸€äº›å›¾ç‰‡éš¾ä»¥è¡¨è¾¾çš„ä¿¡æ¯å¾ˆéš¾é€šè¿‡è¿™ç§æ–¹å¼å­¦åˆ°ã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>äº¤å‰é¢†åŸŸçš„ç ”ç©¶æ€»æ˜¯ä¼šå¸¦ç»™å¤§å®¶æƒŠå–œï¼Œäº¤å‰é¢†åŸŸçš„äº¤å‰é¢†åŸŸæ›´æ˜¯å¦‚æ­¤ï¼Œè¿™ä¸ªé¢†åŸŸåˆšåˆšå¼€å‘ï¼Œæ¬¢è¿å„ä½æœ‰å¿—ä¹‹å£«è·³å‘ã€‚å¹¶ä¸”åœ¨2016å¹´ä¸¾åŠäº†ç¬¬ä¸€å±Šå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMultimodal Machine Translationï¼‰å’Œå¤šè¯­çœ‹å›¾è¯´è¯ï¼ˆCrosslingual Image Descriptionï¼‰æ¯”èµ›ï¼Œæ¯”èµ›ä¸»é¡µ<a href="http://www.statmt.org/wmt16/multimodal-task.html" target="_blank" rel="external">http://www.statmt.org/wmt16/multimodal-task.html</a>, æ€»ç»“æ€§çš„paper <a href="http://anthology.aclweb.org/W/W16/W16-2346.pdf" target="_blank" rel="external">http://anthology.aclweb.org/W/W16/W16-2346.pdf</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•&quot;&gt;&lt;a href=&quot;#å¼•&quot; class=&quot;headerlink&quot; title=&quot;å¼•&quot;&gt;&lt;/a&gt;å¼•&lt;/h1&gt;&lt;p&gt;å¤šä¿¡æ¯èåˆæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¶‹åŠ¿ï¼Œå°¤å…¶æ˜¯å¯¹äºè®­ç»ƒæ•°æ®ç¼ºä¹çš„ä»»åŠ¡æ¥è¯´ï¼Œå¦‚ä½•èå…¥å…¶ä»–ç›¸å…³ä¿¡æ¯æ¥æé«˜æœ¬ä»»åŠ¡çš„å‡†ç¡®ç‡æ˜¯ä¸€ä¸ªéå¸¸å€¼å¾—ç ”ç©¶çš„é—®é¢˜ã€‚æœºå™¨ç¿»è¯‘æ˜¯ä¸€ä¸ªçƒ­
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æœ¬å‘¨å€¼å¾—è¯»(2016.12.26-2017.01.06)</title>
    <link href="http://rsarxiv.github.io/2017/01/06/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-26-2017-01-06/"/>
    <id>http://rsarxiv.github.io/2017/01/06/æœ¬å‘¨å€¼å¾—è¯»-2016-12-26-2017-01-06/</id>
    <published>2017-01-07T06:06:34.000Z</published>
    <updated>2017-01-07T06:24:47.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Neural-Hawkes-Process-A-Neurally-Self-Modulating-Multivariate-Point-Process"><a href="#The-Neural-Hawkes-Process-A-Neurally-Self-Modulating-Multivariate-Point-Process" class="headerlink" title="The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process"></a><a href="http://t.cn/RMwnQmN" target="_blank" rel="external">The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process</a></h1><p>ã€æ—¶é—´åºåˆ—æ¨¡å‹ã€‘ æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è¿ç»­æ—¶é—´åºåˆ—æ¨¡å‹â€”Neural Hawkes processï¼Œç”¨æ¥å­¦ä¹ äº‹ä»¶æµä¸­ä¸åŒäº‹ä»¶ä¹‹é—´çš„å½±å“å…³ç³»ï¼Œè¿›è€Œå¯¹æœªæ¥äº‹ä»¶çš„å‘ç”Ÿæ—¶é—´å’Œç±»å‹è¿›è¡Œé¢„æµ‹ã€‚è¯¥æ¨¡å‹åœ¨ä¼ ç»ŸHawkes processçš„åŸºç¡€ä¸Šï¼Œç”¨ Recurrent Neural Network æ¥æ€»ç»“äº‹ä»¶æµçš„å†å²ä¿¡æ¯ï¼Œå¹¶åŠ¨æ€åœ°ä¼°è®¡ä¸åŒæ—¶åˆ»ä¸åŒäº‹ä»¶ä¹‹é—´å¤æ‚çš„ç›¸äº’å½±å“å…³ç³»ï¼Œè¿›è€Œå¾—å‡ºæœªæ¥äº‹ä»¶çš„å‘ç”Ÿæ—¶é—´å’Œç±»å‹çš„æ¦‚ç‡åˆ†å¸ƒã€‚æ­¤æ¨¡å‹å¯ä»¥ç”¨äºå¤šç§äº‹ä»¶æµçš„åˆ†æï¼ŒåŒ…æ‹¬åŒ»å­¦è¯Šæ–­ï¼Œæ¶ˆè´¹è€…è¡Œä¸ºï¼Œå’Œç¤¾äº¤ç½‘ç»œæ´»åŠ¨çš„é¢„æµ‹ç­‰ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä½œè€…æ¥è‡ªçº¦ç¿°éœæ™®é‡‘æ–¯å¤§å­¦NLPç»„ï¼Œä¸»é¡µåœ°å€<a href="http://www.cs.jhu.edu/~hmei/" target="_blank" rel="external">http://www.cs.jhu.edu/~hmei/</a>  æœ‰éœ€è¦è®¨è®ºçš„å¯ä»¥ç›´æ¥è”ç³»ä½œè€…æœ¬æ–‡  hmei@cs.jhu.edu</p>
<h1 id="Understanding-Image-and-Text-Simultaneously-a-Dual-Vision-Language-Machine-Comprehension-Task"><a href="#Understanding-Image-and-Text-Simultaneously-a-Dual-Vision-Language-Machine-Comprehension-Task" class="headerlink" title="Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task"></a><a href="http://t.cn/RIYdnYx" target="_blank" rel="external">Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task</a></h1><p>ã€å¤šæ¨¡æ€ã€‘image captionä»»åŠ¡çš„è‡ªåŠ¨è¯„ä»·å­˜åœ¨ä¸€å®šçš„å¼Šç«¯ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„ä»»åŠ¡ï¼Œå³ç»™å®šä¸€å¹…å›¾ï¼Œç»™å‡ºnä¸ªcaptioné€‰é¡¹ï¼Œåªæœ‰ä¸€ä¸ªæ˜¯æ­£ç¡®ç­”æ¡ˆï¼Œé€šè¿‡å‡†ç¡®ç‡æ¥è¯„ä»·ç®—æ³•çš„æ•ˆæœã€‚æ„å»ºè¿™æ ·ä¸€ä¸ªä»»åŠ¡éœ€è¦å…ˆé’ˆå¯¹æ¯ä¸€å¼ å›¾ç”Ÿæˆå¤šä¸ªéš¾åº¦è¾ƒé«˜çš„å¹²æ‰°é€‰é¡¹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€äº›æ„é€ æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨cocoæ•°æ®é›†ä¸Šç”Ÿæˆäº†æœ¬æ–‡çš„æ•°æ®é›†ï¼Œé€šè¿‡äººå·¥é€‰æ‹©è·å¾—è¯¥ä»»åŠ¡çš„å‡†ç¡®ç‡ä¸Šé™ã€‚æ•°æ®é›†å·²å¼€æ”¾ï¼Œåœ°å€ <a href="https://github.com/google/mcic-coco" target="_blank" rel="external">https://github.com/google/mcic-coco</a></p>
<h1 id="A-Joint-Speaker-Listener-Reinforcer-Model-for-Referring-Expressions"><a href="#A-Joint-Speaker-Listener-Reinforcer-Model-for-Referring-Expressions" class="headerlink" title="A Joint Speaker-Listener-Reinforcer Model for Referring Expressions"></a><a href="http://t.cn/RMhX58u" target="_blank" rel="external">A Joint Speaker-Listener-Reinforcer Model for Referring Expressions</a></h1><p>ã€å¤šæ¨¡æ€ã€‘æœ¬æ–‡ç ”ç©¶çš„é—®é¢˜éå¸¸æœ‰è¶£ï¼Œæ˜¯Referring Expressionsï¼Œç®€å•ç‚¹è¯´å°±æ˜¯ç»™ä¸€å¼ å›¾å’Œä¸€ä¸ªæè¿°ï¼Œè¦æ±‚æ‰¾åˆ°æè¿°ä¸­å¯¹åº”çš„objectï¼Œé€šå¸¸åŒ…æ‹¬ä¸¤ä¸ªä»»åŠ¡ï¼š1ã€æ ¹æ®å›¾ç‰‡å’ŒæŒ‡å®šobjectç”Ÿæˆä¸€ä¸ªæè¿°ï¼›2ã€æ ¹æ®å›¾ç‰‡å’Œæè¿°æ¥æ‰¾objectã€‚æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªè”åˆè®­ç»ƒæ¨¡å‹ï¼Œå°†ä¸¤ä¸ªä»»åŠ¡ä¸€èµ·è®­ç»ƒï¼ŒåŠ ä¸Šä¸€å±‚å¢å¼ºå­¦ä¹ æ¥æé«˜æ‰€ç”Ÿæˆæè¿°çš„å¤šæ ·æ€§ï¼Œå¾—åˆ°äº†ä¸é”™çš„ç»“æœã€‚demoå’Œdatasetåœ°å€ï¼š<a href="https://vision.cs.unc.edu/refer/" target="_blank" rel="external">https://vision.cs.unc.edu/refer/</a></p>
<h1 id="Supervised-Opinion-Aspect-Extraction-by-Exploiting-Past-Extraction-Results"><a href="#Supervised-Opinion-Aspect-Extraction-by-Exploiting-Past-Extraction-Results" class="headerlink" title="Supervised Opinion Aspect Extraction by Exploiting Past Extraction Results"></a><a href="http://t.cn/RIYgOoK" target="_blank" rel="external">Supervised Opinion Aspect Extraction by Exploiting Past Extraction Results</a></h1><p>ã€è§‚ç‚¹æŒ–æ˜ã€‘ã€è¿ç§»å­¦ä¹ ã€‘æœ¬æ–‡åšçš„å·¥ä½œæ˜¯å°†æŸä¸€äº›é¢†åŸŸä¸­å·²ç»æŠ½å–çš„éå¸¸å¥½çš„aspectè¿ç§»è‡³æ–°çš„é¢†åŸŸï¼Œæ¯”å¦‚screenåœ¨è‹¹æœæ‰‹æœºä¸­å­˜åœ¨è¿™ä¹ˆä¸€ä¸ªaspectï¼Œå…¶ä»–å“ç‰Œçš„æ‰‹æœºä¹Ÿå­˜åœ¨ï¼Œå…¶ä»–çš„ç”µå­è®¾å¤‡å¯èƒ½ä¹Ÿå­˜åœ¨ï¼Œåˆ©ç”¨å·²æœ‰çš„â€œçŸ¥è¯†â€æ¥æé«˜å‡†ç¡®ç‡ã€‚</p>
<h1 id="Language-Modeling-with-Gated-Convolutional-Networks"><a href="#Language-Modeling-with-Gated-Convolutional-Networks" class="headerlink" title="Language Modeling with Gated Convolutional Networks"></a><a href="http://t.cn/RIYkG0b" target="_blank" rel="external">Language Modeling with Gated Convolutional Networks</a></h1><p>ã€CNNè¯­è¨€æ¨¡å‹ã€‘æœ¬æ–‡çš„å·¥ä½œæ˜¯å°†CNNæ¨¡å‹å’Œä¸€ç§gateæœºåˆ¶ç»“åˆèµ·æ¥åšè¯­è¨€æ¨¡å‹ï¼ŒæŒ‘æˆ˜äº†RNNåœ¨è¿™ä¸ªé¢†åŸŸçš„éœ¸ä¸»åœ°ä½ã€‚å·¥ä½œæ¥è‡ªFacebookï¼Œä»–ä»¬å¯¹CNNæœ‰éå¸¸çš„åå¥½ã€‚ </p>
<h1 id="Understanding-Neural-Networks-through-Representation-Erasure"><a href="#Understanding-Neural-Networks-through-Representation-Erasure" class="headerlink" title="Understanding Neural Networks through Representation Erasure"></a><a href="http://t.cn/RIRFnkr" target="_blank" rel="external">Understanding Neural Networks through Representation Erasure</a></h1><p>ã€è§£é‡Šç¥ç»ç½‘ç»œã€‘å¤§å®¶éƒ½çŸ¥é“æ·±åº¦å­¦ä¹ æ•ˆæœå¥½ï¼Œä½†åŸå› ç¡®å®è§£é‡Šä¸æ¸…æ¥šã€‚æœ¬æ–‡å°è¯•ç€åšäº†ä¸€äº›è§£é‡Šæ–¹é¢çš„å·¥ä½œï¼Œé€šè¿‡â€œeraseâ€æ‰ä¸€äº›representationæ¥ç ”ç©¶ç»“æœçš„å˜åŒ–ï¼Œç”šè‡³é€šè¿‡å¢å¼ºå­¦ä¹ æ¥ç ”ç©¶æœ€å¤šâ€œeraseâ€æ‰å“ªäº›representationä»ä¸å½±å“æœ€ç»ˆçš„ç»“æœã€‚æ·±åº¦å­¦ä¹ å¦‚æœæœ‰äº†å¯è§£é‡Šæ€§ï¼Œç›¸ä¿¡åˆå°†ä¼šæ˜¯ä¸€ä¸ªæ–°çš„ç ”ç©¶æ°´å¹³äº†ã€‚ </p>
<h1 id="Shortcut-Sequence-Tagging"><a href="#Shortcut-Sequence-Tagging" class="headerlink" title="Shortcut Sequence Tagging"></a><a href="http://t.cn/RMw38iV" target="_blank" rel="external">Shortcut Sequence Tagging</a></h1><p>ã€æ–°ç½‘ç»œç»“æ„ã€‘æœ¬æ–‡é’ˆå¯¹å¤šå±‚RNNéš¾è®­ç»ƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§gateæœºåˆ¶å’Œshortcutsæœºåˆ¶æ··åˆçš„æ–¹æ³•ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒçš„ç»„åˆæ•ˆæœã€‚æ–¹æ³•åœ¨åºåˆ—æ ‡æ³¨é—®é¢˜ä¸Šè¿›è¡ŒéªŒè¯ï¼Œä»ç»“æœä¸Šæ¥çœ‹ï¼Œæé«˜çš„ä¸å¤šï¼Œä¹Ÿä»ä¾§é¢åæ˜ å‡ºä¸€ä¸ªé—®é¢˜ï¼Œç°æœ‰çš„ç½‘ç»œç»“æ„åŠ ä¸€äº›æ’åˆ—ç»„åˆæˆ–è€…å°æ”¹åŠ¨å¾ˆéš¾è§£å†³æ ¹æœ¬æ€§çš„é—®é¢˜ã€‚</p>
<h1 id="Unsupervised-neural-and-Bayesian-models-for-zero-resource-speech-processing"><a href="#Unsupervised-neural-and-Bayesian-models-for-zero-resource-speech-processing" class="headerlink" title="Unsupervised neural and Bayesian models for zero-resource speech processing"></a><a href="http://t.cn/RMLugMZ" target="_blank" rel="external">Unsupervised neural and Bayesian models for zero-resource speech processing</a></h1><p>ã€æ— ç›‘ç£ã€‘ã€è´å¶æ–¯ã€‘æœ¬æ–‡æ˜¯ä¸€ç¯‡æ¥è‡ªçˆ±ä¸å ¡å¤§å­¦çš„åšå£«è®ºæ–‡ã€‚</p>
<h1 id="Textual-Entailment-with-Structured-Attentions-and-Composition"><a href="#Textual-Entailment-with-Structured-Attentions-and-Composition" class="headerlink" title="Textual Entailment with Structured Attentions and Composition"></a><a href="http://t.cn/RM4seBe" target="_blank" rel="external">Textual Entailment with Structured Attentions and Composition</a></h1><p>ã€æ–‡æœ¬è•´å«ã€‘æœ¬æ–‡çš„è´¡çŒ®åœ¨äºå°†attentionåº”ç”¨åˆ°äº†å¥æ³•æ ‘ä¸Šï¼Œè€Œä¸æ˜¯åªå¯¹å¥å­åšattentionã€‚ </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;The-Neural-Hawkes-Process-A-Neurally-Self-Modulating-Multivariate-Point-Process&quot;&gt;&lt;a href=&quot;#The-Neural-Hawkes-Process-A-Neurally-Self
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬äºŒåæœŸ</title>
    <link href="http://rsarxiv.github.io/2017/01/06/PaperWeekly-%E7%AC%AC%E4%BA%8C%E5%8D%81%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2017/01/06/PaperWeekly-ç¬¬äºŒåæœŸ/</id>
    <published>2017-01-06T18:31:23.000Z</published>
    <updated>2017-01-06T18:49:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GANï¼ˆGenerative-Adversarial-Netsï¼‰ç ”ç©¶è¿›å±•"><a href="#GANï¼ˆGenerative-Adversarial-Netsï¼‰ç ”ç©¶è¿›å±•" class="headerlink" title="GANï¼ˆGenerative Adversarial Netsï¼‰ç ”ç©¶è¿›å±•"></a>GANï¼ˆGenerative Adversarial Netsï¼‰ç ”ç©¶è¿›å±•</h1><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>1ã€Unsupervised learning</p>
<p>é¦–å…ˆæˆ‘ä»¬ä»generative modelè¯´èµ·ã€‚generattive modelçš„ç›®çš„æ˜¯æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°å¯ä»¥æœ€å¤§çš„è¿‘ä¼¼æ•°æ®çš„çœŸå®åˆ†å¸ƒã€‚å¦‚æœæˆ‘ä»¬ç”¨ f(X; ğœƒ) æ¥è¡¨ç¤ºè¿™æ ·ä¸€ä¸ªå‡½æ•°ï¼Œé‚£ä¹ˆæ‰¾åˆ°ä¸€ä¸ªä½¿ç”Ÿæˆçš„æ•°æ®æœ€åƒçœŸå®æ•°æ®çš„ ğœƒ å°±æ˜¯ä¸€ä¸ªmaximum likelihood estimationçš„è¿‡ç¨‹ã€‚é—®é¢˜åœ¨äºï¼Œå½“æ•°æ®çš„åˆ†å¸ƒæ¯”è¾ƒå¤æ‚æ—¶ï¼Œæˆ‘ä»¬éœ€è¦çš„ f ä¹Ÿä¼šå˜å¤æ‚ã€‚ç°åœ¨æˆ‘ä»¬æœ‰æ·±åº¦ç½‘ç»œç»“æ„å¯ä»¥è¡¨è¾¾è¿™æ ·ä¸€ä¸ªå¤æ‚çš„å‡½æ•°ï¼ˆdeep generative modelï¼‰ï¼Œä½†æ˜¯è®­ç»ƒè¿‡ç¨‹æˆä¸ºäº†å…³é”®ã€‚åŸºäºsamplingçš„è®­ç»ƒè¿‡ç¨‹æ˜¾ç„¶ä¸æ˜¯å¾ˆé«˜æ•ˆçš„ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡æ¨¡å‹ä»¥ä¾¿åˆ©ç”¨backpropagationæ¥è®­ç»ƒç½‘ç»œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç›®æ ‡ã€‚å½“å‰ä¸¤ä¸ªæ¯”è¾ƒçªå‡ºçš„æ¨¡å‹å®ç°çš„å°±æ˜¯è¿™ä¸ªç›®çš„ï¼Œä¸€ä¸ªæ˜¯variational autoencoder(VAE)ï¼Œå¦ä¸€ä¸ªå°±æ˜¯è¿™ç¯‡æ–‡ç« çš„ä¸»é¢˜generative adversarial netsã€‚</p>
<p>è¿™ç¯‡æ–‡ç« ä¼šä»åŸºæœ¬çš„GANæ¨¡å‹è®²èµ·ï¼Œé‡ç‚¹è®¨è®ºæ¨¡å‹å…¬å¼èƒŒåçš„åŸç†ã€‚ä¹‹åä¼šè®¨è®ºå‡ ç¯‡GANçš„æ‰©å±•å·¥ä½œï¼Œå¸Œæœ›èƒ½å¤Ÿæ‰©å±•ä¸€ä¸‹å¤§å®¶çš„æ€è·¯ï¼Œä¹Ÿå¯ä»¥åŠ æ·±å¯¹GANæ¨¡å‹çš„ç†è§£ã€‚ä¸‹é¢çš„å…³ç³»å›¾å¤§è‡´æè¿°äº†è¿™äº›æ¨¡å‹ä¹‹é—´çš„ç»§æ‰¿å…³ç³»ã€‚æˆ‘ä»¬ä¼šæŒ‰ç…§å›¾ä¸­çš„å…³ç³»ä¸€ä¸ªä¸€ä¸ªå±•å¼€ã€‚</p>
<p><img src="media/gan-kg.png" alt="gan-kg"></p>
<p>2ã€GAN</p>
<p>é¦–å…ˆæ˜¯æœ€ç»å…¸çš„GANæ¨¡å‹ã€‚ç”±Ian Goodfellowå’ŒBengioç­‰åœ¨2014å¹´æå‡ºã€‚ä¸ºäº†ç®€æ˜æ‰¼è¦ï¼Œæˆ‘ä»¬ç›´æ¥çœ‹å›¾è¯´è¯ã€‚</p>
<p><img src="media/gan-formula.png" alt="gan-formula"></p>
<p>å›¾ä¸­ä¸ŠåŠéƒ¨åˆ†æ˜¯GANæ¨¡å‹çš„åŸºæœ¬æ¶æ„ã€‚æˆ‘ä»¬å…ˆä»ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªå™ªå£°ä¿¡å· zï¼ˆå®é™…ä¸­å¯ä»¥é‡‡ç”¨[0, 1]çš„å‡åŒ€åˆ†å¸ƒæˆ–è€…æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰ï¼Œç„¶åç»è¿‡ä¸€ä¸ªç”Ÿæˆå‡½æ•°åæ˜ å°„ä¸ºæˆ‘ä»¬æƒ³è¦çš„æ•°æ®åˆ†å¸ƒ Xg ï¼ˆz å’Œ X éƒ½æ˜¯å‘é‡ï¼‰ã€‚ç”Ÿæˆçš„æ•°æ®å’ŒçœŸå®æ•°æ®éƒ½ä¼šè¾“å…¥ä¸€ä¸ªè¯†åˆ«ç½‘ç»œ Dã€‚è¯†åˆ«ç½‘ç»œé€šè¿‡åˆ¤åˆ«è¾“å‡ºä¸€ä¸ªæ ‡é‡ï¼Œè¡¨ç¤ºæ•°æ®æ¥è‡ªçœŸå®æ•°æ®çš„<strong>æ¦‚ç‡</strong>ã€‚åœ¨å®ç°ä¸Šï¼ŒG å’Œ D éƒ½æ˜¯å¯å¾®åˆ†å‡½æ•°ï¼Œéƒ½å¯ä»¥ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå®ç°ã€‚å› æ­¤ä¸Šé¢çš„æ•´ä¸ªæ¨¡å‹çš„å‚æ•°å°±å¯ä»¥åˆ©ç”¨backpropagationæ¥è®­ç»ƒå¾—åˆ°ã€‚</p>
<p>å›¾ä¸­çš„ä¸‹åŠéƒ¨åˆ†æ˜¯æ¨¡å‹è®­ç»ƒä¸­çš„ç›®æ ‡å‡½æ•°ã€‚ä»”ç»†çœ‹å¯ä»¥å‘ç°è¿™ä¸ªå…¬å¼å¾ˆåƒcross entropyï¼Œæ³¨æ„Dæ˜¯ P(Xdata) çš„è¿‘ä¼¼ã€‚å¯¹äº D è€Œè¨€è¦å°½é‡ä½¿å…¬å¼æœ€å¤§åŒ–ï¼ˆè¯†åˆ«èƒ½åŠ›å¼ºï¼‰ï¼Œè€Œå¯¹äº G åˆæƒ³ä½¿ä¹‹æœ€å°ï¼ˆç”Ÿæˆçš„æ•°æ®æ¥è¿‘å®é™…æ•°æ®ï¼‰ã€‚æ•´ä¸ªè®­ç»ƒæ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œä½†æ˜¯åœ¨è¿­ä»£ä¸­ï¼Œå¯¹ D çš„ä¼˜åŒ–åˆæ˜¯å†…å¾ªç¯ã€‚æ‰€ä»¥æ¯æ¬¡è¿­ä»£ï¼ŒD å…ˆè®­ç»ƒ kæ¬¡ï¼ŒG è®­ç»ƒä¸€æ¬¡ã€‚</p>
<p>GANæ¨¡å‹æœ€å¤§çš„ä¼˜åŠ¿å°±æ˜¯è®­ç»ƒç®€å•ï¼Œä½†æ˜¯ä¹Ÿæœ‰ç¼ºç‚¹æ¯”å¦‚è®­ç»ƒçš„ç¨³å®šæ€§ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨è¿™ç¯‡æ–‡ç« future workéƒ¨åˆ†ï¼Œä½œè€…æå‡ºäº†5ä¸ªå¯èƒ½æ‰©å±•çš„æ–¹å‘ï¼Œè€Œç°åœ¨å›è¿‡å¤´æ¥çœ‹ï¼Œåç»­çš„å¾ˆå¤šå·¥ä½œçœŸçš„å°±æ˜¯åœ¨ç…§ç€è¿™å‡ ä¸ªæ€è·¯å¡«å‘ã€‚æ¯”å¦‚ç¬¬ä¸€ä¸ªconditional generative modelå°±æ˜¯åé¢è¦è®²çš„conditional GANçš„æ€è·¯ï¼Œè€Œæœ€åä¸€ä¸ªdeterming better distribution to sample z from during trainingåˆ™æ˜¯åé¢InfoGANçš„æ€è·¯ã€‚</p>
<p>ä¸‹é¢æ˜¯æ¥è‡ªtwitter[9] çš„ä¸€å¹…å›¾ï¼Œå¾ˆå¥½çš„æ€»ç»“äº†å„ç§è¡ç”Ÿæ¨¡å‹çš„ç»“æ„ã€‚</p>
<p><img src="media/gan.jpeg" alt="gan"></p>
<p>2.1 DCGAN </p>
<p>ä¸Šé¢Ian J. Goodfellowç­‰äººçš„æ–‡ç« æå‡ºäº†GANçš„æ¨¡å‹å’Œè®­ç»ƒæ¡†æ¶ï¼Œä½†æ˜¯æ²¡æœ‰æè¿°å…·ä½“çš„å®ç°ï¼Œè€ŒDCGAN[2] è¿™ç¯‡æ–‡ç« è®²çš„å°±æ˜¯ç”¨deep convolutional networkå®ç°ä¸€ä¸ªç”Ÿæˆå›¾ç‰‡çš„GANæ¨¡å‹ã€‚è¿™ç¯‡æ–‡ç« æ²¡æœ‰åœ¨åŸºæœ¬æ¨¡å‹ä¸Šæœ‰æ‰€æ‰©å±•ï¼Œä½†æ˜¯ä»–æè¿°äº†å¾ˆå¤šå®ç°ä¸Šç»†èŠ‚ï¼Œå°¤å…¶æ˜¯è®©GANæ¨¡å‹stableçš„æ–¹æ³•ã€‚æ‰€ä»¥å¦‚æœå¯¹äºGANçš„å®ç°æœ‰å…´è¶£ï¼Œè¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯å¿…è¯»ã€‚æ­¤å¤–ï¼Œæœ€æ–°NIPS2016ä¹Ÿæœ‰æœ€æ–°çš„å…³äºè®­ç»ƒGANæ¨¡å‹çš„æ€»ç»“ [How to Train a GAN? Tips and tricks to make GANs work] (<a href="https://github.com/soumith/ganhacks" target="_blank" rel="external">https://github.com/soumith/ganhacks</a> â€œGAN tricksâ€)ã€‚</p>
<p>3ã€InfoGAN</p>
<p>åœ¨GANæ¨¡å‹ä¸­ï¼Œç”Ÿæˆæ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªè¿ç»­çš„å™ªå£°ä¿¡å·ï¼Œç”±äºæ²¡æœ‰ä»»ä½•çº¦æŸï¼Œå³ä¾¿æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„ç”Ÿæˆæ¨¡å‹ï¼Œzä¹Ÿä¸èƒ½è¢«å¾ˆå¥½çš„è§£é‡Šã€‚ä¸ºäº†ä½¿è¾“å…¥åŒ…å«å¯ä»¥è§£é‡Šï¼Œæ›´æœ‰ä¿¡æ¯çš„æ„ä¹‰ï¼ŒInfoGAN[7]çš„æ¨¡å‹åœ¨zä¹‹å¤–ï¼Œåˆå¢åŠ äº†ä¸€ä¸ªè¾“å…¥cï¼Œç§°ä¹‹ä¸ºéšå«è¾“å…¥(latent code)ï¼Œç„¶åé€šè¿‡çº¦æŸcä¸ç”Ÿæˆæ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œä½¿å¾—cé‡Œé¢å¯ä»¥åŒ…å«æŸäº›è¯­ä¹‰ç‰¹å¾(semantic feature)ï¼Œæ¯”å¦‚å¯¹MNISTæ•°æ®ï¼Œcå¯ä»¥æ˜¯digit(0-9)ï¼Œå€¾æ–œåº¦ï¼Œç¬”ç”»åšåº¦ç­‰ã€‚å…·ä½“åšæ³•æ˜¯ï¼šé¦–å…ˆæˆ‘ä»¬ç¡®å®šéœ€è¦è¡¨è¾¾å‡ ä¸ªç‰¹å¾ä»¥åŠè¿™äº›ç‰¹å¾çš„æ•°æ®ç±»å‹ï¼Œæ¯”å¦‚æ˜¯ç±»åˆ«(categorical)è¿˜æ˜¯è¿ç»­æ•°å€¼ï¼Œå¯¹æ¯ä¸ªç‰¹å¾æˆ‘ä»¬ç”¨ä¸€ä¸ªç»´åº¦è¡¨ç¤ºci  ã€‚</p>
<p>æ¥ä¸‹æ¥ï¼Œåˆ©ç”¨äº’ä¿¡æ¯é‡æ¥çº¦æŸcã€‚åŸç†åœ¨äºï¼Œå¦‚æœ c å’Œç”Ÿæˆçš„å›¾åƒä¹‹é—´å­˜åœ¨æŸç§ç‰¹å®šçš„å¯¹åº”ï¼ˆå¦‚æœcæ˜¯å›¾åƒçš„æŸäº›ç‰¹å¾ï¼Œåˆ™æœ‰è¿™æ ·çš„å‡½æ•°å­˜åœ¨ï¼‰ï¼Œé‚£ä¹ˆcå’ŒG(z,c)ä¹‹é—´å°±åº”è¯¥æœ‰äº’ä¿¡æ¯é‡ã€‚å¦‚æœæ˜¯æ— çº¦æŸçš„æƒ…å†µï¼Œæ¯”å¦‚zå•ç‹¬çš„æ¯ä¸€ä¸ªç»´åº¦éƒ½è·Ÿå’ŒG(z)æ²¡æœ‰ç‰¹å®šçš„å…³ç³»ï¼Œé‚£ä¹ˆå®ƒä»¬ä¹‹é—´çš„äº’ä¿¡æ¯é‡åº”è¯¥æ¥è¿‘0ã€‚æ‰€ä»¥åŠ ä¸Šè¿™ä¸ªçº¦æŸä¹‹åï¼Œè¦ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°å°±å˜æˆäº†</p>
<pre><code>min max V(D,G) = V(D,G) - ğœ† I(c;G(z,c))
</code></pre><p>æ¥ä¸‹æ¥å°±æ˜¯å¦‚ä½•å¤„ç† I(c; G)â€‹ã€‚ç”±äº I(c;G(z,c))â€‹ çš„è®¡ç®—éœ€è¦ p(c|x)â€‹ï¼Œè€Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“çœŸå®çš„åˆ†å¸ƒã€‚è¿™æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ä¸€ä¸ª Q(c|x)â€‹ æ¥è¿‘ä¼¼ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒQå¯ä»¥ç”¨ç¥ç»ç½‘ç»œæ¥å®ç°ã€‚æ­¤å¤–ï¼Œ å¯ä»¥åˆ©ç”¨reparametrizationï¼ˆè§é™„å½•ï¼‰çš„æŠ€å·§æ¥ç®€åŒ–ç½‘ç»œã€‚</p>
<p>åœ¨å®é™…ä¸­ï¼Œç”±äºQå’ŒDéƒ½æ˜¯è¾“å…¥ xï¼Œè€Œä¸”è¯†åˆ«ç½‘ç»œDé™¤äº†å¯ä»¥è¾“å‡ºæ¦‚ç‡ï¼Œä¹Ÿå¯ä»¥åšç‰¹å¾æå–ï¼Œå› æ­¤Qå¯ä»¥å’ŒDå…±äº«å‚æ•°ã€‚åœ¨æ­£å¸¸çš„Dä¹‹åï¼Œé¢å¤–åŠ ä¸€å±‚full connected layerï¼Œåˆ©ç”¨softmaxç­‰å¯ä»¥è¾“å‡ºcã€‚è¿™ä¹Ÿæ˜¯å›¾3ä¸­çš„ç»“æ„ã€‚</p>
<p>4ã€ Conditional GAN</p>
<p>Conditional GANçš„åŸºæœ¬æ¨¡å‹è§å›¾3ã€‚æ‰€è°“conditionalçš„æ„æ€å°±æ˜¯ï¼Œç”Ÿæˆå›¾ç‰‡çš„æ¨¡å‹å˜æˆäº† P(X|z, c)ï¼Œè€Œcæ˜¯æˆ‘ä»¬é¢å¤–æä¾›çš„ä¿¡æ¯ã€‚è¿™é‡Œè¦æ³¨æ„conditional GANå’ŒInfo GANçš„ç»“æ„åŒºåˆ«</p>
<ul>
<li>Infoä¸­cä¿¡æ¯æ˜¯éœ€è¦ç½‘ç»œå»å­¦ä¹ æå–çš„ç‰¹å¾ï¼Œè€Œè¿™é‡Œæ˜¯éœ€è¦æˆ‘ä»¬è¾“å…¥ç½‘ç»œçš„ä¿¡æ¯ã€‚</li>
<li>Infoä¸­cåªè¾“å…¥ç”Ÿæˆç½‘ç»œï¼Œè€Œè¿™é‡Œéœ€è¦åŒæ—¶è¾“å…¥ç”Ÿæˆå’Œè¯†åˆ«ç½‘ç»œï¼Œä»¥ä¾¿è®©ç½‘ç»œå­¦ä¹ åˆ°å®ƒä»¬ä¹‹é—´çš„å…³è”ã€‚</li>
</ul>
<p>åœ¨Conditional GANä¸­ï¼Œéšç€cçš„å˜æ¢å¯ä»¥è¡ç”Ÿå‡ºå¾ˆå¤šåº”ç”¨ï¼Œæ¯”å¦‚è¾“å…¥å¯ä»¥æ˜¯labelï¼Œå¯ä»¥æ˜¯åˆ†ç±»ã€‚ç”šè‡³æ˜¯å¦å¤–ä¸€ä¸ªå›¾ç‰‡ï¼Œæ¯”å¦‚å¯ä»¥åšimage to imageçš„é£æ ¼è½¬æ¢ï¼Œä¹Ÿå¯ä»¥åšåˆ†è¾¨ç‡æå‡super-resolutionã€‚è¿™é‡Œæˆ‘ä»¬ä»¥Text-to-Image[5] ä¸ºä¾‹ï¼Œè®²ä¸€ä¸‹conditional GANçš„ä¸€ç§å»ºæ¨¡æ–¹æ³•ã€‚</p>
<p>åŒæ ·ï¼Œå…ˆä¸Šå›¾ï¼š</p>
<p><img src="media/text2img.png" alt="text2img"></p>
<p>æ¨¡å‹çš„ä»»åŠ¡æ˜¯ç»™å®šä¸€å¥æ–‡å­—æè¿°ï¼Œç„¶åå¯ä»¥ç”Ÿæˆç¬¦åˆæè¿°çš„å›¾åƒã€‚å¯ä»¥çœ‹åˆ°ï¼Œç½‘ç»œçš„è¾“å…¥é™¤äº†é‡‡æ ·å™ªå£°zä»¥å¤–è¿˜æœ‰æ–‡å­—ä¿¡æ¯ã€‚æ•´ä¸ªä»»åŠ¡åˆ†ä¸ºä¸¤å¤§éƒ¨åˆ†ï¼šç¬¬ä¸€éƒ¨åˆ†æ˜¯è¦å¯¹æ–‡å­—è¿›è¡Œç¼–ç (text encoding)ï¼Œè¿™éƒ¨åˆ†å¹¶ä¸æ˜¯Conditonal GANæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥ä½¿ç”¨RNNæˆ–è€…char-CNNç­‰ã€‚æ–‡ä¸­ç”¨çš„æ˜¯deep convolutional and recurrent text encoder[4] ï¼Œæ„Ÿå…´è¶£å¯ä»¥å»çœ‹è¿™ç¯‡æ–‡ç« [4]ã€‚</p>
<p>åœ¨æ¨¡å‹ä¸­ï¼Œæ–‡å­—ä¿¡æ¯åŒæ—¶è¾“å…¥ G å’Œ D æ˜¯å…³é”®æ‰€åœ¨ï¼Œè¿™æ ·ç½‘ç»œæ‰èƒ½å¤Ÿå°†æ–‡å­—å’Œå›¾ç‰‡å…³è”èµ·æ¥ã€‚å…¶æ¬¡ï¼Œåœ¨è®­ç»ƒä¸­ï¼ŒåŸGANä¸­ D åªéœ€è¦åˆ¤æ–­ä¸¤ç§æ•°æ®ï¼šreal/fakeçš„å›¾ç‰‡ã€‚è€Œè¿™é‡Œï¼ŒD éœ€è¦åˆ¤æ–­ï¼ˆè¾“å…¥ï¼‰ä¸‰ç§æ•°æ®{real image, right text}ï¼Œ{real image, wrong text}ä»¥åŠ{fake image, right text}ã€‚</p>
<p>5ã€ StackGAN</p>
<p>StackGAN[8] æ¨¡å‹æœ¬è´¨å°±æ˜¯æ˜¯Conditional GANï¼Œåªä¸è¿‡å®ƒä½¿ç”¨äº†ä¸¤å±‚conditional GANæ¨¡å‹ï¼Œç¬¬ä¸€å±‚æ¨¡å‹ P(X1|z, c) åˆ©ç”¨è¾“å…¥çš„æ–‡å­—ä¿¡æ¯cç”Ÿæˆä¸€ä¸ªè¾ƒä½åˆ†è¾¨ç‡çš„å›¾ç‰‡ã€‚ä¹‹åç¬¬äºŒå±‚æ¨¡å‹ P(X|c,,X1) åŸºäºç¬¬ä¸€å±‚ç”Ÿæˆçš„å›¾ç‰‡ä»¥åŠæ–‡å­—ä¿¡æ¯ç”Ÿæˆæ›´åŠ ä¼˜åŒ–çš„å›¾ç‰‡ã€‚æ–‡ä¸­ç»™å‡ºçš„å®éªŒæ•ˆæœéå¸¸çš„æƒŠäººï¼Œå¯ä»¥ç”Ÿæˆ256x256çš„éå¸¸çœŸå®çš„å›¾ç‰‡ã€‚è¿™é‡Œä¸å†é‡å¤ç»†èŠ‚ã€‚ä¸‹å›¾ä¸ºç®€åŒ–çš„StackGANæ¨¡å‹ã€‚</p>
<p><img src="media/stackGAN.png" alt="stackGAN"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Goodfellow, Ian, et al. â€œGenerative adversarial nets.â€ <em>Advances in Neural Information Processing Systems</em>. 2014.</li>
<li>Radford, Alec, Luke Metz, and Soumith Chintala. â€œUnsupervised representation learning with deep convolutional generative adversarial networks.â€ <em>arXiv preprint arXiv:1511.06434</em> (2015).</li>
<li>Reed, Scott, et al. â€œGenerative adversarial text to image synthesis.â€ <em>arXiv preprint arXiv:1605.05396</em> (2016).</li>
<li>Reed, Scott, et al. â€œLearning Deep Representations of Fine-Grained Visual Descriptions.â€ <em>arXiv preprint arXiv:1605.05395</em> (2016).</li>
<li>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</li>
<li>Image-to-Image Translation with Conditional Adversarial Networks</li>
<li>Chen, Xi, et al. â€œInfogan: Interpretable representation learning by information maximizing generative adversarial nets.â€ <em>Advances in Neural Information Processing Systems</em>. 2016.</li>
<li>Zhang, Han, et al. â€œStackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.â€ <em>arXiv preprint arXiv:1612.03242</em> (2016).</li>
<li><a href="https://twitter.com/ch402/status/793535193835417601" target="_blank" rel="external">https://twitter.com/ch402/status/793535193835417601</a></li>
</ol>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>çœ‹äº†å‡ ç¯‡å…³äºGANçš„æ–‡ç« ï¼Œå‘ç°æœ‰å‡ ä¸ªå»ºæ¨¡çš„å°trick</p>
<ul>
<li>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä¹‹æ‰€ä»¥å¯ä»¥ä»ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒé‡‡æ ·ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªç½‘ç»œï¼ˆå‚æ•°éœ€è¦å­¦ä¹ ï¼‰å»è¿‘ä¼¼æ•°æ®çš„åˆ†å¸ƒ èƒŒåçš„åŸç†æ˜¯</li>
</ul>
<blockquote>
<p>Any distribution in d dim can be generated by taking a set of d normal distribution variables. mapping through a sufficiently complicated function. So provided powerful function approximators, we can simply learn a function mapping independent norm distribution z to whatever X.</p>
</blockquote>
<ul>
<li><p>åœ¨æ¨¡å‹ä¸­ï¼Œå¦‚æœç›®æ ‡å‡½æ•°ä¸­æŸä¸ªæ¡ä»¶æ¦‚ç‡æ— æ³•ç›´æ¥å¾—åˆ°ï¼Œé‚£ä¹ˆå¯ä»¥å­¦ä¹ ä¸€ä¸ªç½‘ç»œQå»è¿‘ä¼¼ã€‚åˆ©ç”¨KL divergence D{KL}[P||Q] = H(P,Q) - H(P) ä»¥åŠ<br>D{KL} &gt;= 0 å¯ä»¥æ¨å‡ºä¸€ä¸ªæ›´æ˜“ä¼˜åŒ–çš„ä¸Š/ä¸‹ç•Œã€‚</p>
</li>
<li><p><strong>reparametrization trick</strong> ä¸¾ä¸ªä¾‹å­ï¼Œæ¯”å¦‚æ¨¡å‹ä¸­ç”¨ä¸€ä¸ªç½‘ç»œ Q(z|x) æ¥è¿‘ä¼¼çœŸå®çš„ P(z|x)ï¼Œæˆ‘ä»¬å¸¸ç”¨æ­£æ€åˆ†å¸ƒæ¥å»ºæ¨¡Qï¼Œå³<br>N(Î¼, ğ›´)ï¼ˆè¿™é‡Œ Î¼ å’Œ ğ›´ éƒ½æ˜¯å¸¦å‚æ•°çš„ç½‘ç»œï¼Œé€šè¿‡å­¦ä¹ å¾—åˆ°ï¼‰ã€‚å½“é‡‡æ ·çš„ x é€šè¿‡ Q åå°±å¯ä»¥å¾—åˆ°zã€‚ä½†æ˜¯ç”±äºè¿™ä¸€æ­¥æ˜¯éšæœºè¿‡ç¨‹ï¼Œbackpropagationå°±ä¼šä¸­æ–­ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ N(Î¼, ğ›´) = N(0, I) â¨‰ ğœ® + Î¼ å°†éšæœºè¿‡ç¨‹è½¬ç§»åˆ°è¾“å…¥ç«¯ã€‚å…ˆä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ · z0ï¼Œæ­¤æ—¶ç½‘ç»œ Q å¹¶ä¸ç›´æ¥è¾“å‡ºzï¼Œè€Œæ˜¯è¾“å‡ºä¸¤ä¸ªå‚æ•°Î¼ å’Œ ğ›´ï¼Œä¹‹ååœ¨é€šè¿‡ z=z0 â¨‰ ğ›´ + Î¼ å¾—åˆ°zã€‚ç”±äºä¸­é—´èŠ‚ç‚¹å˜æˆäº†å¸¸è§„è¿ç®—ï¼Œå› æ­¤backpropagationå¯ä»¥æ­£å¸¸ä¼ å›è¾“å…¥ç«¯ã€‚</p>
<p>  â€‹</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;GANï¼ˆGenerative-Adversarial-Netsï¼‰ç ”ç©¶è¿›å±•&quot;&gt;&lt;a href=&quot;#GANï¼ˆGenerative-Adversarial-Netsï¼‰ç ”ç©¶è¿›å±•&quot; class=&quot;headerlink&quot; title=&quot;GANï¼ˆGenerative Adver
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>2016å¹´è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸ15ç¯‡å€¼å¾—è¯»çš„Paper</title>
    <link href="http://rsarxiv.github.io/2016/12/29/2016%E5%B9%B4%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E9%A2%86%E5%9F%9F10%E7%AF%87%E5%80%BC%E5%BE%97%E8%AF%BB%E7%9A%84Paper/"/>
    <id>http://rsarxiv.github.io/2016/12/29/2016å¹´è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸ10ç¯‡å€¼å¾—è¯»çš„Paper/</id>
    <published>2016-12-30T04:10:12.000Z</published>
    <updated>2017-01-03T03:49:29.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learning-to-Compose-Neural-Networks-for-Question-Answering"><a href="#Learning-to-Compose-Neural-Networks-for-Question-Answering" class="headerlink" title="Learning to Compose Neural Networks for Question Answering"></a><a href="https://arxiv.org/abs/1601.01705" target="_blank" rel="external">Learning to Compose Neural Networks for Question Answering</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Department of Electrical Engineering and Computer Sciences<br>University of California, Berkeley</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Question Answering</p>
<h1 id="Text-understanding-with-the-attention-sum-reader-network"><a href="#Text-understanding-with-the-attention-sum-reader-network" class="headerlink" title="Text understanding with the attention sum reader network"></a><a href="https://arxiv.org/abs/1603.01547" target="_blank" rel="external">Text understanding with the attention sum reader network</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>IBM Watson</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Machine Reading Comprehension </p>
<h1 id="Improving-Information-Extraction-by-Acquiring-External-Evidence-with-Reinforcement-Learning"><a href="#Improving-Information-Extraction-by-Acquiring-External-Evidence-with-Reinforcement-Learning" class="headerlink" title="Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning"></a><a href="https://arxiv.org/abs/1603.07954" target="_blank" rel="external">Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Karthik Narasimhan, Adam Yala, Regina Barzilay</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>CSAIL, MIT</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Information Extraction; Reinforcement Learning</p>
<h1 id="Pointing-the-Unknown-Words"><a href="#Pointing-the-Unknown-Words" class="headerlink" title="Pointing the Unknown Words"></a><a href="https://arxiv.org/abs/1603.08148" target="_blank" rel="external">Pointing the Unknown Words</a></h1><h2 id="ä½œè€…-3"><a href="#ä½œè€…-3" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio</p>
<h2 id="å•ä½-3"><a href="#å•ä½-3" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Universite de MontrÂ´eal<br>IBM T.J. Watson Research<br>CIFAR Senior Fellow</p>
<h2 id="å…³é”®è¯-3"><a href="#å…³é”®è¯-3" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Unknown Words</p>
<h1 id="Sequence-to-Sequence-Learning-as-Beam-Search-Optimization"><a href="#Sequence-to-Sequence-Learning-as-Beam-Search-Optimization" class="headerlink" title="Sequence-to-Sequence Learning as Beam-Search Optimization"></a><a href="https://arxiv.org/abs/1606.02960" target="_blank" rel="external">Sequence-to-Sequence Learning as Beam-Search Optimization</a></h1><h2 id="ä½œè€…-4"><a href="#ä½œè€…-4" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Sam Wiseman, Alexander M. Rush</p>
<h2 id="å•ä½-4"><a href="#å•ä½-4" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>School of Engineering and Applied Sciences, Harvard University</p>
<h2 id="å…³é”®è¯-4"><a href="#å…³é”®è¯-4" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Seq2Seq; Beam Search</p>
<h1 id="SQuAD-100-000-Questions-for-Machine-Comprehension-of-Text"><a href="#SQuAD-100-000-Questions-for-Machine-Comprehension-of-Text" class="headerlink" title="SQuAD: 100,000+ Questions for Machine Comprehension of Text"></a><a href="https://arxiv.org/abs/1606.05250" target="_blank" rel="external">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></h1><h2 id="ä½œè€…-5"><a href="#ä½œè€…-5" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang</p>
<h2 id="å•ä½-5"><a href="#å•ä½-5" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Computer Science Department<br>Stanford University</p>
<h2 id="å…³é”®è¯-5"><a href="#å…³é”®è¯-5" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Machine Reading Comprehension; Dataset</p>
<h1 id="End-to-End-Reinforcement-Learning-of-Dialogue-Agents-for-Information-Access"><a href="#End-to-End-Reinforcement-Learning-of-Dialogue-Agents-for-Information-Access" class="headerlink" title="End-to-End Reinforcement Learning of Dialogue Agents for Information Access"></a><a href="https://arxiv.org/abs/1609.00777" target="_blank" rel="external">End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a></h1><h2 id="ä½œè€…-6"><a href="#ä½œè€…-6" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, Li Deng</p>
<h2 id="å•ä½-6"><a href="#å•ä½-6" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>School of Computer Science, Carnegie Mellon University<br>Microsoft Research<br>National Taiwan University</p>
<h2 id="å…³é”®è¯-6"><a href="#å…³é”®è¯-6" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Reinforcement Learning; Dialogue System</p>
<h1 id="ReasoNet-Learning-to-Stop-Reading-in-Machine-Comprehension"><a href="#ReasoNet-Learning-to-Stop-Reading-in-Machine-Comprehension" class="headerlink" title="ReasoNet: Learning to Stop Reading in Machine Comprehension"></a><a href="https://arxiv.org/abs/1609.05284" target="_blank" rel="external">ReasoNet: Learning to Stop Reading in Machine Comprehension</a></h1><h2 id="ä½œè€…-7"><a href="#ä½œè€…-7" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen</p>
<h2 id="å•ä½-7"><a href="#å•ä½-7" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Microsoft Research Redmond</p>
<h2 id="å…³é”®è¯-7"><a href="#å…³é”®è¯-7" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Machine Reading Comprehension </p>
<h1 id="Personalizing-a-Dialogue-System-with-Transfer-Learning"><a href="#Personalizing-a-Dialogue-System-with-Transfer-Learning" class="headerlink" title="Personalizing a Dialogue System with Transfer Learning"></a><a href="https://arxiv.org/abs/1610.02891" target="_blank" rel="external">Personalizing a Dialogue System with Transfer Learning</a></h1><h2 id="ä½œè€…-8"><a href="#ä½œè€…-8" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, Qiang Yang</p>
<h2 id="å•ä½-8"><a href="#å•ä½-8" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>The Hong Kong University of Science and Technology</p>
<h2 id="å…³é”®è¯-8"><a href="#å…³é”®è¯-8" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Dialogue System; Transfer Learning</p>
<h1 id="LightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Network"><a href="#LightRNN-Memory-and-Computation-Efficient-Recurrent-Neural-Network" class="headerlink" title="LightRNN Memory and Computation-Efficient Recurrent Neural Network"></a><a href="https://arxiv.org/abs/1610.09893" target="_blank" rel="external">LightRNN Memory and Computation-Efficient Recurrent Neural Network</a></h1><h2 id="ä½œè€…-9"><a href="#ä½œè€…-9" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Xiang Li, Tao Qin, Jian Yang, Tie-Yan Liu</p>
<h2 id="å•ä½-9"><a href="#å•ä½-9" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Nanjing University of Science and Technology<br>Microsoft Research Asia</p>
<h2 id="å…³é”®è¯-9"><a href="#å…³é”®è¯-9" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>New Recurrent Neural Network</p>
<h1 id="Dual-Learning-for-Machine-Translation"><a href="#Dual-Learning-for-Machine-Translation" class="headerlink" title="Dual Learning for Machine Translation"></a><a href="https://arxiv.org/abs/1611.00179" target="_blank" rel="external">Dual Learning for Machine Translation</a></h1><h2 id="ä½œè€…-10"><a href="#ä½œè€…-10" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma</p>
<h2 id="å•ä½-10"><a href="#å•ä½-10" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>University of Science and Technology of China<br>Key Laboratory of Machine Perception (MOE), School of EECS, Peking University<br>Microsoft Research</p>
<h2 id="å…³é”®è¯-10"><a href="#å…³é”®è¯-10" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Dual Learning; Neural Machine Translation</p>
<h1 id="Neural-Machine-Translation-with-Reconstruction"><a href="#Neural-Machine-Translation-with-Reconstruction" class="headerlink" title="Neural Machine Translation with Reconstruction"></a><a href="https://arxiv.org/abs/1611.01874" target="_blank" rel="external">Neural Machine Translation with Reconstruction</a></h1><h2 id="ä½œè€…-11"><a href="#ä½œè€…-11" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, Hang Li</p>
<h2 id="å•ä½-11"><a href="#å•ä½-11" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Noahâ€™s Ark Lab, Huawei Technologies<br>Department of Computer Science and Technology, Tsinghua University</p>
<h2 id="å…³é”®è¯-11"><a href="#å…³é”®è¯-11" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Neural Machine Translation</p>
<h1 id="Linguistically-Regularized-LSTMs-for-Sentiment-Classification"><a href="#Linguistically-Regularized-LSTMs-for-Sentiment-Classification" class="headerlink" title="Linguistically Regularized LSTMs for Sentiment Classification"></a><a href="https://arxiv.org/abs/1611.03949" target="_blank" rel="external">Linguistically Regularized LSTMs for Sentiment Classification</a></h1><h2 id="ä½œè€…-12"><a href="#ä½œè€…-12" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Qiao Qian, Minlie Huang, Xiaoyan Zhu</p>
<h2 id="å•ä½-12"><a href="#å•ä½-12" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology<br>Dept. of Computer Science and Technology, Tsinghua University</p>
<h2 id="å…³é”®è¯-12"><a href="#å…³é”®è¯-12" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Sentiment Classification; LSTM</p>
<h1 id="Googleâ€™s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation"><a href="#Googleâ€™s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation" class="headerlink" title="Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"></a><a href="https://arxiv.org/abs/1611.04558" target="_blank" rel="external">Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a></h1><h2 id="ä½œè€…-13"><a href="#ä½œè€…-13" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda ViÃ©gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean</p>
<h2 id="å•ä½-13"><a href="#å•ä½-13" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Google</p>
<h2 id="å…³é”®è¯-13"><a href="#å…³é”®è¯-13" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Multilingual Neural Machine Translation; Zero-Shot</p>
<h1 id="Language-Modeling-with-Gated-Convolutional-Networks"><a href="#Language-Modeling-with-Gated-Convolutional-Networks" class="headerlink" title="Language Modeling with Gated Convolutional Networks"></a><a href="https://arxiv.org/abs/1612.08083" target="_blank" rel="external">Language Modeling with Gated Convolutional Networks</a></h1><h2 id="ä½œè€…-14"><a href="#ä½œè€…-14" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier</p>
<h2 id="å•ä½-14"><a href="#å•ä½-14" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Facebook AI Research</p>
<h2 id="å…³é”®è¯-14"><a href="#å…³é”®è¯-14" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Language Modeling; Gated CNN</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Learning-to-Compose-Neural-Networks-for-Question-Answering&quot;&gt;&lt;a href=&quot;#Learning-to-Compose-Neural-Networks-for-Question-Answering&quot; cl
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
  </entry>
  
  <entry>
    <title>å‘Šåˆ«2016ï¼Œè¿æ¥2017</title>
    <link href="http://rsarxiv.github.io/2016/12/29/%E5%91%8A%E5%88%AB2016%EF%BC%8C%E8%BF%8E%E6%8E%A52017/"/>
    <id>http://rsarxiv.github.io/2016/12/29/å‘Šåˆ«2016ï¼Œè¿æ¥2017/</id>
    <published>2016-12-29T19:20:43.000Z</published>
    <updated>2016-12-30T18:02:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>2016å¹´å³å°†ç»“æŸï¼Œé¦–å…ˆå¯¹æ”¯æŒPaperWeeklyçš„å„ä½ç«¥é‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼Œæ„Ÿè°¢ä½ ä»¬è®©æˆ‘æœ‰åŠ¨åŠ›å°†è¿™ä¸ªç”¨æ¥ç£ä¿ƒæˆ‘è‡ªå·±å¤šè¯»paperçš„side projectåšæŒä¸€ç›´åšä¸‹æ¥ï¼Œæ„Ÿè°¢å„ä½å¯¹è‡ªç„¶è¯­è¨€å¤„ç†æ„Ÿå…´è¶£å¹¶ä¸”æ„¿æ„ç‰ºç‰²ä¸€äº›ä¸ªäººæ—¶é—´æ¥å†™paper noteçš„å°ä¼™ä¼´ï¼Œä¹Ÿæ„Ÿè°¢æ¯å¤©åšæŒä¸€èµ·åˆ·arXivæ¥ä¿è¯å‘¨æœ«æ¨èâ€œæ¯å‘¨å€¼å¾—è¯»â€è´¨é‡çš„å‡ ä½ç«¥é‹ï¼Œæ„Ÿè°¢åŠ å…¥PaperWeeklyäº¤æµç¾¤æ¯å¤©éƒ½è´¡çŒ®å¾ˆå¤šé«˜è´¨é‡è®¨è®ºå†…å®¹çš„å„ä½æœ‹å‹ï¼Œæ„Ÿè°¢ä¸ºä¸‰ä¸ªäº¤æµç¾¤åšæ¶ˆæ¯åŒæ­¥æœºå™¨äººçš„ç§ç“œåŒå­¦ã€‚</p>
<p>PaperWeeklyä»åˆšå¼€å§‹åªæœ‰æˆ‘ä¸€ä¸ªäººï¼Œåˆ°ç°åœ¨ä¸€å…±æœ‰50å¤šä½ä¸€èµ·æ„¿æ„åˆ†äº«å†…å®¹çš„å°ä¼™ä¼´ï¼Œå¹¶ä¸”è¿™ä¸ªæ•°å­—éšç€å¤§å®¶çš„çƒ­æƒ…å‚ä¸ä¼šé€æ¸å¢åŠ ï¼Œæ­£æ˜¯æœ‰äº†è¿™ä¹ˆå¤šç§¯æå‚ä¸çš„å°ä¼™ä¼´ï¼Œæ‰æœ‰äº†PaperWeeklyä¸€å‘¨æ•¢åšä¸€ä¸ªtopicçš„åº•æ°”ã€‚ä»9æœˆ1å·é‡æ–°ç»„ç»‡PaperWeeklyçš„å†…å®¹å½¢å¼ï¼Œåˆ°ä»Šå¤©ä¸ºæ­¢ï¼Œä¸€å…±å‘å¸ƒäº†17æœŸå†…å®¹ï¼ŒåŠ ä¸Šä¹‹å‰æˆ‘è‡ªå·±å†™çš„2æœŸå†…å®¹ï¼Œä¸€å…±æ˜¯19æœŸå†…å®¹ï¼Œ19æœŸæ„å‘³ç€19å‘¨çš„æ—¶é—´ï¼Œ19å‘¨çš„æ—¶é—´æˆ‘ä»¬å¯ä»¥èµ°è¿‡å¾ˆå¤šåœ°æ–¹ï¼Œåƒè¿‡å¾ˆå¤šç¾é£Ÿï¼Œçœ‹è¿‡å¾ˆå¤šç¾æ™¯ï¼Œè€Œæˆ‘ä»¬é€‰æ‹©äº†è¯»19å‘¨çš„paperï¼Œé€‰æ‹©äº†å†™19å‘¨çš„paper noteï¼Œé€‰æ‹©äº†æ¨èè¿™19å‘¨ä¸­é«˜è´¨é‡çš„paperï¼Œé€‰æ‹©äº†åˆ†äº«è¿™19å‘¨ä»¥æ¥å¤§å®¶çš„æˆé•¿ã€ç§¯ç´¯ä¸æ€è€ƒã€‚</p>
<p>19å‘¨çš„æ—¶é—´ï¼ŒPaperWeeklyä¸€å…±å®Œæˆäº†83ç¯‡paper notesï¼Œè€Œè¿™83ç¯‡paperå¯ä»¥ç”¨19ä¸ªç‹¬ç«‹çš„topicç»„ç»‡èµ·æ¥ï¼Œæ¯”å¦‚ï¼š</p>
<p>1ã€æé«˜seq2seqç”Ÿæˆå¯¹è¯çš„æµç•…æ€§å’Œå¤šæ ·æ€§ï¼›<br>2ã€é€šè¿‡æ— ç›‘ç£/åŠç›‘ç£çš„æ–¹æ³•æ¥åšå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼›<br>3ã€å“ªäº›ICLR2017çš„paperå€¼å¾—å…³æ³¨ï¼›<br>4ã€Attentionæ¨¡å‹åœ¨NMTä»»åŠ¡ä¸­çš„åº”ç”¨å’Œè¿›å±•ï¼›<br>5ã€æ–‡æœ¬æ‘˜è¦æŠ€æœ¯çš„è¿›å±•æƒ…å†µï¼›<br>6ã€å¢å¼ºå­¦ä¹ åœ¨å¯¹è¯ç”Ÿæˆä¸­çš„åº”ç”¨ï¼›<br>7ã€GANçš„ç ”ç©¶è¿›å±•ï¼›</p>
<p>æ¯ä¸ªtopicéƒ½æ¶‰åŠåˆ°äº†ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼Œæœ‰çš„å†…å®¹éå¸¸çƒ­é—¨ï¼Œæ¯”å¦‚GANï¼Œæœ‰çš„å†…å®¹éå¸¸ç»å…¸ï¼Œæ¯”å¦‚NERï¼Œæ¯ä¸ªtopicéƒ½ä¼šæŠ“ä½ä¸€äº›ç‰¹ç‚¹æ¥å½’çº³å‡ ç¯‡paperï¼Œä¸ºå‡†å¤‡å…¥é—¨ã€æ­£åœ¨å…¥é—¨ã€å·²ç»å…¥é—¨çš„åŒå­¦æä¾›äº†æœåŠ¡å’Œæ–¹ä¾¿ã€‚</p>
<p>ä»8.25å¼€å§‹ï¼ŒPaperWeeklyæ¨å‡ºäº†â€œæ¯å‘¨å€¼å¾—è¯»â€æ ç›®ï¼Œæ—¨åœ¨å……å½“arXivä¸Šè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢çš„äººå·¥è¿‡æ»¤å™¨ï¼Œæ—¨åœ¨è§£å†³ä¿¡æ¯è¿‡è½½é—®é¢˜ï¼Œæ—¨åœ¨å¸®åŠ©å¤§å®¶æ›´å¿«åœ°äº†è§£åˆ°å“ªäº›paperæ›´å€¼å¾—å…³æ³¨ã€‚</p>
<p>ä»8.25å¼€å§‹ï¼ŒPaperWeeklyä¸€å…±æ¨èäº†153ç¯‡é«˜è´¨é‡çš„paperï¼Œå½“ç„¶æ¯ä¸ªäººå¯¹äºè´¨é‡çš„ç†è§£éƒ½ä¼šæœ‰æ‰€åå·®ï¼Œæœ‰çš„paperç»™æ•´ä¸ªç ”ç©¶å¸¦æ¥äº†å·¨å¤§çš„å½±å“ï¼Œæœ‰çš„paperå¯èƒ½å¯¹æŸä¸ªé¢†åŸŸæœ‰æ‰€æé«˜ï¼Œæœ‰çš„paperæ‰€è•´å«çš„æ€æƒ³ä¼šå¸¦æ¥å¾ˆå¤šçš„å¯å‘ï¼Œè¿™æ˜¯ä¸€ä»¶ä»è€…è§ä»æ™ºè€…è§æ™ºçš„äº‹æƒ…ã€‚</p>
<p>åœ¨åšPaperWeeklyçš„æ—¶å€™ï¼Œæˆ‘è§‚å¯Ÿåˆ°å¤§å®¶æœ‰ä¸€å®šçš„æ‹›è˜éœ€æ±‚ï¼Œå¯èƒ½æ˜¯å…¬å¸ï¼Œä¹Ÿå¯èƒ½æ˜¯é™¢æ ¡æˆ–è€…ç§‘ç ”æœºæ„ï¼Œä½†æ˜¯åœ¨äº¤æµç¾¤ä¸­å‘çš„æ•ˆæœåˆä¸æ˜¯å¾ˆå¥½ï¼Œäºæ˜¯åšäº†ä¸ªå†³å®šï¼Œåœ¨11æœˆä¸­æ—¬å¼€å§‹æ¨å‡ºäº†ä¸€é¡¹æ–°çš„æœåŠ¡â€”å…¬ç›Šå¹¿å‘ŠæœåŠ¡ï¼Œä»ç¬¬ä¸€ä¸ªå¸®åŠ©æ¸…åå¤§å­¦åˆ˜çŸ¥è¿œè€å¸ˆæ‹›åšå£«åå¼€å§‹è‡³ä»Šå·²ç»å‘äº†ä¸€äº›å¹¿å‘Šäº†ï¼Œè™½ç„¶è¿˜æ²¡æœ‰åšæ•ˆæœåé¦ˆå·¥ä½œï¼Œä½†æˆ‘ä»¬ç¡®å®å°½åŠ›åœ¨å¸®è¿™äº›éœ€è¦å¸®åŠ©çš„ä¼ä¸šæˆ–è€…é™¢æ ¡ï¼Œå¦‚æœæ‚¨æœ‰è¿™æ ·çš„æ‹›è˜éœ€æ±‚ï¼Œå¯ä»¥ç§ä¿¡æ¥è”ç³»æˆ‘ï¼Œå¦‚æœå¯ä»¥ä¸PaperWeeklyåˆä½œå†™ä¸€æœŸæ–‡ç« ä¼šæ›´å¥½ï¼</p>
<p>åœ¨åšPaperWeeklyçš„æ—¶å€™ï¼Œæˆ‘ä¹Ÿæœ‰è¿‡ä¸€é˜µè¿·èŒ«ï¼Œå°±æ˜¯å…³äºPaperWeeklyåˆ°åº•æ˜¯ä»€ä¹ˆçš„æ€è€ƒï¼Œè®°å¾—æ˜¯ä¸€ä¸ªå‘¨æ—¥æ™šä¸Šï¼Œæˆ‘åˆ°äº†å¤œé‡Œ3ç‚¹ä»æ²¡æœ‰ç¡ç€ï¼Œå°±çˆ¬èµ·æ¥å†™äº†ä¸€ç¯‡ã€ŠPaperWeeklyåˆ°åº•æ˜¯ä»€ä¹ˆã€‹çš„æ–‡ç« ï¼Œæ¥å¥½å¥½åœ°å®šä¹‰äº†ä¸€ä¸‹æˆ‘ä»¬æ‰€åšçš„äº‹æƒ…ä»¥åŠæ‰€æƒ³è¿½æ±‚çš„ä¸œè¥¿ã€‚æœ€åï¼Œæˆ‘æ˜¯è¿™ä¹ˆå®šä¹‰PaperWeeklyçš„ï¼Œâ€œPaperWeeklyæ˜¯ä¸€ä¸ªç”±50å¤šåå–œæ¬¢åˆ†äº«çŸ¥è¯†çš„ç«¥é‹åˆ©ç”¨å®è´µçš„ä¸šä½™æ—¶é—´æ¥ä¸€èµ·ï¼Œä»¥ä¸€å‘¨ä¸ºå•ä½ã€å¯¹ä¸€ä¸ªtopicè¿›è¡Œå¤šç¯‡paperè§£è¯»å’Œå¯¹æ¯”æ€»ç»“çš„ã€ä¸è¿½æ±‚çƒ­ç‚¹ã€ä¸æäº›å™±å¤´çš„çˆ±å¿ƒå…¬ç›Šç»„ç»‡ï¼Œæ—¨åœ¨åˆ†äº«çŸ¥è¯†ã€‚â€</p>
<p>å¯¹ä¸€ä¸ªä¸œè¥¿çš„å®šä½å¾ˆé‡è¦ï¼Œç›´æ¥å†³å®šäº†å¯¹è¿™ä¸ªä¸œè¥¿çš„æ€åº¦å’Œæ‰€åº”é‡‡å–çš„æ–¹å¼ã€æ–¹æ³•ã€‚æˆ‘åšä¸åˆ°æ‹¿ä¸€äº›å“—ä¼—å–å® çš„åå­—æ¥å‘½åæ–‡ç« æ ‡é¢˜ï¼Œä¹Ÿåšä¸åˆ°è¿‡åˆ†åœ°å¤¸å¤§æˆ–è€…è´¬ä½æŸä¸€ä¸ªä¸œè¥¿ï¼Œæˆ‘åªæƒ³çº¯ç²¹åœ°åšè¿™ä¹ˆä¸€ä»¶äº‹æƒ…ã€‚å„ç§æŒ‡æ ‡å¯¹æˆ‘ä»¬æ¥è¯´æ²¡æœ‰æ„ä¹‰ï¼Œå“ªæ€•æ²¡æœ‰äººæ¥è¯»æ–‡ç« ï¼Œè€Œæˆ‘ä»¬æ¯å¤©æ‰€è¯»çš„è¿™äº›paperï¼Œæ‰€å­¦åˆ°çš„çŸ¥è¯†éƒ½ä¸ä¼šå‡å°‘ï¼Œå½“ç„¶æˆ‘å¸Œæœ›å¤§å®¶å†™çš„ä¸œè¥¿å¯ä»¥åˆ†äº«ç»™æ›´å¤šçš„äººï¼Œè®©æ›´å¤šçš„äººä¸€èµ·æ¥æ„Ÿå—ç§‘æŠ€çš„è¿›æ­¥å’Œå­¦æœ¯çš„å‰æ²¿ï¼Œä½†æˆ‘ä»¬ä¸ä¼šåˆ»æ„åœ°å»è¿½æ±‚ä»€ä¹ˆã€‚æˆ‘ä¸€ç›´è®¤ä¸ºäººèƒ½å¤ŸåšæŒå¹¶ä¸”åŠªåŠ›åšå¥½ä¸€ä»¶äº‹æƒ…çš„æœ€å¤§åŠ¨åŠ›æ˜¯çƒ­çˆ±ï¼Œæ˜¯é‚£ç§æ²¡æœ‰åŠç‚¹è™šä¼ªã€æ²¡æœ‰åŠç‚¹åŠŸåˆ©çš„çƒ­çˆ±ã€‚å› ä¸ºçƒ­çˆ±ï¼Œæ‰€ä»¥çº¯ç²¹ã€‚</p>
<p>PaperWeeklyä¸æ˜¯ä¸€ä¸ªå®Œç¾çš„ä¸œè¥¿ï¼Œä½†æ˜¯ä¸€ä¸ªæˆé•¿çš„ä¸œè¥¿ï¼Œæ˜¯ä¸€ä¸ªä¸€ç›´åœ¨åŠªåŠ›å˜å¥½çš„ä¸œè¥¿ã€‚2016å¿«è¦ç»“æŸäº†ï¼Œåœ¨2017å¹´é‡Œï¼Œæˆ‘ä»¬å°†ä¸æ–­åœ°å®Œå–„æ–‡ç« è´¨é‡ï¼Œä¸°å¯Œæ–‡ç« çš„å½¢å¼ï¼Œå¢åŠ ä¸€äº›ç¾¤å†…çš„ç›´æ’­äº¤æµæ´»åŠ¨ï¼Œæ¯”å¦‚é’ˆå¯¹æŸä¸€ç¯‡ã€æŸå‡ ç¯‡paperçš„è®¨è®ºï¼Œä¸å®šæœŸåœ°é‚€è¯·æ›´å¤šçš„ä¸šç•Œå¤§ç‰›æ¥è®²ä¸€è®²ç†è®ºå’ŒæŠ€æœ¯å¦‚ä½•åœ¨å·¥ä¸šç•Œè½åœ°ç­‰ç­‰ã€‚</p>
<p>PaperWeeklyæ˜¯ä¸€ä¸ªéå¸¸å¼€æ”¾çš„ç»„ç»‡ï¼Œéšæ—¶æ¬¢è¿æƒ³ä¸€èµ·å†™paper notesæˆ–è€…å†™åˆ†äº«çš„ç«¥é‹åŠ å…¥ï¼Œè®©æˆ‘ä»¬ä¸æ–­åœ°åŠªåŠ›ï¼Œä¸æ–­åœ°å£®å¤§åŠ›é‡ï¼Œåœ¨2017å¹´ä¹¦å†™å‡ºæ›´å¤šå€¼å¾—è¯»çš„æ–‡ç« ï¼Œäº§ç”Ÿæ›´å¤šé«˜è´¨é‡çš„è®¨è®ºå†…å®¹ï¼Œä¸€èµ·ä¸ºå›½å†…è‡ªç„¶è¯­è¨€å¤„ç†çš„å‘å±•è´¡çŒ®ä¸€ç‚¹ç‚¹åŠ›é‡ã€‚</p>
<p>æœ€åï¼Œæ„Ÿè°¢å„ä½åˆä½œä¼™ä¼´å¯¹PaperWeeklyçš„å¤§åŠ›æ”¯æŒï¼Œæ„Ÿè°¢æœºå™¨ä¹‹å¿ƒã€ç§‘ç ”åœˆã€IEEEè®¡ç®—ç§‘å­¦è¯„è®ºã€ChatbotChinaã€å°†é—¨åˆ›æŠ•ç­‰åª’ä½“å’Œæœºæ„çš„æ”¯æŒã€‚</p>
<p>2016å¹´æ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œä¹Ÿä»…ä»…æ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œ2017å¹´å³å°†åˆ°æ¥ï¼ŒPaperWeeklyå°†ä¸æ·±åº¦å­¦ä¹ ç¤¾åŒºAI100è¿›è¡Œæ·±åº¦åˆä½œï¼Œä¸ºå¤§å®¶æä¾›æ›´å¥½çš„æœåŠ¡ï¼</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2016å¹´å³å°†ç»“æŸï¼Œé¦–å…ˆå¯¹æ”¯æŒPaperWeeklyçš„å„ä½ç«¥é‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼Œæ„Ÿè°¢ä½ ä»¬è®©æˆ‘æœ‰åŠ¨åŠ›å°†è¿™ä¸ªç”¨æ¥ç£ä¿ƒæˆ‘è‡ªå·±å¤šè¯»paperçš„side projectåšæŒä¸€ç›´åšä¸‹æ¥ï¼Œæ„Ÿè°¢å„ä½å¯¹è‡ªç„¶è¯­è¨€å¤„ç†æ„Ÿå…´è¶£å¹¶ä¸”æ„¿æ„ç‰ºç‰²ä¸€äº›ä¸ªäººæ—¶é—´æ¥å†™paper noteçš„å°ä¼™ä¼´ï¼Œä¹Ÿæ„Ÿè°¢æ¯å¤©åšæŒä¸€
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æœ¬å‘¨å€¼å¾—è¯»(2016.12.19-2016.12.23)</title>
    <link href="http://rsarxiv.github.io/2016/12/25/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-19-2016-12-23/"/>
    <id>http://rsarxiv.github.io/2016/12/25/æœ¬å‘¨å€¼å¾—è¯»-2016-12-19-2016-12-23/</id>
    <published>2016-12-25T18:31:57.000Z</published>
    <updated>2016-12-25T18:39:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Machine-Reading-with-Background-Knowledge"><a href="#Machine-Reading-with-Background-Knowledge" class="headerlink" title="Machine Reading with Background Knowledge"></a><a href="http://t.cn/RIx3EjP" target="_blank" rel="external">Machine Reading with Background Knowledge</a></h2><p>ã€è¯­ä¹‰ç†è§£ã€‘åœ¨ç†è§£ä¸€å¥è¯çš„æ—¶å€™é€šå¸¸æ˜¯ç›´æ¥åˆ†æè¯¥å¥è¯ï¼Œè€Œæ²¡æœ‰å€ŸåŠ©å…¶ä»–å¤–éƒ¨çš„çŸ¥è¯†ï¼Œæ‰€ä»¥å¸¸å¸¸ä¼šäº§ç”Ÿä¸€äº›æ­§ä¹‰æˆ–è€…é”™è¯¯ã€‚æœ¬æ–‡çš„æ€è·¯æ˜¯åœ¨åˆ†æä¸€å¥è¯æ—¶ï¼Œå€ŸåŠ©ä¸€äº›èƒŒæ™¯çŸ¥è¯†æ¥è¿›è¡Œè¾…åŠ©ï¼Œæ–‡ä¸­ç»™å‡ºäº†ä¸¤ä¸ªä»»åŠ¡ï¼Œä¸€ä¸ªæ˜¯å¥æ³•åˆ†æä¸­çš„ä»‹è¯çŸ­è¯­æ¶ˆæ­§ï¼Œä¸€ä¸ªæ˜¯åè¯çŸ­è¯­çš„å…³ç³»æŠ½å–ï¼Œéƒ½å–å¾—äº†æ˜æ˜¾çš„æ•ˆæœã€‚æœ¬æ–‡ä½œè€…åŒ…æ‹¬äº†ã€Šæœºå™¨å­¦ä¹ ã€‹çš„ä½œè€…Tom M. Mitchellæ•™æˆã€‚å•çº¯åœ°åŸºäºç»Ÿè®¡æ–¹æ³•æ¥åšå¥æ³•åˆ†ææˆ–è€…è¯­ä¹‰è§’è‰²æ ‡æ³¨ç¡®å®ä¼šé‡åˆ°ä¸€äº›ç“¶é¢ˆï¼Œå€ŸåŠ©å¤–éƒ¨çš„èƒŒæ™¯çŸ¥è¯†æ˜¯ä¸€ä¸ªä¸é”™çš„æ€è·¯ã€‚éšç€æœ¬æ–‡ä¸€èµ·è¿˜å¼€æ”¾äº†ä¸€ä¸ªæ•°æ®é›† Prepositional Phrase Attachment Ambiguity (PPA) dataset <a href="http://t.cn/RIx1lEm" target="_blank" rel="external">http://t.cn/RIx1lEm</a></p>
<h2 id="A-User-Simulator-for-Task-Completion-Dialogues"><a href="#A-User-Simulator-for-Task-Completion-Dialogues" class="headerlink" title="A User Simulator for Task-Completion Dialogues"></a><a href="http://t.cn/RI9czrW" target="_blank" rel="external">A User Simulator for Task-Completion Dialogues</a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘æœ¬æ–‡ç ”ç©¶çš„é—®é¢˜éå¸¸æœ‰ç”¨ï¼Œäººäººéƒ½åœ¨åšchatbotï¼Œå´è‹¦äºæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œç”¨æˆ·æ¨¡æ‹Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„æ€è·¯ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æ¥è®­ç»ƒchatbotçš„æ–¹æ³•ï¼Œæ–‡ä¸­ç»™å‡ºäº†æ¨¡æ‹Ÿå™¨çš„è®¾è®¡å’Œéƒ¨åˆ†ä»£ç ï¼Œæ¶‰åŠåˆ°çš„é¢†åŸŸåŒ…æ‹¬æ‰¾ç”µå½±å’Œè®¢ç”µå½±ç¥¨ã€‚è™½ç„¶æ•ˆæœæœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œä½†æ˜¯ä¸ªä¸é”™çš„å°è¯•ã€‚æ¨èç»™ç ”ç©¶å’Œå¼€å‘chatbotçš„ç«¥é‹ã€‚æºä»£ç ä¹ŸåŒæ—¶å¼€æ”¾äº†ï¼Œåœ°å€ <a href="http://t.cn/RICfMSB" target="_blank" rel="external">http://t.cn/RICfMSB</a> æ„Ÿå…´è¶£çš„ç«¥é‹å¯ä»¥ç ”ç©¶ä¸‹ã€‚</p>
<h2 id="Reducing-Redundant-Computations-with-Flexible-Attention"><a href="#Reducing-Redundant-Computations-with-Flexible-Attention" class="headerlink" title="Reducing Redundant Computations with Flexible Attention"></a><a href="http://t.cn/RI9f3Bx" target="_blank" rel="external">Reducing Redundant Computations with Flexible Attention</a></h2><p>ã€æ³¨æ„åŠ›æ¨¡å‹ä¼˜åŒ–ã€‘æ³¨æ„åŠ›å·²ç»æ˜¯ä¸€ä¸ªåº”ç”¨æ¯”è¾ƒå¹¿æ³›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæœ¬æ–‡å¯¹decodingè¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæå‡ºäº†ä¸€ç§Flexibleæ³¨æ„åŠ›æ¨¡å‹ï¼Œåœ¨æ¯ä¸€æ­¥è§£ç æ—¶éƒ½ä¼šé€šè¿‡ä¸€ä¸ªæƒ©ç½šå‡½æ•°æ¥è¿‡æ»¤æ‰ä¸€äº›ä¸é‡è¦çš„encoder unitï¼Œä»è€Œé™ä½è®¡ç®—é‡ã€‚ </p>
<h2 id="Improving-Tweet-Representations-using-Temporal-and-User-Context"><a href="#Improving-Tweet-Representations-using-Temporal-and-User-Context" class="headerlink" title="Improving Tweet Representations using Temporal and User Context"></a><a href="http://t.cn/RI9I8LS" target="_blank" rel="external">Improving Tweet Representations using Temporal and User Context</a></h2><p>ã€ç”¨æˆ·ç”»åƒã€‘æœ¬æ–‡åœ¨å¯¹tweetè¿›è¡Œè¡¨ç¤ºå­¦ä¹ æ—¶ï¼Œé€šè¿‡å¼•å…¥ç”¨æˆ·timelineä¸Šç›¸é‚»çš„tweetsæ¥æé«˜å‡†ç¡®åº¦ã€‚</p>
<h2 id="Automatic-Generation-of-Grounded-Visual-Questions"><a href="#Automatic-Generation-of-Grounded-Visual-Questions" class="headerlink" title="Automatic Generation of Grounded Visual Questions"></a><a href="http://t.cn/RIKmwoo" target="_blank" rel="external">Automatic Generation of Grounded Visual Questions</a></h2><p>ã€VQAã€‘ã€é—®é¢˜ç”Ÿæˆã€‘å¯è§†åŒ–é—®ç­”æ˜¯ä¸ªå¾ˆæœ‰æ„æ€çš„ä¸œè¥¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸å›¾ç‰‡å†…å®¹ç›¸å…³çš„é—®é¢˜ï¼Œæœ‰ä¸€ç‚¹image captionçš„æ„æ€ï¼Œåªæ˜¯è¯´è¿™é‡Œç”¨æ¥æé—®ã€‚æ„Ÿå…´è¶£çš„ç«¥é‹å¯ä»¥å…³æ³¨ä¸€ä¸‹ã€‚ </p>
<h2 id="CLEVR-A-Diagnostic-Dataset-for-Compositional-Language-and-Elementary-Visual-Reasoning"><a href="#CLEVR-A-Diagnostic-Dataset-for-Compositional-Language-and-Elementary-Visual-Reasoning" class="headerlink" title="CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"></a><a href="http://t.cn/RICIat8" target="_blank" rel="external">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</a></h2><p>ã€VQAã€‘ã€æ•°æ®ç¦åˆ©ã€‘Li Feifeiç»„å‘å¸ƒçš„ä¸€ç»„VQAæ•°æ®é›†ï¼Œ100kè§„æ¨¡çš„å›¾ç‰‡é›†ï¼Œå€¼å¾—å…³æ³¨ï¼æ–‡ä¸­æåˆ°æ•°æ®å’Œç›¸å…³çš„å¤„ç†ä»£ç è¿‘æœŸä¼šå…¬å¼€ã€‚</p>
<h2 id="Fast-Domain-Adaptation-for-Neural-Machine-Translation"><a href="#Fast-Domain-Adaptation-for-Neural-Machine-Translation" class="headerlink" title="Fast Domain Adaptation for Neural Machine Translation"></a><a href="http://t.cn/RICJro8" target="_blank" rel="external">Fast Domain Adaptation for Neural Machine Translation</a></h2><p>ã€æœºå™¨ç¿»è¯‘ã€‘ã€è¿ç§»å­¦ä¹ ã€‘æœ¬æ–‡çš„å·¥ä½œæ˜¯å°†æŸä¸€ä¸ªé¢†åŸŸä¸­è®­ç»ƒå¥½çš„æ¨¡å‹ä»¥æœ€ä½çš„ä»£ä»·è¿ç§»åˆ°é¢†åŸŸå¤–ï¼ŒåŒæ—¶ä¿è¯é¢†åŸŸå†…å’Œé¢†åŸŸå¤–éƒ½æœ‰ä¸é”™çš„æ•ˆæœã€‚å…·ä½“çš„æ€è·¯æ˜¯ï¼šå…ˆè®­ç»ƒå‡ºä¸€ä¸ªä¸é”™çš„baseline modelï¼Œç„¶ååœ¨baselineçš„åŸºç¡€ä¸Šä½¿ç”¨é¢†åŸŸå¤–çš„å°‘é‡æ•°æ®è¿›è¡Œå‡ ä¸ªå›åˆçš„è®­ç»ƒï¼Œå¾—åˆ°ä¸€ä¸ªcontinue modelï¼Œç„¶åå°†baselineå’Œcontinueè¿›è¡Œmixï¼Œå¾—åˆ°æœ€ç»ˆçš„modelã€‚</p>
<h2 id="A-Context-aware-Attention-Network-for-Interactive-Question-Answering"><a href="#A-Context-aware-Attention-Network-for-Interactive-Question-Answering" class="headerlink" title="A Context-aware Attention Network for Interactive Question Answering"></a><a href="http://t.cn/RINpcT9" target="_blank" rel="external">A Context-aware Attention Network for Interactive Question Answering</a></h2><p>ã€äº¤äº’å¼QAã€‘æœ¬æ–‡çš„å·¥ä½œäº®ç‚¹åœ¨äºåšé—®ç­”æ—¶æä¾›äº†ä¸€ç§äº¤äº’æœºåˆ¶ï¼Œå½“answeræ¨¡å—è§‰å¾—ç°æœ‰çš„ä¿¡æ¯æ— æ³•å›ç­”questionçš„è¯ï¼Œä¼šç”Ÿæˆä¸€ä¸ªæ›´åŠ æ·±å…¥çš„é—®é¢˜ç»™ç”¨æˆ·ï¼Œé€šè¿‡å­¦ä¹ ç”¨æˆ·çš„åé¦ˆæ¥ç”Ÿæˆç­”æ¡ˆã€‚</p>
<h2 id="ä¸­æ–‡ä¿¡æ¯å¤„ç†å‘å±•æŠ¥å‘Š"><a href="#ä¸­æ–‡ä¿¡æ¯å¤„ç†å‘å±•æŠ¥å‘Š" class="headerlink" title="ä¸­æ–‡ä¿¡æ¯å¤„ç†å‘å±•æŠ¥å‘Š"></a><a href="http://t.cn/RINHLN8" target="_blank" rel="external">ä¸­æ–‡ä¿¡æ¯å¤„ç†å‘å±•æŠ¥å‘Š</a></h2><p>ä¸­å›½ä¸­æ–‡ä¿¡æ¯å­¦ä¼šå‘å¸ƒ2016å¹´ã€Šä¸­æ–‡ä¿¡æ¯å¤„ç†å‘å±•æŠ¥å‘Šã€‹ï¼Œå€¼å¾—ä¸€è¯»ï¼</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Machine-Reading-with-Background-Knowledge&quot;&gt;&lt;a href=&quot;#Machine-Reading-with-Background-Knowledge&quot; class=&quot;headerlink&quot; title=&quot;Machine Re
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬åä¹æœŸ</title>
    <link href="http://rsarxiv.github.io/2016/12/23/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B9%9D%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/23/PaperWeekly-ç¬¬åä¹æœŸ/</id>
    <published>2016-12-23T18:40:03.000Z</published>
    <updated>2016-12-23T18:56:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•"><a href="#å¼•" class="headerlink" title="å¼•"></a>å¼•</h1><p>æœ¬æœŸçš„PaperWeeklyä¸€å…±åˆ†äº«å››ç¯‡æœ€è¿‘arXivä¸Šå‘å¸ƒçš„é«˜è´¨é‡paperï¼ŒåŒ…æ‹¬ï¼šæƒ…æ„Ÿåˆ†æã€æœºå™¨é˜…è¯»ç†è§£ã€çŸ¥è¯†å›¾è°±ã€æ–‡æœ¬åˆ†ç±»ã€‚äººå·¥æ™ºèƒ½åŠå…¶ç›¸å…³ç ”ç©¶æ—¥æ–°æœˆå¼‚ï¼Œæœ¬æ–‡å°†å¸¦ç€å¤§å®¶äº†è§£ä¸€ä¸‹ä»¥ä¸Šå››ä¸ªç ”ç©¶æ–¹å‘éƒ½æœ‰å“ªäº›æœ€æ–°è¿›å±•ã€‚å››ç¯‡paperåˆ†åˆ«æ˜¯ï¼š</p>
<p>1ã€Linguistically Regularized LSTMs for Sentiment Classification, 2016.11<br>2ã€End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension, 2016.10<br>3ã€Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples, 2016.10<br>4ã€AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification, 2016.11</p>
<h1 id="Linguistically-Regularized-LSTMs-for-Sentiment-Classification"><a href="#Linguistically-Regularized-LSTMs-for-Sentiment-Classification" class="headerlink" title="Linguistically Regularized LSTMs for Sentiment Classification"></a><a href="https://arxiv.org/pdf/1611.03949v1.pdf" target="_blank" rel="external">Linguistically Regularized LSTMs for Sentiment Classification</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Qiao Qian, Minlie Huang, Xiaoyan Zhu</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>sentiment classification, neural network models, linguistically coherent representations,</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016.11</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>åˆ©ç”¨è¯­è¨€èµ„æºå’Œç¥ç»ç½‘ç»œç›¸ç»“åˆæ¥æå‡æƒ…æ„Ÿåˆ†ç±»é—®é¢˜çš„ç²¾åº¦</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>åœ¨LSTMå’ŒBi-LSTMæ¨¡å‹çš„åŸºç¡€ä¸ŠåŠ å…¥å››ç§è§„åˆ™çº¦æŸï¼Œè¿™å››ç§è§„åˆ™åˆ†åˆ«æ˜¯: Non-Sentiment Regularizer,Sentiment Regularizer, Negation Regularizer, Intensity Regularizer.å› æ­¤ï¼Œæ–°çš„loss functionå˜ä¸º:</p>
<p><img src="media/eqn.png" alt="eqn"></p>
<p>ä¸åŒçš„è§„åˆ™çº¦æŸå¯¹åº”ä¸åŒçš„Lå‡½æ•°</p>
<h2 id="èµ„æº"><a href="#èµ„æº" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€Movie Review (MR) <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><br>2ã€Stanford Sentiment Tree- bank (SST) <a href="http://nlp.stanford.edu/sentiment/treebank.html" target="_blank" rel="external">http://nlp.stanford.edu/sentiment/treebank.html</a></p>
<h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>1ã€Neural Networks for Sentiment Classification<br><a href="https://arxiv.org/abs/1412.3555" target="_blank" rel="external">Empirical evaluation of gated recurrent neural networks on sequence modeling</a><br><a href="https://pdfs.semanticscholar.org/5807/664af8e63d5207f59fb263c9e7bd3673be79.pdf" target="_blank" rel="external">Hybrid speech recognition with deep bidirectional lstm</a><br>2ã€Applying Linguistic Knowledge for Sentiment Classification<br><a href="http://www.site.uottawa.ca/~diana/publications/ci.pdf" target="_blank" rel="external">Sentiment classification of movie reviews using contextual valence shifters</a></p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¯­è¨€èµ„æºçº¦æŸå’ŒLSTM/Bi-LSTMçš„æ¨¡å‹ç”¨äºæƒ…æ„Ÿåˆ†ç±»ï¼Œå¹¶é€šè¿‡åœ¨MRå’ŒSSTæ•°æ®é›†ä¸Šçš„å®éªŒå’Œå¯¹RNN/RNTN,LSTM,Tree-LSTM,CNNçš„æ•ˆæœå¯¹æ¯”è¯æ˜äº†è¿™ä¸€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæœ¬æ–‡è¿˜åŸºäºä¸åŒçš„çº¦æŸè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜çš„ä¸åŒçš„çº¦æŸåœ¨æé«˜åˆ†ç±»ç²¾åº¦ä¸Šçš„ä½œç”¨ã€‚æœ¬æ–‡å®éªŒä¸°å¯Œï¼Œæ•ˆæœçš„æå‡è™½ä¸æ˜¾è‘—ï¼Œä½†æ–°çš„æ¨¡å‹ç¡®å®åœ¨ä¸åŒç¨‹åº¦ä¸Šå…‹æœäº†æ—§æ¨¡å‹çš„ä¸€äº›ä¸è¶³ã€‚</p>
<h1 id="End-to-End-Answer-Chunk-Extraction-and-Ranking-for-Reading-Comprehension"><a href="#End-to-End-Answer-Chunk-Extraction-and-Ranking-for-Reading-Comprehension" class="headerlink" title="End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension"></a><a href="https://arxiv.org/pdf/1610.09996v2.pdf" target="_blank" rel="external">End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, Bowen Zhou</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>IBM Watson</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Reading Comprehension, Chunk extraction, Ranking</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016.10</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>é’ˆå¯¹ç­”æ¡ˆéå®šé•¿çš„é˜…è¯»ç†è§£ä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºäº†DCRï¼ˆdynamic chunk readerï¼‰æ¨¡å‹<br>æ¥ä»ç»™å®šçš„æ–‡æ¡£ä¸­æŠ½å–å¯èƒ½çš„å€™é€‰ç­”æ¡ˆå¹¶è¿›è¡Œæ’åºã€‚</p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æœ¬æ–‡æå‡ºçš„æ¨¡å‹ç»“æ„å…±åˆ†ä¸ºå››éƒ¨åˆ†ï¼Œ<br>1ã€Encoder Layer<br>å¦‚å›¾æ‰€ç¤ºï¼Œè¿™éƒ¨åˆ†æ˜¯ç”¨åŒå‘GRUåˆ†åˆ«å¯¹æ–‡æ¡£ï¼ˆPassageï¼‰å’Œé—®é¢˜ï¼ˆQuestionï¼‰è¿›è¡Œç¼–ç ã€‚<br>2ã€Attention Layer<br>è¯¥å±‚é‡‡ç”¨çš„æ–¹æ³•ä¸ç›¸å…³å·¥ä½œä¸­çš„mLSTMç±»ä¼¼ï¼Œæ–‡æ¡£æ¯ä¸ªæ—¶åˆ»çš„çŠ¶æ€h<sub>j</sub><sup>p</sup>éƒ½ä¸é—®é¢˜ä¸­çš„æ¯ä¸ªçŠ¶æ€h<sub>k</sub><sup>q</sup>è¿›è¡ŒåŒ¹é…å¾—åˆ°ä¸€ä¸ªæƒé‡å‘é‡Î±<sub>k</sub>ï¼Œç„¶åå†æ ¹æ®è¯¥æƒé‡å‘é‡å¯¹é—®é¢˜çš„GRUéšå±‚è¾“å‡ºh<sup>p</sup>è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æ–‡æ¡£ä¸­è¯¥æ—¶åˆ»çŠ¶æ€h<sub>j</sub><sup>p</sup>å¯¹åº”çš„ä¸Šä¸‹æ–‡å‘é‡Î²<sub>j</sub>ï¼Œä¸¤ä¸ªå‘é‡h<sub>j</sub><sup>p</sup>å’ŒÎ²<sub>j</sub>æ‹¼æ¥åœ¨ä¸€èµ·ä½œä¸ºè¯¥æ—¶åˆ»æ–°çš„è¡¨ç¤ºv<sub>j</sub>ã€‚æœ€åå†å°†ä¸Šè¿°ä¸é—®é¢˜ç›¸å…³çš„æ–°æ–‡æ¡£è¡¨ç¤ºvé€šè¿‡åŒå‘GRUï¼Œå¾—åˆ°æ–‡æ¡£æœ€ç»ˆçš„è¡¨ç¤ºÎ³ã€‚<br><img src="media/DCR.png" alt="DC"></p>
<p>3ã€Chunk-Representation Layer<br>ä¸Šä¸€éƒ¨åˆ†è·å¾—äº†ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£è¡¨ç¤ºÎ³ï¼Œé‚£ä¹ˆè¿™éƒ¨åˆ†åˆ™æ˜¯è€ƒè™‘å¦‚ä½•æŠ½å–å€™é€‰ç­”æ¡ˆï¼Œå¹¶è·å¾—å€™é€‰ç­”æ¡ˆçš„è¡¨ç¤ºå‘é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å€™é€‰ç­”æ¡ˆæŠ½å–æ–¹æ³•ï¼Œç¬¬ä¸€ç§æ–¹æ³•æ˜¯æŠ½å–æ‰€æœ‰æ»¡è¶³è®­ç»ƒæ•°æ®ä¸­ç­”æ¡ˆå¯¹åº”è¯æ€§æ ‡æ³¨æ¨¡å¼çš„å€™é€‰é¡¹ï¼Œç¬¬äºŒç§æ–¹æ³•åˆ™æ˜¯ç®€å•ç²—æš´åœ°ç¡®å®šä¸€ä¸ªå€™é€‰é¡¹æœ€å¤§é•¿åº¦ï¼Œç„¶åéå†æ‰€æœ‰å¯èƒ½çš„å€™é€‰é¡¹ã€‚è‡³äºå€™é€‰ç­”æ¡ˆçš„è¡¨ç¤ºæ–¹å¼ï¼Œæœ¬æ–‡å°†å€™é€‰ç­”æ¡ˆå‰å‘GRUçš„æœ€åä¸€ä¸ªæ—¶åˆ»çŠ¶æ€å’Œåå‘GRUç¬¬ä¸€ä¸ªæ—¶åˆ»çŠ¶æ€æ‹¼æ¥åœ¨ä¸€èµ·ä½œä¸ºæœ€ç»ˆå€™é€‰é¡¹çš„è¡¨ç¤ºã€‚<br>4ã€Ranker Layer<br>å·²ç»è·å¾—äº†æ‰€æœ‰å€™é€‰é¡¹çš„è¡¨ç¤ºï¼Œé‚£ä¹ˆæ¥ç€å°±æ˜¯å¯¹æ‰€æœ‰å€™é€‰é¡¹è¿›è¡Œæ‰“åˆ†æ’åºã€‚æœ¬æ–‡ä¸­æ‰“åˆ†æ˜¯é‡‡ç”¨é—®é¢˜çš„è¡¨ç¤ºå’Œå€™é€‰é¡¹çš„è¡¨ç¤ºè®¡ç®—å†…ç§¯çš„æ–¹å¼å¾—åˆ°çš„ï¼Œæœ¬æ–‡è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰é‡‡ç”¨å¸¸è§äºæ’åºä»»åŠ¡çš„Margin ranking lossï¼Œè€Œæ˜¯å…ˆç”¨softmaxå¯¹æ‰€æœ‰å€™é€‰é¡¹è®¡ç®—ä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œç„¶åé‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚</p>
<p>æœ¬æ–‡åœ¨SQuADæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œæå‡ºçš„æ–¹æ³•æ•ˆæœæ¯”ä¹‹å‰ä¸¤ç¯‡SQuADç›¸å…³paperçš„æ–¹æ³•æœ‰è¾ƒå¤§çš„æå‡ã€‚</p>
<h2 id="èµ„æº-1"><a href="#èµ„æº-1" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€SQuAD <a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="external">https://rajpurkar.github.io/SQuAD-explorer/</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-1"><a href="#ç›¸å…³å·¥ä½œ-1" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>1ã€æ•°æ®é›†ç›¸å…³è®ºæ–‡<br>SQuAD: 100,000+ Questions for Machine Comprehension of Text<br>2ã€æ¨¡å‹ç›¸å…³è®ºæ–‡<br>MACHINE COMPREHENSION USING MATCH-LSTM</p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>åœ¨å¯¹æ–‡æ¡£å’Œé—®é¢˜ç¼–ç é˜¶æ®µï¼Œæœ¬ç¯‡è®ºæ–‡æå‡ºçš„æ¨¡å‹ä¸ä¹‹å‰mLSTMé‚£ç¯‡paperæœ‰äº›ç›¸ä¼¼ã€‚ä¸¤ç¯‡è®ºæ–‡ä¸­æ¨¡å‹çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼šmLSTMé‚£ç¯‡è®ºæ–‡é‡‡ç”¨é¢„æµ‹èµ·å§‹ã€ç»ˆæ­¢ä½ç½®çš„æ–¹æ³•æ¥ç¡®å®šç­”æ¡ˆï¼Œè€Œæœ¬æ–‡åˆ™æ˜¯å…ˆé‡‡ç”¨ä¸€äº›è§„åˆ™æˆ–Patternçš„æ–¹æ³•æ¥æŠ½å–ä¸€äº›å€™é€‰ç­”æ¡ˆï¼Œç„¶åå†å¯¹å€™é€‰ç­”æ¡ˆè¿›è¡Œæ’åºã€‚</p>
<h2 id="è”ç³»æ–¹å¼"><a href="#è”ç³»æ–¹å¼" class="headerlink" title="è”ç³»æ–¹å¼"></a>è”ç³»æ–¹å¼</h2><p>æœ‰DLæˆ–è€…NLPç›¸å…³è¯é¢˜ï¼Œæ¬¢è¿è®¨è®ºã€‚destin.bxwang@gmail.com </p>
<h1 id="Knowledge-will-Propel-Machine-Understanding-of-Content-Extrapolating-from-Current-Examples"><a href="#Knowledge-will-Propel-Machine-Understanding-of-Content-Extrapolating-from-Current-Examples" class="headerlink" title="Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples"></a><a href="https://arxiv.org/abs/1610.07708?from=groupmessage&amp;isappinstalled=0" target="_blank" rel="external">Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Amit Sheth, Sujan Perera, and Sanjaya Wijeratne</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Kno.e.sis Center, Wright State University Dayton, Ohio, USA</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Semantic analysis of multimodal dataï¼ŒMachine intelligence,Understanding complex textï¼ŒEmojiNet</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016.10</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>åˆ©ç”¨çŸ¥è¯†å’Œå¤šæ¨¡æ€æ•°æ®æ¥è§£å†³ç‰¹å®šæƒ…å†µä¸‹çš„å¤æ‚æ–‡æœ¬çš„æ·±å±‚ç†è§£é—®é¢˜</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>1ã€ç°çŸ¥è¯†åº“åœ¨å¤„ç†ç‰¹å®šé¢†åŸŸé—®é¢˜ä¸­çš„å±€é™æ€§åŠè§£å†³æ–¹æ³•<br>ï¼ˆ1ï¼‰çŸ¥è¯†åº“çš„æ‚ä¹±<br>è§£å†³æ–¹æ³•ï¼šé‡‡ç”¨è‡ªåŠ¨åˆ¤åˆ«æŠ€æœ¯ï¼Œé¢†åŸŸçŸ¥è¯†åº“ç´¢å¼•æŠ€æœ¯ï¼Œåˆ©ç”¨å®ä½“å’Œå…³ç³»çš„è¯­ä¹‰å»åˆ¤åˆ«æ‰€ç»™å®šçŸ¥è¯†åº“é¢†åŸŸä¸­çš„ç›¸å…³éƒ¨åˆ†ã€‚<br>ï¼ˆ2ï¼‰çŸ¥è¯†åº“æ•°æ®çš„ä¸å®Œå¤‡å’Œä¸å……è¶³<br>è§£å†³æ–¹æ³•ï¼šä½¿ç”¨ human-in-the-loopæ¨¡å‹åœ¨çœŸå®çš„ä¸´åºŠæ•°æ®å’Œå·²æœ‰çš„çŸ¥è¯†åº“ä¸­å»å‘ç°æ›´å¤šçš„å®ä½“ä¸å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚<br>ï¼ˆ3ï¼‰çŸ¥è¯†è¡¨ç¤ºæŠ€æœ¯å’Œæ¨ç†æŠ€æœ¯çš„å±€é™æ€§<br>è§£å†³æ–¹æ³•ï¼šåœ¨å•ä¸ªå±æ€§çš„è¡¨ç¤ºä¸­åŠ å…¥äº†ä¸‰å…ƒç»„å’Œè½¯é€»è¾‘çš„è§£é‡Šèƒ½åŠ›åŠå…¶ç›¸å…³æ¦‚ç‡å€¼å’Œç†ç”±ã€‚</p>
<p>2ã€æ–°çš„ç ”ç©¶åº”ç”¨<br>ï¼ˆ1ï¼‰éšå®ä½“é“¾æ¥<br>ï¼ˆ2ï¼‰è¡¨æƒ…ç¬¦å·è¯­ä¹‰æ¶ˆæ­§<br>ï¼ˆ3ï¼‰ç†è§£å’Œåˆ†æwebè®ºå›ä¸­å…³äºè¯ç‰©æ»¥ç”¨çš„ç›¸å…³è®¨è®º<br>åˆ©ç”¨ç›¸å…³èƒŒæ™¯çŸ¥è¯†åŠ å¼ºä¸åŒç§ç±»ä¿¡æ¯çš„ä¿¡æ¯æŠ½å–æ¨¡å‹<br><img src="media/img1.png" alt="img1"></p>
<p>3ã€åœ¨å¥åº·é¢†åŸŸä¸­çš„æ–‡æœ¬ç†è§£æ¨¡å‹<br><img src="media/img2.png" alt="img2"></p>
<p>4ã€ä½¿ç”¨æ„ŸçŸ¥å™¨å’Œæ–‡æœ¬èµ„æ–™äº†è§£åŸå¸‚äº¤é€šæƒ…å†µ<br>(1)äº¤é€šé¢†åŸŸçš„æ¦‚å¿µå…³ç³»ç½‘æ¨¡å‹<br>(2)æ¦‚ç‡å›¾æ¨¡å‹<br><img src="media/img3.png" alt="img3"></p>
<p>ä½¿ç”¨é¢†åŸŸçŸ¥è¯†å…³è”ä¸åŒæ¨¡æ€ä¸‹çš„ä¸Šä¸‹æ–‡ç›¸å…³æ•°æ®<br><img src="media/img4.png" alt="img4"></p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡ä¸»è¦ä¸¾ä¾‹è¯´æ˜äº†çŸ¥è¯†å°†æ¨åŠ¨æœºå™¨å¯¹å†…å®¹çš„ç†è§£ã€‚æ€»ä½“æ¥çœ‹æœ¬æ–‡åƒä¸€ç¯‡ç»¼è¿°æ€§çš„æ–‡ç« ï¼Œç»™å‡ºäº†åœ¨çŸ¥è¯†åº“åˆ›å»ºè¿‡ç¨‹ä¸­æ‰€é‡åˆ°çš„é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä»¥å®é™…æ¡ˆä¾‹æ¥é˜è¿°çŸ¥è¯†åœ¨æˆ‘ä»¬å®é™…é—®é¢˜ä¸­åº”ç”¨ã€‚</p>
<h1 id="AC-BLSTM-Asymmetric-Convolutional-Bidirectional-LSTM-Networks-for-Text-Classification"><a href="#AC-BLSTM-Asymmetric-Convolutional-Bidirectional-LSTM-Networks-for-Text-Classification" class="headerlink" title="AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification"></a><a href="https://arxiv.org/pdf/1611.01884v2.pdf" target="_blank" rel="external">AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification</a></h1><h2 id="ä½œè€…-3"><a href="#ä½œè€…-3" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Depeng Liang and Yongdong Zhang</p>
<h2 id="å•ä½-3"><a href="#å•ä½-3" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Guangdong Province Key Laboratory of Computational Science, School of Data and<br>Computer Science, Sun Yat-sen University, Guang Zhou, China</p>
<h2 id="å…³é”®è¯-3"><a href="#å…³é”®è¯-3" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>ACNN; BLSTM; Text Classification</p>
<h2 id="æ–‡ç« æ¥æº-3"><a href="#æ–‡ç« æ¥æº-3" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016.11</p>
<h2 id="é—®é¢˜-3"><a href="#é—®é¢˜-3" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ·±åº¦å­¦ä¹ çš„æ¨¡å‹â€“AC-BLSTMçš„æ¨¡å‹ï¼ˆå³ï¼šå°†ACNNå’ŒBLSTMç»„åˆåœ¨ä¸€èµ·ï¼‰ï¼Œç”¨äºå¥å­å’Œæ–‡ç« å±‚é¢çš„åˆ†ç±»ã€‚</p>
<h2 id="æ¨¡å‹-3"><a href="#æ¨¡å‹-3" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>AC-BLSTMæ¨¡å‹å¯ä»¥åˆ†æˆå››ä¸ªéƒ¨åˆ†,å¦‚Figure 1æ‰€ç¤ºï¼š<br>1.è¾“å…¥: è¾“å…¥æ˜¯ä¸€ä¸ªsentenceï¼Œä½¿ç”¨ ( L <em> d )çš„çŸ©é˜µè¡¨ç¤ºï¼Œå…¶ä¸­Lè¡¨ç¤ºå¥å­ä¸­çš„Lä¸ªè¯ï¼Œdè¡¨ç¤ºæ¯ä¸ªè¯çš„è¯å‘é‡çš„ç»´åº¦<br>2.ACNN(Asymmetric CNN): ä¼ ç»Ÿçš„CNNé‡‡ç”¨çš„æ˜¯ ( k </em> d ) å¤§å°çš„filterï¼ŒACNNåˆ™æŠŠfilterçš„è¿‡ç¨‹åˆ†æˆ ( 1 <em> d ) å’Œ ( k </em> 1 ) çš„ä¸¤ä¸ªè¿‡ç¨‹ï¼Œç›¸å½“äºæ˜¯æŠŠ ( k <em> d ) çš„filteråšå› å¼åˆ†è§£ã€‚<br>è¿™ä¸€å±‚çš„è¾“å…¥æ˜¯ä¸€ä¸ª ( L </em> d ) çš„çŸ©é˜µï¼Œå¯¹äºnä¸ªå°ºåº¦ä¸º( 1 <em> d ) å’Œ( ki </em> 1 )çš„å·ç§¯å±‚çš„è¾“å‡ºæ˜¯ä¸€ä¸ª [ (L - ki + 1) <em> n ]çš„çŸ©é˜µï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ¬æ–‡é‡‡ç”¨äº†3ç§ä¸åŒçš„å·ç§¯æ ¸ï¼Œæ‰€ä»¥è¾“å‡ºæ˜¯3ç§ä¸åŒçš„[ (L - ki + 1) </em> n ]çš„çŸ©é˜µï¼ˆå›¾ä¸­ä¸€ä¸ªå½©è‰²çš„å°æ–¹å—è¡¨ç¤º (1 * n)çš„å‘é‡ï¼‰<br>3.è¿æ¥å±‚: ä¸ºäº†ç»™BLSTMæ„é€ è¾“å…¥ï¼Œè¿æ¥å±‚å°†3ç§ä¸åŒå·ç§¯å±‚çš„è¾“å‡ºï¼Œä»¥Ct^iè¡¨ç¤ºç¬¬1ç§å·ç§¯å±‚ä¸ºLSTMç¬¬tä¸ªtime stepè´¡çŒ®çš„è¾“å…¥ï¼Œåˆ™LSTMç½‘ç»œçš„ç¬¬tæ­¥è¾“å…¥Ct = [Ct^1, Ct^2, Ct^3]ï¼Œå…¶ä¸­tå±äº{1,2,â€¦,L-K+1}, K = max{ki}<br>4.BLSTM: LSTMèƒ½å¤Ÿå¾ˆå¥½çš„è§£å†³long time delay å’Œlong range contextçš„é—®é¢˜ï¼Œä½†å…¶å¤„ç†æ˜¯å•å‘çš„ï¼Œè€ŒBLSTMèƒ½å¤Ÿè§£å†³given pointçš„åŒè¾¹çš„ä¾èµ–å…³ç³»ï¼Œå› æ­¤ï¼Œæœ¬æ–‡é€‰æ‹©äº†BLSTMç½‘ç»œå±‚æ¥å­¦ä¹ ACNNè¾“å…¥çš„ç‰¹å¾çš„dependencies<br>5.Softmaxå±‚: ä¸ºäº†åº”ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œæœ¬æ–‡åœ¨æœ€åä½¿ç”¨å…¨è¿æ¥å±‚å’Œsoftmaxå‡½æ•°æ¥å®ç°åˆ†ç±»ã€‚<br><img src="media/Figure1.jpg" alt="Figure1"></p>
<h2 id="èµ„æº-2"><a href="#èµ„æº-2" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>æ–‡ç« ä¸­ä½¿ç”¨çš„æ•°æ®é›†<br>1ã€SST-1 <a href="http://nlp.stanford.edu/sentiment/index.html" target="_blank" rel="external">http://nlp.stanford.edu/sentiment/index.html</a><br>2ã€SST-2 <a href="http://nlp.stanford.edu/sentiment/index.html" target="_blank" rel="external">http://nlp.stanford.edu/sentiment/index.html</a><br>3ã€Movie Review(MR) <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><br>4ã€SUBJ <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a><br>5ã€TREC <a href="http://cogcomp.cs.illinois.edu/Data/QA/QC/" target="_blank" rel="external">http://cogcomp.cs.illinois.edu/Data/QA/QC/</a><br>6ã€YELP13 <a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="external">https://www.yelp.com/dataset_challenge</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-2"><a href="#ç›¸å…³å·¥ä½œ-2" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>1ã€Yoon Kimäº2014å¹´åœ¨<a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" rel="external"><strong>Convolutional neural networks for sentence classification</strong></a>ä¸€æ–‡ä¸­æå‡ºå°†è¯å‘é‡å’ŒCNNç»“åˆï¼Œç”¨äºå¥å­åˆ†ç±»çš„æ¨¡å‹ã€‚åœ¨è¯¥æ–‡ä¸­ï¼ŒKimå°†ä¸åŒé•¿åº¦çš„filterçš„ç»„åˆåœ¨ä¸€èµ·ï¼Œä¸”æå‡ºäº†staticæˆ–è€…å¯ä»¥fine-tuningçš„word embeddingæ¨¡å‹<br>2ã€Zhou et al.åˆ™äº2015å¹´åœ¨<a href="https://arxiv.org/abs/1511.08630" target="_blank" rel="external"><strong>A C-LSTM neural network for text classification</strong></a>ä¸€æ–‡ä¸­æå‡ºå°†CNNå’ŒLSTMå åŠ çš„æ¨¡å‹ï¼Œä¸”ä½¿ç”¨å›ºå®šçš„word embedding<br>3ã€Szegedy et al.äº2015å¹´åœ¨<a href="https://arxiv.org/pdf/1512.00567v3.pdf" target="_blank" rel="external"><strong>Rethinking the Inception Architecture for Computer Vision</strong></a>ä¸­æå‡ºäº†ACNNæ¨¡å‹ï¼Œè¿™å‡å°‘äº†å‚æ•°çš„ä¸ªæ•°ä¸”æé«˜äº†æ¨¡å‹çš„è¡¨å¾</p>
<h2 id="ç®€è¯„-3"><a href="#ç®€è¯„-3" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¿™ç¯‡æ–‡ç« ä¸»è¦è´¡çŒ®å°±æ˜¯æå‡ºäº†ä¸€ä¸ªAC-BSLTMçš„æ¨¡å‹ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œäº®ç‚¹å°±åœ¨äºï¼šACNNå¯ä»¥åœ¨å‡å°‘å‚æ•°çš„ä¸ªæ•°çš„åŒæ—¶é€šè¿‡å¢åŠ æ›´å¤šçš„éçº¿æ€§æ€§æ¥æé«˜è¡¨è¾¾èƒ½åŠ›ï¼Œè€ŒBLSTMèƒ½å¤Ÿæ•æ‰è¾“å…¥çš„ä¸¤ç«¯çš„ä¿¡æ¯ã€‚ä¸¤è€…çš„ç»“åˆå°±æé«˜äº†åˆ†ç±»çš„ç²¾åº¦ã€‚ä½†äº‹å®ä¸Šï¼Œè¿™ä¸¤ä¸ªç½‘ç»œæ¨¡å‹éƒ½æ˜¯ç°æœ‰çš„ï¼Œæœ¬æ–‡çš„å·¥ä½œæ„Ÿè§‰åªæ˜¯ä¸¤ä¸ªç½‘ç»œçš„è¿æ¥ï¼Œåœ¨æœ¬è´¨ä¸Šæ²¡æœ‰å¤ªå¤§çš„æ”¹è¿›ï¼Œä¸”åœ¨åˆ†ç±»ç²¾åº¦ä¸Šçš„æé«˜ä¹Ÿæ¯”è¾ƒæœ‰é™ã€‚</p>
<h1 id="è‡´è°¢"><a href="#è‡´è°¢" class="headerlink" title="è‡´è°¢"></a>è‡´è°¢</h1><p>æ„Ÿè°¢@æ–¹å˜‰å€© @destin wang å’Œ @min279 ä¸‰ä½ç«¥é‹çš„è¾›å‹¤å·¥ä½œã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•&quot;&gt;&lt;a href=&quot;#å¼•&quot; class=&quot;headerlink&quot; title=&quot;å¼•&quot;&gt;&lt;/a&gt;å¼•&lt;/h1&gt;&lt;p&gt;æœ¬æœŸçš„PaperWeeklyä¸€å…±åˆ†äº«å››ç¯‡æœ€è¿‘arXivä¸Šå‘å¸ƒçš„é«˜è´¨é‡paperï¼ŒåŒ…æ‹¬ï¼šæƒ…æ„Ÿåˆ†æã€æœºå™¨é˜…è¯»ç†è§£ã€çŸ¥è¯†å›¾è°±ã€æ–‡æœ¬åˆ†ç±»ã€‚äººå·¥æ™ºèƒ½åŠå…¶ç›¸å…³ç ”
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æœ¬å‘¨å€¼å¾—è¯»(2016.12.12-2016.12.16)</title>
    <link href="http://rsarxiv.github.io/2016/12/18/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-12-2016-12-16/"/>
    <id>http://rsarxiv.github.io/2016/12/18/æœ¬å‘¨å€¼å¾—è¯»-2016-12-12-2016-12-16/</id>
    <published>2016-12-18T17:16:11.000Z</published>
    <updated>2016-12-18T17:28:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Building-Large-Machine-Reading-Comprehension-Datasets-using-Paragraph-Vectors"><a href="#Building-Large-Machine-Reading-Comprehension-Datasets-using-Paragraph-Vectors" class="headerlink" title="Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors "></a><a href="http://t.cn/RIbpc9X" target="_blank" rel="external">Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors </a></h2><p>ã€æœºå™¨é˜…è¯»ç†è§£ã€‘ã€æ•°æ®ç¦åˆ©ã€‘<br>æœ¬æ–‡åˆ©ç”¨ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•æ„å»ºäº†ä¸€ç»„å¤§å‹çš„æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†ã€‚å…¶ä¸­æœºå™¨é˜…è¯»ç†è§£é—®é¢˜æ˜¯æä¾›ä¸€ç¯‡æ–°é—»ï¼Œä»5ä¸ªå€™é€‰æ ‡é¢˜ä¸­é€‰æ‹©ä¸€ä¸ªæ­£ç¡®çš„ã€‚æ— ç›‘ç£çš„æ–¹æ³•ç”¨äº†Mikolovæå‡ºçš„Paragraph Vectorï¼ˆWord2Vecçš„æ–‡æ¡£ç‰ˆï¼‰ï¼Œç”¨æ¥è®­ç»ƒå’Œè®¡ç®—å„ä¸ªæ–°é—»æ ‡é¢˜ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œäº§ç”Ÿå€™é€‰ç­”æ¡ˆã€‚æœ¬æ–‡æ‰€ç”Ÿæˆçš„æ•°æ®é›†åœ°å€ï¼š<a href="https://github.com/google/mcafp" target="_blank" rel="external">https://github.com/google/mcafp</a></p>
<h2 id="Multi-Perspective-Context-Matching-for-Machine-Comprehension"><a href="#Multi-Perspective-Context-Matching-for-Machine-Comprehension" class="headerlink" title="Multi-Perspective Context Matching for Machine Comprehension "></a><a href="http://t.cn/RIbdvXM" target="_blank" rel="external">Multi-Perspective Context Matching for Machine Comprehension </a></h2><p>ã€æœºå™¨é˜…è¯»ç†è§£ã€‘æœ¬æ–‡çš„ç ”ç©¶åŸºäºSQuADæ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å‹ï¼Œä¸»è¦çš„æ€è·¯æ˜¯passageä¸­ä¸é—®é¢˜ç›¸ä¼¼çš„spanæ›´åŠ å€¾å‘äºæ˜¯æ­£ç¡®ç­”æ¡ˆã€‚SQuADæ˜¯è¿™ä¸ªé¢†åŸŸä¸­æœ‰åçš„æ•°æ®é›†ï¼Œç›¸åº”çš„æ¨¡å‹å¾ˆå¤šï¼Œæœ¬æ–‡çš„ç»“æœç›¸å¯¹ä¸€èˆ¬ã€‚</p>
<h2 id="ConceptNet-5-5-An-Open-Multilingual-Graph-of-General-Knowledge"><a href="#ConceptNet-5-5-An-Open-Multilingual-Graph-of-General-Knowledge" class="headerlink" title="ConceptNet 5.5: An Open Multilingual Graph of General Knowledge "></a><a href="http://t.cn/RIbgeA5" target="_blank" rel="external">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge </a></h2><p>ã€çŸ¥è¯†å›¾è°±ã€‘ã€èµ„æºæ¨èã€‘æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé€šç”¨çŸ¥è¯†å›¾è°±ConceptNet 5.5ï¼Œå›¾è°±ä¸»é¡µçš„åœ°å€ï¼š<a href="http://conceptnet.io/" target="_blank" rel="external">http://conceptnet.io/</a>  ç›¸å…³çš„codeå’Œæ–‡æ¡£åœ°å€ï¼š <a href="https://github.com/commonsense/conceptnet5" target="_blank" rel="external">https://github.com/commonsense/conceptnet5</a></p>
<h2 id="Tracking-the-World-State-with-Recurrent-Entity-Networks"><a href="#Tracking-the-World-State-with-Recurrent-Entity-Networks" class="headerlink" title="Tracking the World State with Recurrent Entity Networks "></a><a href="http://t.cn/RIbsLuo" target="_blank" rel="external">Tracking the World State with Recurrent Entity Networks </a></h2><p>ã€Dynamic Memoryã€‘æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼ŒRecurrent Entity Network (EntNet)ï¼Œå¼•ç”¨å¤–éƒ¨åŠ¨æ€é•¿ç¨‹è®°å¿†æ¥åšæ¨ç†ï¼Œå¹¶åœ¨ SYNTHETIC WORLD MODELã€bAbIå’ŒCBTä¸‰ä¸ªä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œå€¼å¾—å…³æ³¨ã€‚æœ¬æ–‡å·¥ä½œæ¥è‡ªFB LeCunç»„ã€‚</p>
<h2 id="Online-Sequence-to-Sequence-Reinforcement-Learning-for-Open-Domain-Conversational-Agents"><a href="#Online-Sequence-to-Sequence-Reinforcement-Learning-for-Open-Domain-Conversational-Agents" class="headerlink" title="Online Sequence-to-Sequence Reinforcement Learning for Open-Domain Conversational Agents "></a><a href="http://t.cn/RIbsrka" target="_blank" rel="external">Online Sequence-to-Sequence Reinforcement Learning for Open-Domain Conversational Agents </a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘ç”¨å‡ ä¸ªå…³é”®è¯æ¥æ¦‚æ‹¬ä¸€ä¸‹æœ¬æ–‡çš„å·¥ä½œï¼š1ã€åœ¨çº¿è®­ç»ƒï¼›2ã€seq2seqï¼›3ã€æ·±åº¦å¢å¼ºå­¦ä¹ ï¼›4ã€å¼€æ”¾åŸŸé—®é¢˜ã€‚å»ºè®®å¯¹å¯¹è¯ç³»ç»Ÿæ„Ÿå…´è¶£çš„ç«¥é‹ç ”è¯»ã€‚</p>
<h2 id="Neural-Emoji-Recommendation-in-Dialogue-Systems"><a href="#Neural-Emoji-Recommendation-in-Dialogue-Systems" class="headerlink" title="Neural Emoji Recommendation in Dialogue Systems "></a><a href="http://t.cn/RIqZTsq" target="_blank" rel="external">Neural Emoji Recommendation in Dialogue Systems </a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘ã€Emojiã€‘Emojiè¡¨æƒ…æ˜¯å¤§å®¶åœ¨å¹³æ—¶èŠå¤©æ—¶ç»å¸¸ä¼šç”¨åˆ°çš„ï¼Œå¾€å¾€ä¸€ä¸ªè¡¨æƒ…èƒœè¿‡ä¸€å¥è¯çš„è¡¨è¾¾ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤šè½®å¯¹è¯ä¸­å¦‚ä½•é€šè¿‡ä¸Šä¸‹æ–‡æ¥é¢„æµ‹å’Œæ¨èemojiè¡¨æƒ…ï¼Œæ˜¯ä¸ªå¾ˆå¥½ç©çš„å·¥ä½œã€‚å¦‚æœèƒ½å¤Ÿåˆ†æå’Œé¢„æµ‹æ›´å¹¿æ³›çš„è¡¨æƒ…åŒ…ï¼ˆä¸ä»…é™äºemojiï¼‰çš„è¯ï¼Œå¯èƒ½æ˜¯ä»¶æ›´å¥½ç©çš„äº‹æƒ…ã€‚</p>
<h2 id="Learning-Through-Dialogue-Interactions"><a href="#Learning-Through-Dialogue-Interactions" class="headerlink" title="Learning Through Dialogue Interactions "></a><a href="http://t.cn/RI5dgWk" target="_blank" rel="external">Learning Through Dialogue Interactions </a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘Jiwei Liçš„æ–°æ–‡ç« ï¼Œé€šè¿‡å’ŒTeacherçš„äº¤äº’ï¼ˆåŸºäºçŸ¥è¯†åº“ç›¸äº’é—®å’Œç­”ï¼‰æ¥æé«˜botçš„å­¦ä¹ èƒ½åŠ›ï¼Œæ•´ä½“æ¡†æ¶ä»æ˜¯å¢å¼ºå­¦ä¹ ï¼Œå€¼å¾—ç²¾è¯»ã€‚ä»£ç å’Œæ•°æ®éƒ½å·²å¼€æ”¾ï¼Œåœ°å€ï¼š<a href="https://github.com/facebook/MemNN/tree/master/AskingQuestions" target="_blank" rel="external">https://github.com/facebook/MemNN/tree/master/AskingQuestions</a> torchå®ç°ã€‚</p>
<h2 id="Diverse-Beam-Search-Decoding-Diverse-Solutions-from-Neural-Sequence-Models"><a href="#Diverse-Beam-Search-Decoding-Diverse-Solutions-from-Neural-Sequence-Models" class="headerlink" title="Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models "></a><a href="http://t.cn/RVbp10D" target="_blank" rel="external">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models </a></h2><p>ã€seq2seqå¤šæ ·æ€§ã€‘ã€æŸ±æœç´¢ã€‘ä¸€ç¯‡è€ƒè™‘äº†ç”Ÿæˆå†…å®¹å¤šæ ·æ€§çš„beam searchæ”¹è¿›ç®—æ³•ï¼Œå¯ä»¥åº”ç”¨åœ¨chatbotã€nmtã€image captionã€vqaç­‰å„ç§åœºæ™¯ä¸­ã€‚å¼€æºä»£ç ç”¨torchå®ç°çš„ï¼ŒåŸºäºneuraltalk2ä»£ç ã€‚åœ°å€ï¼š<a href="https://github.com/ashwinkalyan/dbs" target="_blank" rel="external">https://github.com/ashwinkalyan/dbs</a>  åœ¨çº¿demoåœ°å€ï¼š<a href="http://dbs.cloudcv.org/captioning" target="_blank" rel="external">http://dbs.cloudcv.org/captioning</a></p>
<h2 id="Multilingual-Word-Embeddings-using-Multigraphs"><a href="#Multilingual-Word-Embeddings-using-Multigraphs" class="headerlink" title="Multilingual Word Embeddings using Multigraphs "></a><a href="http://t.cn/RIqqODu" target="_blank" rel="external">Multilingual Word Embeddings using Multigraphs </a></h2><p>ã€è¯å‘é‡ã€‘æœ¬æ–‡ç»™äº†ä¸€ç»„å•è¯­å’Œå¤šè¯­çš„è¯å‘é‡å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºSkipGramæ¨¡å‹ï¼Œskipgramçš„contextè€ƒè™‘æ¯”è¾ƒç®€å•ï¼Œæœ¬æ–‡ä¸»è¦æ˜¯åœ¨contextä¸Šåšäº†ä¸€äº›æ–‡ç« ï¼Œæ·»åŠ äº†ä¸€äº›ç‰¹å¾ï¼Œæ¯”å¦‚syntactic dependencies and word alignmentsç­‰ã€‚</p>
<h2 id="FastText-zip-Compressing-text-classification-models"><a href="#FastText-zip-Compressing-text-classification-models" class="headerlink" title="FastText.zip: Compressing text classification models "></a><a href="http://t.cn/RI4uuHE" target="_blank" rel="external">FastText.zip: Compressing text classification models </a></h2><p>ã€æ¨¡å‹å‹ç¼©ã€‘æ¨¡å‹è¿‡å¤§æ˜¯DLçš„ä¸€ä¸ªé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨éƒ¨ç½²æ¨¡å‹æ—¶ï¼Œè¿™ä¸ªé—®é¢˜å°¤å…¶æ˜æ˜¾ã€‚æœ¬æ–‡å·¥ä½œæ¥è‡ªFBï¼Œæ˜¯å¼€æºåˆ†ç±»å·¥å…·fasttextçš„ä¸€ä¸ªæ¨¡å‹å‹ç¼©ç‰ˆã€‚FastTextçš„åœ°å€ï¼š<a href="https://github.com/facebookresearch/fastText" target="_blank" rel="external">https://github.com/facebookresearch/fastText</a></p>
<h2 id="Mining-Compatible-Incompatible-Entities-from-Question-and-Answering-via-Yes-No-Answer-Classification-using-Distant-Label-Expansion"><a href="#Mining-Compatible-Incompatible-Entities-from-Question-and-Answering-via-Yes-No-Answer-Classification-using-Distant-Label-Expansion" class="headerlink" title="Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion "></a><a href="http://t.cn/RIqG4QU" target="_blank" rel="external">Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion </a></h2><p>ã€è¯„è®ºæŒ–æ˜ã€‘æœ¬æ–‡é’ˆå¯¹çš„åº”ç”¨åœºæ™¯æ˜¯ä»å•†å“è¯„è®ºä¸­æŒ–æ˜å„ç§å•†å“çš„å…¼å®¹æ€§ï¼Œæ¯”å¦‚ä¹°äº†ä¸ªé¼ æ ‡ï¼Œæƒ³çŸ¥é“è¿™ä¸ªé¼ æ ‡å’Œipadã€pcçš„å…¼å®¹æ€§å¦‚ä½•ã€‚æ–‡ä¸­çš„Complementary Entity Recognition æ–¹æ³•æ¥è‡ªä¸Šå‘¨åŒä½œè€…çš„ä¸€ç¯‡æ–‡ç« ï¼Œåœ°å€æ˜¯<a href="https://arxiv.org/abs/1612.01039" target="_blank" rel="external">https://arxiv.org/abs/1612.01039</a> è¿™ä¸ªåº”ç”¨åœºæ™¯æ¯”è¾ƒæ¥åœ°æ°”ï¼Œå»ºè®®å¯¹è¯„è®ºæŒ–æ˜æ„Ÿå…´è¶£çš„ç«¥é‹é˜…è¯»ã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Building-Large-Machine-Reading-Comprehension-Datasets-using-Paragraph-Vectors&quot;&gt;&lt;a href=&quot;#Building-Large-Machine-Reading-Comprehensio
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬åå…«æœŸ</title>
    <link href="http://rsarxiv.github.io/2016/12/17/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%85%AB%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/17/PaperWeekly-ç¬¬åå…«æœŸ/</id>
    <published>2016-12-17T18:37:27.000Z</published>
    <updated>2016-12-17T19:35:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h1><p>å¯¹è¯ç³»ç»Ÿæ˜¯å½“å‰çš„ç ”ç©¶çƒ­ç‚¹ï¼Œä¹Ÿæ˜¯é£é™©æŠ•èµ„çš„çƒ­ç‚¹ï¼Œä»2016å¹´åˆå¼€å§‹ï¼Œæˆç«‹äº†æ— æ•°å®¶åšchatbotã€è¯­éŸ³åŠ©æ‰‹ç­‰ç±»ä¼¼äº§å“çš„å…¬å¸ï¼Œä¸ç®¡æ˜¯å¯¹ç”¨æˆ·çš„ï¼Œè¿˜æ˜¯å¯¹ä¼ä¸šçš„ï¼Œå°†å¯¹è¯ç³»ç»Ÿè¿™ä¸€åº”ç”¨æ¨åˆ°äº†ä¸€ä¸ªæ–°çš„é«˜åº¦ã€‚seq2seqæ˜¯å½“å‰æµè¡Œçš„ç®—æ³•æ¡†æ¶ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ï¼Œæ¨¡å‹è‡ªåŠ¨ç»™å‡ºä¸€ä¸ªä¸é”™çš„è¾“å‡ºï¼Œå¬èµ·æ¥éƒ½æ˜¯ä¸€ä»¶ç¾å¥½çš„äº‹æƒ…ã€‚seq2seqåœ¨å¯¹è¯ç³»ç»Ÿä¸­çš„ç ”ç©¶æ¯”è¾ƒå¤šï¼Œæœ¬æœŸPaperWeeklyåˆ†äº«4ç¯‡éå¸¸æ–°çš„paper notesï¼Œæ¶‰åŠåˆ°å¦‚ä½•æé«˜æ‰€ç”Ÿæˆå¯¹è¯çš„æµç•…åº¦å’Œå¤šæ ·æ€§ï¼Œä½¿å¾—å¯¹è¯ç³»ç»Ÿèƒ½å¤Ÿæ›´åŠ æ¥è¿‘äººç±»çš„å¯¹è¯ã€‚4ç¯‡paperå¦‚ä¸‹ï¼š</p>
<p>1ã€Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation, 2016<br>2ã€A Simple, Fast Diverse Decoding Algorithm for Neural Generation, 2016<br>3ã€DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS, 2016<br>4ã€A Diversity-Promoting Objective Function for Neural Conversation Models, 2015</p>
<h1 id="Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation"><a href="#Sequence-to-Backward-and-Forward-Sequences-A-Content-Introducing-Approach-to-Generative-Short-Text-Conversation" class="headerlink" title="Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation"></a><a href="http://cn.arxiv.org/pdf/1607.00970" target="_blank" rel="external">Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China<br>Institute of Software, Peking University, China<br>Institute of Network Computing and Information Systems, Peking Univerity, China<br>Institute of Computer Science and Technology, Peking University, China</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>content-introducing approach<br>neural network-based<br>generative dialogue systems<br>seq2BF</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ä½¿ç”¨å¼•å…¥å†…å®¹æ–¹æ³•ï¼Œç”¨äºå¤„ç†åŸºäºç¥ç»ç½‘ç»œçš„ç”Ÿæˆå¼å¯¹è¯ç³»ç»Ÿ</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p><img src="media/model-18.png" alt="mode"></p>
<p>è¯¥æ¨¡å‹ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š<br>1ã€use PMI to predict a keyword for the reply<br>ä½¿ç”¨é€ç‚¹äº’ä¿¡æ¯(PMI)è¿›è¡Œé¢„æµ‹ï¼Œé€‰å–PMIå€¼æœ€å¤§çš„å•è¯ä½œä¸ºå›ç­”ä¸­çš„å…³é”®è¯ï¼Œè¯¥å…³é”®è¯å¯ä»¥å‡ºç°åœ¨å›ç­”è¯­å¥ä¸­çš„ä»»æ„ä½ç½®ã€‚<br><img src="media/%E5%85%AC%E5%BC%8F.png" alt="å…¬å¼"></p>
<p>2ã€generate a reply conditioned on the keyword as well as the query<br>ä½¿ç”¨sequence to backward and forward sequences(seq2BF)æ¨¡å‹æ¥ç”ŸæˆåŒ…å«å…³é”®è¯çš„å›ç­”ã€‚ä»¥è¯¥å…³é”®è¯ä¸ºåŸºç‚¹ï¼Œå°†å›ç­”è¯­å¥åˆ’åˆ†ä¸ºä¸¤ä¸ªåºåˆ—ï¼š<br>(1) åå‘åºåˆ—ï¼šå…³é”®è¯å·¦ä¾§çš„æ‰€æœ‰å•è¯ä»¥é€†åºæ’åˆ—<br>(2) æ­£å‘åºåˆ—ï¼šå…³é”®è¯å³ä¾§çš„æ‰€æœ‰å•è¯ä»¥é¡ºåºæ’åˆ—</p>
<p>seq2BFæ¨¡å‹å…·ä½“å·¥ä½œå¦‚ä¸‹ï¼š<br>(1) ä½¿ç”¨seq2seqç¥ç»ç½‘ç»œå°†é—®é¢˜ç¼–ç ï¼Œä»…å¯¹å…³é”®è¯å·¦ä¾§çš„å•è¯è¿›è¡Œè§£ç ï¼Œé€†åºè¾“å‡ºæ¯ä¸ªå•è¯<br>(2) ä½¿ç”¨å¦ä¸€ä¸ªseq2seqæ¨¡å‹å°†é—®é¢˜å†æ¬¡ç¼–ç ï¼Œåœ¨ç»™å®šä¸Šæ­¥ä¸­è§£ç åçš„é€†åºå•è¯åºåˆ—ä¸‹ï¼Œå¯¹å›ç­”ä¸­çš„å‰©ä½™å•è¯è¿›è¡Œé¡ºåºè§£ç ï¼Œè¾“å‡ºæœ€ç»ˆå•è¯åºåˆ—<br><img src="media/%E5%85%AC%E5%BC%8F3.png" alt="å…¬å¼3"></p>
<h2 id="èµ„æº"><a href="#èµ„æº" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>Datasetï¼š<a href="http://tieba.baidu.com" target="_blank" rel="external">http://tieba.baidu.com</a></p>
<h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>1ã€ Dialogue Systems<br>(1) (Isbell et al., 2000; Wang et al., 2013) retrieval methods<br>(2)  (Ritter et al., 2011) phrase-based machine translation<br>(3)  (Sordoni et al., 2015; Shang et al., 2015) recurrent neural networks </p>
<p>2ã€ Neural Networks for Sentence Generation<br>(1)  (Sordoni et al., 2015) bag-of-words features<br>(2)  (Shang et al., 2015) seq2seq-like neural networks<br>(3)  (Yao et al., 2015; Serban et al., 2016a) design hierarchical neural networks<br>(4)  (Li et al., 2016a) mutual information training objective</p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œä¸åŒä¸ç›®å‰æ™®éå­˜åœ¨çš„ä»å¥é¦–åˆ°å¥å°¾é¡ºåºç”Ÿæˆç›®æ ‡å•è¯çš„æ–¹æ³•ï¼Œå¼•å…¥é€ç‚¹äº’ä¿¡æ¯æ–¹æ³•æ¥é¢„æµ‹å›ç­”è¯­å¥ä¸­çš„å…³é”®è¯ï¼Œä½¿ç”¨seq2BFæœºåˆ¶ç¡®ä¿è¯¥å…³é”®è¯å¯ä»¥å‡ºç°åœ¨ç›®æ ‡å›ç­”è¯­å¥çš„ä»»æ„ä½ç½®ä¹‹ä¸­å¹¶ç¡®ä¿è¾“å‡ºçš„æµåˆ©åº¦ï¼Œç›¸æ¯”äºseq2seqçš„ç”Ÿæˆæ–¹æ³•æ˜¾è‘—åœ°æå‡äº†å¯¹è¯ç³»ç»Ÿçš„è´¨é‡ã€‚</p>
<h1 id="A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation"><a href="#A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation" class="headerlink" title="A Simple, Fast Diverse Decoding Algorithm for Neural Generation"></a><a href="https://arxiv.org/abs/1611.08562" target="_blank" rel="external">A Simple, Fast Diverse Decoding Algorithm for Neural Generation</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Jiwei Li, Will Monroe and Dan Jurafsky</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Stanford</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>seq2seq, diversity, RL</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>seq2seqæ¨¡å‹decoderæ—¶æ”¹è¿›beam searchï¼Œå¼•å…¥æƒ©ç½šå› å­å½±å“æ’åºç»“æœï¼Œå¹¶åŠ å…¥å¼ºåŒ–å­¦ä¹ æ¨¡å‹æ¥è‡ªåŠ¨å­¦ä¹ diversity rateï¼Œä½¿å¾—è§£ç å‡ºçš„ç»“æœæ›´å…·å¤šæ ·æ€§</p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p><img src="media/18-0.png" alt="18-0"></p>
<p>å¯¹æ¯”æ ‡å‡†beam searchï¼Œæœ¬æ¨¡å‹å¼•å…¥æƒ©ç½šå› å­ï¼Œå…¬å¼å¦‚ä¸‹</p>
<p><img src="media/18-1.png" alt="18-1"></p>
<p>å…¶ä¸­$\gamma$ç§°ä¸ºdiversity rateï¼Œkâ€™èŒƒå›´ä¸º[1,k]ï¼ŒKä¸ºbeam size<br>å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä¸­ï¼Œç­–ç•¥ä¸º</p>
<p><img src="media/18-2.png" alt="18-2"></p>
<p>rewardä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ä¸­çš„BLEUå€¼ç­‰</p>
<h2 id="èµ„æº-1"><a href="#èµ„æº-1" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€å›å¤ç”Ÿæˆå®éªŒæ•°æ®é›†ï¼šOpenSubtitles <a href="https://github.com/jiweil/mutual-information-for-neural-machine-translation" target="_blank" rel="external">https://github.com/jiweil/mutual-information-for-neural-machine-translation</a><br>ï¼ˆä»£ç æ¨¡å‹å¯ä»ä½œè€…å¦å¤–ä¸€ç¯‡æ–‡ç« çš„æºç ç¨åŠ æ”¹åŠ¨ï¼‰</p>
<p>2ã€æœºå™¨ç¿»è¯‘æ•°æ®é›†ï¼šWMTâ€™14 <a href="http://www.statmt.org/wmt13/translation-task.html" target="_blank" rel="external">http://www.statmt.org/wmt13/translation-task.html</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-1"><a href="#ç›¸å…³å·¥ä½œ-1" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p><img src="media/18-3.png" alt="18-3"></p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ¨¡å‹çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥æƒ©ç½šå› å­ï¼Œä½¿å¾—decoderæ—¶å¯¹standard beam searchç®—æ³•è¿›è¡Œé‡æ’åºï¼Œå¹¶å¼•å…¥å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œè‡ªåŠ¨å­¦ä¹ diversity rateã€‚ä½œè€…åˆ†åˆ«åœ¨ä¸‰ä¸ªå®éªŒä¸Šè¿›è¡ŒéªŒè¯ï¼Œæœºå™¨ç¿»è¯‘ã€æ‘˜è¦æŠ½å–ä¸å¯¹è¯å›å¤ç”Ÿæˆï¼Œå®éªŒè¡¨æ˜åœ¨ä¸åŒçš„å®éªŒä¸Šæœ‰ä¸åŒçš„è¡¨ç°ï¼Œä½†æ˜¯æ€»ä½“è€Œè¨€æœ¬æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£ç å‡ºæ›´å…·æœ‰å¤šæ ·æ€§çš„å¥å­ã€‚ï¼ˆæ€è·¯ç®€æ˜æ¸…æ™°ï¼Œå¯¹äºä¼ ç»Ÿçš„beam searchç¨åŠ æ”¹åŠ¨ï¼ŒåŸæ–‡ä¸­ä½œè€…æåˆ°åœ¨Matlabä»£ç ä¸­åªæ”¹åŠ¨ä¸€è¡Œå³å¯ï¼‰</p>
<h1 id="DIVERSE-BEAM-SEARCH-DECODING-DIVERSE-SOLUTIONS-FROM-NEURAL-SEQUENCE-MODELS"><a href="#DIVERSE-BEAM-SEARCH-DECODING-DIVERSE-SOLUTIONS-FROM-NEURAL-SEQUENCE-MODELS" class="headerlink" title="DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS"></a><a href="https://arxiv.org/abs/1610.02424" target="_blank" rel="external">DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun1 Stefan Lee, David Crandall &amp; Dhruv Batra</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Virginia Tech, Blacksburg, VA, USA<br>Indiana University, Bloomington, IN, USA</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Beam Search; Diversity; Image Caption; Machine Translation; Visual Question Answer; Chatbot</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016.10</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>å¦‚ä½•æ”¹è¿›beam searchè§£ç ç®—æ³•ï¼Œä½¿å…¶åœ¨seq2seqæ¨¡å‹ä¸­å¯ä»¥ç”Ÿæˆæ›´åŠ ä¸°å¯Œçš„ç»“æœï¼Ÿ</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>ç»å…¸çš„beam searchç®—æ³•ä»¥æœ€å¤§åéªŒæ¦‚ç‡ä½œä¸ºä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œæ¯ä¸€ä¸ªtime stepåªä¿ç•™Bä¸ªæœ€ä¼˜çš„çŠ¶æ€ï¼Œæ˜¯ä¸€ç§å…¸å‹çš„è´ªå¿ƒç®—æ³•ï¼Œè¿™ä¸ªç»å…¸ç®—æ³•å¸¸å¸¸è¢«ç”¨äºè§£ç å¯é€‰çŠ¶æ€æ•°é‡å¤šçš„æƒ…å½¢ï¼Œæ¯”å¦‚ç”Ÿæˆå¯¹è¯ã€ç”Ÿæˆå›¾ç‰‡æè¿°ã€æœºå™¨ç¿»è¯‘ç­‰ï¼Œæ¯ä¸€æ­¥éƒ½æœ‰è¯è¡¨å¤§å°çš„å¯é€‰çŠ¶æ€é›†ã€‚seq2seqæ¨¡å‹çš„æµè¡Œï¼Œè®©è¿™ç§è§£ç ç®—æ³•çš„ç ”ç©¶å˜å¾—çƒ­é—¨ã€‚åœ¨ç”Ÿæˆå¯¹è¯ä»»åŠ¡æ—¶ï¼Œç”¨ç»å…¸çš„beam searchä¼šç”Ÿæˆç±»ä¼¼â€œæˆ‘ä¸çŸ¥é“â€ç­‰è¿™ç§æ²¡æœ‰è¥å…»çš„å¯¹è¯ï¼Œè™½ç„¶æ²¡æœ‰è¯­æ³•ä¸Šçš„é”™è¯¯ï¼Œè€Œä¸”å¯èƒ½åœ¨ä¸€å®šçš„è¯„ä»·ä½“ç³»å†…ä¼šå¾—åˆ°ä¸é”™çš„åˆ†æ•°ï¼Œä½†å®é™…åº”ç”¨æ•ˆæœå¤ªå·®ï¼Œå› æ­¤diversityçš„ç ”ç©¶å˜å¾—çƒ­é—¨ã€‚</p>
<p>æœ¬æ–‡é’ˆå¯¹diversityçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›ç‰ˆçš„beam searchç®—æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆæ›´åŠ å¤šæ ·æ€§çš„è¯ã€‚</p>
<p><img src="media/18-5.png" alt="18-5"></p>
<p>æ–°ç®—æ³•çš„ä¸»è¦æ€è·¯æ˜¯å°†ç»å…¸ç®—æ³•ä¸­çš„Beamè¿›è¡Œåˆ†ç»„ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªæƒ©ç½šæœºåˆ¶ï¼Œä½¿å¾—æ¯ä¸€ç»„çš„ç›¸ä¼¼åº¦å°½é‡ä½ï¼Œè¿™ä¸€é¡¹ä¿è¯äº†ç”Ÿæˆçš„è¯ç›¸äº’ä¹‹é—´å·®å¼‚æ›´å¤§ä¸€äº›ï¼Œå³æ»¡è¶³äº†å¤šæ ·æ€§çš„éœ€æ±‚ï¼Œåœ¨æ¯ä¸€ç»„Beamä¸­ï¼Œç”¨ç»å…¸çš„ç®—æ³•è¿›è¡Œä¼˜åŒ–æœç´¢ã€‚å…·ä½“çš„ç®—æ³•æµç¨‹å¦‚ä¸‹å›¾ï¼š</p>
<p><img src="media/18-6.png" alt="18-6"></p>
<p>å®éªŒä¸­ï¼Œç”¨äº†Image Captionã€Machine Translationå’ŒVQAä¸‰ä¸ªä»»åŠ¡è¿›è¡Œäº†å¯¹æ¯”ï¼ŒéªŒè¯äº†æœ¬æ–‡ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”å¯¹ç®—æ³•ä¸­çš„å‡ ä¸ªå‚æ•°è¿›è¡Œäº†æ•æ„Ÿåº¦åˆ†æï¼Œåˆ†æäº†åˆ†ç»„æ•°å¯¹å¤šæ ·æ€§çš„å½±å“ã€‚</p>
<h2 id="èµ„æº-2"><a href="#èµ„æº-2" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€æœ¬æ–‡ç®—æ³•torchå®ç° <a href="https://github.com/ashwinkalyan/dbs" target="_blank" rel="external">https://github.com/ashwinkalyan/dbs</a><br>2ã€æœ¬æ–‡åœ¨çº¿demo dbs.cloudcv.org<br>3ã€neuraltalk2å®ç° <a href="https://github.com/karpathy/neuraltalk2" target="_blank" rel="external">https://github.com/karpathy/neuraltalk2</a><br>4ã€æœºå™¨ç¿»è¯‘å¼€æºå®ç°dl4mt <a href="https://github.com/nyu-dl/dl4mt-tutorial" target="_blank" rel="external">https://github.com/nyu-dl/dl4mt-tutorial</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-2"><a href="#ç›¸å…³å·¥ä½œ-2" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ç›¸å…³çš„å·¥ä½œä¸»è¦åˆ†ç±»ä¸¤ç±»ï¼š<br>1ã€Diverse M-Best Lists<br>2ã€Diverse Decoding for RNNs<br>ä¹‹å‰Jiwei Liå°†è§£ç ç®—æ³•çš„ç›®æ ‡å‡½æ•°æ¢æˆäº†äº’ä¿¡æ¯è¿›è¡Œä¼˜åŒ–è§£ç ï¼Œå¯¹diversityè¿›è¡Œäº†ç ”ç©¶ã€‚</p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡ç ”ç©¶çš„é—®é¢˜æ˜¯ä¸€ç±»åŸºç¡€é—®é¢˜ï¼Œbeam searchç®—æ³•ä½œä¸ºä¸€ç§ç»å…¸çš„è¿‘ä¼¼è§£ç ç®—æ³•ï¼Œåº”ç”¨çš„åœºæ™¯éå¸¸å¤šã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå°¤å…¶æ˜¯å…·ä½“åˆ°ç”Ÿæˆå¯¹è¯ã€ç”Ÿæˆç­”æ¡ˆç­‰ä»»åŠ¡ä¸Šï¼Œå­˜åœ¨ä¸€äº›é€‚åº”æ€§çš„é—®é¢˜ï¼Œæ¯”å¦‚diversityã€‚åªæ˜¯ç”Ÿæˆç®€å•è€Œåˆå®‰å…¨çš„è¯å¯¹äºå®é™…åº”ç”¨æ²¡æœ‰å¤ªå¤šçš„æ„ä¹‰ï¼Œæ‰€ä»¥æœ¬æ–‡çš„ç ”ç©¶éå¸¸æœ‰æ„ä¹‰ã€‚æœ¬æ–‡çš„å®éªŒä»ä¸‰ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šå¯¹æ”¹è¿›åçš„beam searchéƒ½åšäº†å¯¹æ¯”éªŒè¯ï¼Œéå¸¸æ‰å®çš„ç»“æœéªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”å¯¹å‡ ä¸ªå…³é”®å‚æ•°è¿›è¡Œäº†æ•æ„Ÿåº¦åˆ†æï¼Œæœ‰ç†æœ‰æ®ã€‚åŒæ—¶åœ¨githubä¸Šå¼€æºäº†ä»£ç ï¼Œå¹¶ä¸”ç»™å‡ºäº†ä¸€ä¸ªåœ¨çº¿demoã€‚åœ¨è¯„ä»·æ–¹é¢ï¼Œä¸ä»…ä»…è®¾è®¡äº†å‡ ä¸ªè‡ªåŠ¨è¯„ä»·æŒ‡æ ‡ï¼Œè€Œä¸”ç”¨äº†äººå·¥è¯„ä»·çš„æ–¹æ³•å¯¹æœ¬æ–‡ç®—æ³•è¿›è¡Œäº†éªŒè¯ï¼Œæ˜¯ä¸€ç¯‡éå¸¸å¥½çš„paperï¼Œå€¼å¾—å­¦ä¹ ã€‚</p>
<h1 id="A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models"><a href="#A-Diversity-Promoting-Objective-Function-for-Neural-Conversation-Models" class="headerlink" title="A Diversity-Promoting Objective Function for Neural Conversation Models"></a><a href="https://arxiv.org/pdf/1510.03055.pdf" target="_blank" rel="external">A Diversity-Promoting Objective Function for Neural Conversation Models</a></h1><h2 id="ä½œè€…-3"><a href="#ä½œè€…-3" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan</p>
<h2 id="å•ä½-3"><a href="#å•ä½-3" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Stanford University, Stanford, CA, USA<br>Microsoft Research, Redmond, WA, USA</p>
<h2 id="å…³é”®è¯-3"><a href="#å…³é”®è¯-3" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Sequence-to-sequence neural network models, conversational responses, Maximum Mutual Information(MMI)</p>
<h2 id="æ–‡ç« æ¥æº-3"><a href="#æ–‡ç« æ¥æº-3" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2015</p>
<h2 id="é—®é¢˜-3"><a href="#é—®é¢˜-3" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ä½¿ç”¨MMIè®­ç»ƒsequence-to-sequence model for conversational responses generation<br>ä¼ ç»Ÿçš„ML(æœ€å¤§ä¼¼ç„¶ä¼°è®¡)åœ¨è®­ç»ƒsequence-to-sequence modelçš„æ—¶å€™ï¼Œæ˜“äº§ç”Ÿä¸è¾“å…¥æ— å…³çš„â€™safeâ€™ responses(æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å¼Šç—…â€”-always try to cover all mode of input data)<br>ä½œè€…é€šè¿‡ä½¿ç”¨MMI, æœ€å¤§åŒ–è¾“å…¥ä¸è¾“å‡ºçš„äº’ä¿¡æ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…ä¸è¾“å…¥æ— å…³çš„responsesï¼Œå¾—åˆ°æ›´ä¸ºdiverseçš„responses.</p>
<h2 id="æ¨¡å‹-3"><a href="#æ¨¡å‹-3" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>MMIæœ€æ—©åœ¨speech recognitionä¸­æå‡ºå¹¶åº”ç”¨(discriminative training criteria). è¯­éŸ³è¯†åˆ«ä¸­ï¼Œé€šå¸¸å…ˆç”¨MLè®­ç»ƒå£°å­¦æ¨¡å‹ï¼Œç„¶åå†æ¥MMIå’Œè¯­è¨€æ¨¡å‹ï¼Œå¯¹å£°å­¦æ¨¡å‹è¿›ä¸€æ­¥è°ƒä¼˜ã€‚</p>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…é€šè¿‡æå‡ºMMIç”¨äºseq-to-seq modelçš„ä¼˜åŒ–ã€‚ä½œè€…æå‡ºäº†MMI-antiLMå’ŒMMI-bidi ä¸¤ä¸ªä¸åŒçš„MMIçš„formulations. MMIåœ¨seq-to-seqçš„åº”ç”¨ä¸­å­˜åœ¨decodingçš„é—®é¢˜ã€‚</p>
<p>MMI-antiLMä¸­ï¼Œä½œè€…é€šè¿‡ä½¿ç”¨å¸¦æœ‰æƒé‡çš„LMä»¥ç”Ÿæˆæ›´ä¸ºdiverseçš„responses by penalizing first wordã€‚</p>
<p>MMI-bidiä¸­ï¼Œæœç´¢ç©ºé—´çš„æ•°ç›®è¿‡å¤§ï¼Œå¯¼è‡´expolringæ‰€æœ‰çš„å¯èƒ½æ€§åœ¨å®é™…ä¸­æ— æ³•å®ç°ã€‚ä½œè€…é¦–å…ˆäº§ç”ŸN-best list, ç„¶åæ ¹æ®ç›¸åº”çš„å‡†åˆ™å‡½æ•° re-rankå¾—åˆ°çš„N-best listã€‚</p>
<p>åœ¨MMIä¸åŒçš„formulationä¸­ï¼Œä½œè€…é€šè¿‡å¯å‘å¼çš„è®¾è®¡ï¼Œä½¿å¾—decodingæ›´ä¸ºå®¹æ˜“ä¸”äº§ç”Ÿçš„responseæ›´ä¸ºdiverseï¼Œåœ¨ç›¸å…³çš„æ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒå¥½çš„BLEUä¸”äº§ç”Ÿçš„responseæ›´ä¸ºdiverseã€‚</p>
<h2 id="ç®€è¯„-3"><a href="#ç®€è¯„-3" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ€å¤§åéªŒæ¦‚ç‡é€šå¸¸ä½œä¸ºä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œä½†å¾ˆå¤šåº”ç”¨åœºæ™¯ä¸­å¾—åˆ°çš„ç»“æœå¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡é‡‡ç”¨äº†ä¸€ä¸ªæ–°çš„è€Œä¸”ä¹Ÿæ˜¯å…¶ä»–é¢†åŸŸä¸­æ¯”è¾ƒå¸¸è§çš„ç›®æ ‡å‡½æ•°æ¥æ›¿æ¢æœ€å¤§åéªŒæ¦‚ç‡ï¼Œåœ¨ç”Ÿæˆå¯¹è¯æ—¶å¾—åˆ°äº†æ›´åŠ ä¸°å¯Œçš„ç»“æœã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>å¯¹è¯ç³»ç»Ÿæ˜¯ä¸€ä¸ªç›¸å¯¹é«˜çº§çš„ã€ç»¼åˆæ€§å¾ˆå¼ºçš„ä»»åŠ¡ï¼Œæ‰€ä¾èµ–çš„åŸºç¡€ä»»åŠ¡æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚åˆ†è¯ã€å‘½åå®ä½“è¯†åˆ«ã€å¥æ³•åˆ†æã€è¯­ä¹‰è§’è‰²æ ‡æ³¨ç­‰ç­‰ã€‚å¯¹äºè§„èŒƒçš„ä¸­æ–‡è¡¨è¾¾è€Œè¨€ï¼Œå¥æ³•åˆ†æä»æ˜¯ä¸€ä¸ªæ²¡æœ‰è§£å†³å¥½çš„é—®é¢˜ï¼Œæ›´ä½•å†µæ˜¯ä¸é‚£ä¹ˆè§„èŒƒçš„äººè¯ï¼Œå¥æ³•åˆ†æçš„å‡†ç¡®æ€§åˆè¦ä¸‹ä¸€ä¸ªleveläº†ï¼Œéšä¹‹è¯­ä¹‰è§’è‰²æ ‡æ³¨ä¹Ÿå¾—ä¸åˆ°å¥½çš„æ•ˆæœã€‚ç»å…¸çš„ã€åŸºç¡€çš„ä»»åŠ¡è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ï¼Œå¯¹è¯ç³»ç»Ÿè¿™ç§æ›´éš¾ã€æ›´å¤æ‚çš„ä»»åŠ¡ç›¸ä¿¡ä¸æ˜¯ä¸€å¹´ã€ä¸¤å¹´å°±å¯ä»¥çªç ´çš„äº‹æƒ…ï¼Œè™½ç„¶ç°åœ¨å¤§çƒ­ï¼Œåšçš„äººå¾ˆå¤šï¼Œä½†å°±ç›®å‰çš„ç ”ç©¶æ°´å¹³æ¥çœ‹ï¼Œåº”è¯¥è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚seq2seqæ˜¯ä¸ªé€ƒé¿è¿™äº›é—®é¢˜çš„å¥½æ–¹æ³•å’Œå¥½æ€è·¯ï¼Œä½†ç›¸å¯¹æ¥è¯´æ›´åŠ ä¸æˆç†Ÿï¼Œè€Œä¸”å­˜åœ¨ç€å¾ˆå¤šçš„é—®é¢˜ï¼Œæƒ³é€šè¿‡å¤§é‡çš„æ•°æ®æ¥è¦†ç›–æ‰€æœ‰çš„é—®é¢˜ï¼Œæ˜¯ä¸€ç§ä¸å¤ªç§‘å­¦çš„æ€è·¯ã€‚æˆ‘æƒ³ï¼Œseq2seqæ˜¯ä¸ªå¥½æ–¹æ³•ï¼Œä½†ä¼ ç»Ÿçš„NLPæ–¹æ³•ä¹Ÿæ˜¯å¿…ä¸å¯å°‘çš„ï¼Œè€Œä¸”ä¸¤è€…åº”è¯¥æ˜¯ç›¸äº’è¡¥å……çš„ã€‚è¶Šå¤šçš„äººå…³æ³¨å¯¹è¯ç³»ç»Ÿï¼Œå°±ä¼šè¶Šå¿«åœ°æ¨åŠ¨è¿™ä¸ªé¢†åŸŸçš„å‘å±•ï¼Œå¸Œæœ›æ—©æ—¥çœ‹åˆ°é è°±çš„ã€æˆç†Ÿçš„è§£å†³æ–¹æ¡ˆã€‚æ„Ÿè°¢@Pennyã€@tonyaã€@zhangjunå’Œ@çš“å¤© å››ä½ç«¥é‹å®Œæˆçš„paper notesã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•è¨€&quot;&gt;&lt;a href=&quot;#å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;å¼•è¨€&quot;&gt;&lt;/a&gt;å¼•è¨€&lt;/h1&gt;&lt;p&gt;å¯¹è¯ç³»ç»Ÿæ˜¯å½“å‰çš„ç ”ç©¶çƒ­ç‚¹ï¼Œä¹Ÿæ˜¯é£é™©æŠ•èµ„çš„çƒ­ç‚¹ï¼Œä»2016å¹´åˆå¼€å§‹ï¼Œæˆç«‹äº†æ— æ•°å®¶åšchatbotã€è¯­éŸ³åŠ©æ‰‹ç­‰ç±»ä¼¼äº§å“çš„å…¬å¸ï¼Œä¸ç®¡æ˜¯å¯¹ç”¨æˆ·çš„ï¼Œè¿˜
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æœ¬å‘¨å€¼å¾—è¯»(2016.12.05-2016.12.09)</title>
    <link href="http://rsarxiv.github.io/2016/12/11/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-12-05-2016-12-09/"/>
    <id>http://rsarxiv.github.io/2016/12/11/æœ¬å‘¨å€¼å¾—è¯»-2016-12-05-2016-12-09/</id>
    <published>2016-12-11T16:51:29.000Z</published>
    <updated>2016-12-11T17:01:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ä¸€å‘¨å€¼å¾—è¯»"><a href="#ä¸€å‘¨å€¼å¾—è¯»" class="headerlink" title="ä¸€å‘¨å€¼å¾—è¯»"></a>ä¸€å‘¨å€¼å¾—è¯»</h1><h2 id="End-to-End-Joint-Learning-of-Natural-Language-Understanding-and-Dialogue-Manager"><a href="#End-to-End-Joint-Learning-of-Natural-Language-Understanding-and-Dialogue-Manager" class="headerlink" title="End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager "></a><a href="http://t.cn/RfDCS5X" target="_blank" rel="external">End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager </a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘è‡ªç„¶è¯­è¨€ç†è§£å’Œå¯¹è¯ç®¡ç†é€šå¸¸æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼ŒNLUçš„è¯¯å·®ä¼šå½±å“åˆ°å¯¹è¯ç®¡ç†çš„æ•ˆæœã€‚æœ¬æ–‡å°†ä¸¤ä¸ªä»»åŠ¡è”åˆèµ·æ¥è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œå¾—åˆ°äº†ä¸é”™çš„æ•ˆæœã€‚å»ºè®®ç ”ç©¶å¯¹è¯ç³»ç»Ÿçš„ç«¥é‹æ¥è¯»ã€‚</p>
<h2 id="Sequential-Match-Network-A-New-Architecture-for-Multi-turn-Response-Selection-in-Retrieval-based-Chatbots"><a href="#Sequential-Match-Network-A-New-Architecture-for-Multi-turn-Response-Selection-in-Retrieval-based-Chatbots" class="headerlink" title="Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots "></a><a href="http://t.cn/RIhcTFP" target="_blank" rel="external">Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots </a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘æœ¬æ–‡ç ”ç©¶çš„é—®é¢˜æ˜¯åŸºäºæ£€ç´¢çš„å¤šè½®å¯¹è¯æœºå™¨äººï¼Œå•è½®å¯¹è¯å’Œå¤šè½®å¯¹è¯çš„ä¸€å¤§åŒºåˆ«åœ¨äºåè€…éœ€è¦è€ƒè™‘æ›´å¤šçš„ä¸Šä¸‹æ–‡å†…å®¹ï¼Œæœ¬æ–‡åœ¨æ£€ç´¢ç­”æ¡ˆæ—¶é™¤äº†ç›¸å…³æ€§è¿˜è€ƒè™‘äº†ä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³ç³»ï¼Œå»ºè®®ç ”ç©¶æ£€ç´¢å¼èŠå¤©æœºå™¨äººçš„ç«¥é‹æ¥è¯»æœ¬æ–‡ã€‚æœ¬æ–‡è¿˜ç»™å‡ºäº†ä¸€ä¸ªæµ‹è¯•æ•°æ®é›†ï¼Œåœ°å€åœ¨ï¼š<a href="http://t.cn/RIhf4Sh" target="_blank" rel="external">http://t.cn/RIhf4Sh</a></p>
<h2 id="CER-Complementary-Entity-Recognition-via-Knowledge-Expansion-on-Large-Unlabeled-Product-Reviews"><a href="#CER-Complementary-Entity-Recognition-via-Knowledge-Expansion-on-Large-Unlabeled-Product-Reviews" class="headerlink" title="CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews "></a><a href="http://t.cn/RfDpyCm" target="_blank" rel="external">CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews </a></h2><p>ã€ç›¸å…³å®ä½“è¯†åˆ«ã€‘æœ¬æ–‡ç ”ç©¶çš„é—®é¢˜æ˜¯äº§å“è¯„è®ºæ•°æ®ä¸­çš„ç›¸å…³å®ä½“è¯†åˆ«é—®é¢˜ï¼Œè¯„è®ºæ•°æ®æ˜¯ä¸ªå¾ˆæœ‰æ„æ€çš„æ•°æ®ï¼Œç”¨æˆ·åœ¨ä¹°ä¸œè¥¿æ—¶å¸Œæœ›å¯ä»¥é€šè¿‡å¯¹æ¯”ä¹°åˆ°æ›´å¥½çš„äº§å“ã€‚å»ºè®®åšè¯„è®ºæŒ–æ˜çš„ç«¥é‹è¯»ã€‚</p>
<h2 id="The-Evolution-of-Sentiment-Analysis-A-Review-of-Research-Topics-Venues-and-Top-Cited-Papers"><a href="#The-Evolution-of-Sentiment-Analysis-A-Review-of-Research-Topics-Venues-and-Top-Cited-Papers" class="headerlink" title="The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers "></a><a href="http://t.cn/RIvkAop" target="_blank" rel="external">The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers </a></h2><p>ã€æƒ…æ„Ÿåˆ†æã€‘ã€ç»¼è¿°ã€‘ä¸€ç¯‡å¾ˆç»†çš„æƒ…æ„Ÿåˆ†æçš„ç»¼è¿°ï¼Œåˆšåˆšè¿›å…¥è¿™ä¸ªé¢†åŸŸçš„ç«¥é‹å¯ä»¥æ¥è¯»ä¸€è¯»ã€‚</p>
<h2 id="ä¸€å‘¨èµ„æº"><a href="#ä¸€å‘¨èµ„æº" class="headerlink" title="ä¸€å‘¨èµ„æº"></a>ä¸€å‘¨èµ„æº</h2><h2 id="æ–‡æœ¬ä¸Šçš„ç®—æ³•"><a href="#æ–‡æœ¬ä¸Šçš„ç®—æ³•" class="headerlink" title="æ–‡æœ¬ä¸Šçš„ç®—æ³•"></a><a href="http://t.cn/RhtyvzE" target="_blank" rel="external">æ–‡æœ¬ä¸Šçš„ç®—æ³•</a></h2><p>ã€Šæ–‡æœ¬ä¸Šçš„ç®—æ³•ã€‹v4.0ï¼šå¢åŠ è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¯¹è¯ç³»ç»Ÿç« èŠ‚ï¼›ä¸°å¯Œäº†å…¶ä»–å†…å®¹ã€‚</p>
<h2 id="NIPS-2016-Spotlight-Videos"><a href="#NIPS-2016-Spotlight-Videos" class="headerlink" title="NIPS 2016 Spotlight Videos"></a><a href="http://t.cn/RfB5cA2" target="_blank" rel="external">NIPS 2016 Spotlight Videos</a></h2><p>ã€NIPS 2016 Spotlight Videosã€‘NIPS 2016ç„¦ç‚¹è§†é¢‘é›†ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿå¤§ä¼š(Conference and Workshop on Neural Information Processing Systems)ï¼Œç®€ç§°NIPSï¼Œæ˜¯ä¸€ä¸ªå…³äºæœºå™¨å­¦ä¹ å’Œè®¡ç®—ç¥ç»ç§‘å­¦çš„å›½é™…ä¼šè®®ã€‚è¯¥ä¼šè®®å›ºå®šåœ¨æ¯å¹´çš„12æœˆä¸¾è¡Œ,ç”±NIPSåŸºé‡‘ä¼šä¸»åŠã€‚NIPSæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é¡¶çº§ä¼šè®® ã€‚åœ¨ä¸­å›½è®¡ç®—æœºå­¦ä¼šçš„å›½é™…å­¦æœ¯ä¼šè®®æ’åä¸­ï¼ŒNIPSä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„Aç±»ä¼šè®®ã€‚(via @ç½‘è·¯å†·çœ¼)</p>
<h2 id="2016å¹´æ·±åº¦å­¦ä¹ çš„ä¸»è¦è¿›å±•"><a href="#2016å¹´æ·±åº¦å­¦ä¹ çš„ä¸»è¦è¿›å±•" class="headerlink" title="2016å¹´æ·±åº¦å­¦ä¹ çš„ä¸»è¦è¿›å±•"></a><a href="http://t.cn/RIvuIuV" target="_blank" rel="external">2016å¹´æ·±åº¦å­¦ä¹ çš„ä¸»è¦è¿›å±•</a></h2><p>2016å¹´æ·±åº¦å­¦ä¹ çš„ä¸»è¦è¿›å±•ï¼ŒThe major advancements in Deep Learning in 2016 (via @è§†è§‰æœºå™¨äºº)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;a href=&quot;#ä¸€å‘¨å€¼å¾—è¯»&quot; class=&quot;headerlink&quot; title=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;/a&gt;ä¸€å‘¨å€¼å¾—è¯»&lt;/h1&gt;&lt;h2 id=&quot;End-to-End-Joint-Learning-of-Natural-Language-Underst
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬åä¸ƒæœŸ</title>
    <link href="http://rsarxiv.github.io/2016/12/10/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%B8%83%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/10/PaperWeekly-ç¬¬åä¸ƒæœŸ/</id>
    <published>2016-12-10T18:16:44.000Z</published>
    <updated>2016-12-10T20:07:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h1><p>å‘½åå®ä½“è¯†åˆ«æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä¸€ä¸ªéå¸¸åŸºç¡€çš„å·¥ä½œï¼Œæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å…³é”®çš„ä¸€ä¸ªç¯èŠ‚ã€‚ç›‘ç£å­¦ä¹ æ˜¯è§£å†³å‘½åå®ä½“è¯†åˆ«çš„ä¸€ä¸ªåŸºæœ¬æ‰‹æ®µï¼Œä½†æ ‡æ³¨æ•°æ®çš„è·å–æˆæœ¬å¾€å¾€ä¼šæ¯”è¾ƒé«˜ï¼Œæœ¬æœŸPaperWeeklyå°†å¸¦å¤§å®¶æ¥çœ‹ä¸€ä¸‹å¦‚ä½•é€šè¿‡åŠç›‘ç£æˆ–è€…æ— ç›‘ç£çš„æ–¹æ³•æ¥åšå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ã€‚æœ¬æœŸåˆ†äº«çš„4ç¯‡Paper Notesåˆ†åˆ«æ˜¯ï¼š</p>
<p>1ã€Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre), 2016<br>2ã€ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering, 2015<br>3ã€Bootstrapped Text-level Named Entity Recognition for Literature, 2016<br>4ã€Recognizing Named Entities in Tweets, 2011</p>
<h1 id="Building-a-Fine-Grained-Entity-Typing-System-Overnight-for-a-New-X-X-Language-Domain-Genre"><a href="#Building-a-Fine-Grained-Entity-Typing-System-Overnight-for-a-New-X-X-Language-Domain-Genre" class="headerlink" title="Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)"></a><a href="https://arxiv.org/pdf/1603.03112v1.pdf" target="_blank" rel="external">Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Rensselaer Polytechnic Institute,<br>Information Sciences Institute,<br>Rensselaer Polytechnic Institute</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Entity Recognition and Typing, Unspuversied</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv, 2016</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ç»†ç²’åº¦çš„å®ä½“è¯†åˆ«æ˜¯è¿™å‡ å¹´æ¯”è¾ƒæµè¡Œçš„å·¥ä½œã€‚ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯éœ€è¦å…ˆé¢„å®šä¹‰ä¸€ç»„å®ä½“æ‰€å±ç±»å‹ï¼Œéšåä½¿ç”¨å¤§é‡çš„æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒå¤šåˆ†ç±»å™¨ã€‚æœ¬æ–‡é’ˆå¯¹éœ€è¦æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä½¿ç”¨éç›‘ç£å­¦ä¹ çš„æ€è·¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æœ¬æ–‡ä¸­æ–¹æ³•çš„æ¶æ„å¦‚ä¸‹å›¾:</p>
<p><img src="media/overview.png" alt="overvie"></p>
<p>1ï¼‰é€šè¿‡entity mentionçš„è¯­æ–™ï¼Œæ„å»ºentity mentionçš„context<br>2ï¼‰éšåæ„å»ºçŸ¥è¯†åº“çš„è¡¨è¾¾<br>3ï¼‰é€šè¿‡çŸ¥è¯†åº“å’Œentity mentionè¿›è¡Œè¿æ¥<br>4ï¼‰å°†è¿æ¥åçš„æ•°æ®å­¦ä¹ ä¸‰ç§è¡¨è¾¾</p>
<ul>
<li>a general entity distributed representation</li>
<li>a specific context representation</li>
<li>a knowledge representation</li>
</ul>
<p>å…¶ä¸­entity distributed representationä¸»è¦æ˜¯é€šè¿‡ä¸Šä¸‹æ–‡æ¥è¡¨è¾¾å®ä½“ã€‚<br>è€Œ a specific context representationä¸»è¦æ˜¯è¡¨è¾¾ä¸€äº›local featureå’Œä¸€äº›è¯­è¨€ç»“æ„çš„ç‰¹å¾ã€‚<br>æœ€åa knowledge representationä¸»è¦æ˜¯ç”¨æ¥æ¨¡æ‹Ÿé¢†åŸŸç›¸å…³çš„çŸ¥è¯†</p>
<p>æœ€åç®—æ³•é€šè¿‡ä¸€ä¸ªå±‚æ¬¡èšç±»ç®—æ³•æ¥è·å–entity mentionå¯èƒ½çš„åˆ†ç±»ä¿¡æ¯</p>
<p>1ã€General Entity Representation<br>entity mentionçš„è¡¨è¾¾ä½œè€…ä¸»è¦æ˜¯ç”¨äº†Skip-gram modelé€šè¿‡å¤§é‡çš„è¯­æ–™æ¥è®­ç»ƒï¼Œæœ€ç»ˆå¯ä»¥å¾—åˆ°æ¯ä¸ªentity mentionçš„è¡¨è¾¾ã€‚è¿™ä¸ªæ€è·¯çš„å¥½å¤„æ˜¯è®©ä¸¤ä¸ªentity mentionå±äºåŒä¸€ç±»å‹æ—¶ï¼Œentity mentionçš„ä¸Šä¸‹æ–‡ä¼šæ¯”è¾ƒç›¸ä¼¼ï¼Œè¿›è€Œå¯ä»¥å¾—åˆ°ç›¸ä¼¼çš„åˆ†å¸ƒå¼è¡¨è¾¾</p>
<p>2ã€a specific context representation<br>ä¸ºäº†å¾—åˆ°a specific context representationï¼Œæœ¬æ–‡ä½¿ç”¨AMRï¼ˆ(Abstract Meaning Representationï¼‰è¯­æ³•æˆ–è€…å¥æ³•ç»“æ„çš„ä¸Šä¸‹æ–‡ã€‚<br>å…¶ç”Ÿæˆçš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æ ¹æ®ç»™å®šçš„entity mentionä»¥åŠå¯¹åº”å…³ç³»ï¼Œé¦–å…ˆé€‰æ‹©entity mentionå¯èƒ½çš„ç±»å‹ï¼Œå¦‚å…³ç³»ä¸ºARG0 capital of ARG1åˆ™ARG0å¯èƒ½çš„ç±»å‹åˆ™ä¸ºå›½å®¶ï¼ŒåŒç†ARG1å¯èƒ½çš„ç±»å‹ä¸ºåŸå¸‚ã€‚éšåå°†æ‰€æœ‰entity mentionå¯èƒ½çš„å€™é€‰ç±»å‹é€šè¿‡ä¸€ä¸ªencoder-decoderæ¨¡å‹å¾—åˆ°ä¸€ä¸ªå•ä¸€çš„è¡¨è¾¾</p>
<p><img src="media/context%20specific.png" alt="context specifi"></p>
<p>3ã€Knowledge Representation</p>
<p>ç”±äºentity mentionçš„ç±»å‹åœ¨å¾ˆå¤šæƒ…å†µæ˜¯éå¸¸ä¾èµ–é¢†åŸŸç›¸å…³çš„çŸ¥è¯†åº“çš„ã€‚å› æ­¤æœ¬æ–‡ä¹Ÿå¯¹çŸ¥è¯†åº“è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ¨æ–­å‡ºåœ¨æŸä¸ªç›¸å…³é¢†åŸŸä¸‹æ›´ç»†ç²’åº¦çš„å®ä½“ã€‚ä¸ºä¾‹è®¡ç®—Knowledge Representationï¼Œé¦–å…ˆå¯¹entity mentionè·ŸçŸ¥è¯†åº“åšè¿æ¥ã€‚éšåæ ¹æ®é“¾æ¥çš„å®ä½“å’Œå®ä½“å¯¹åº”çš„å±æ€§ä»¥åŠç±»å‹ä¿¡æ¯æ„å»ºä¸€ä¸ªåŸºäºæƒé‡çš„äºŒæ­¥å›¾ã€‚æ„å»ºå¥½çš„äºŒæ­¥å›¾æ ¹æ® Large-scale information network embeddingç®—æ³•æ¥å¯¹è¿™ä¸ªäºŒæ­¥å›¾è®­ç»ƒå¹¶å¾—åˆ°å…¶åˆ†å¸ƒå¼è¡¨è¾¾ã€‚</p>
<p>æœ€åå¯¹äºä¸€ä¸ªentity mentionï¼Œå°†è¯¥entity mentionå¯¹åº”çš„ä¸‰ç§è¡¨è¾¾General Entity Representationï¼Œa specific context representationå’ŒKnowledge Representationæ•´åˆï¼Œé€šè¿‡ä¸€ä¸ªhierarchical X-means clusteringç®—æ³•å¾—åˆ°è¿™ä¸ªentity mentionåœ¨ä¸€ä¸ªåˆ†ç±»ä½“ç³»ä¸‹çš„typeä¿¡æ¯ã€‚æœ€ç»ˆå®Œæˆè¯†åˆ«å®ä½“ç±»å‹çš„ä¿¡æ¯ã€‚</p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>ç»†ç²’åº¦çš„å®ä½“è¯†åˆ«æ˜¯è¿™å‡ å¹´æ¯”è¾ƒæµè¡Œçš„å·¥ä½œã€‚ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯éœ€è¦å…ˆé¢„å®šä¹‰ä¸€ç»„å®ä½“æ‰€å±ç±»å‹ï¼Œéšåä½¿ç”¨å¤§é‡çš„æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒå¤šåˆ†ç±»å™¨ã€‚è¿™ç¯‡æ–‡ç« çš„åˆ›æ–°ç‚¹æ˜¯æå‡ºäº†ä¸€ä¸ªéç›‘ç£å­¦ä¹ çš„ç®—æ³•æ¥è¯†åˆ«å®ä½“æ‰€å±çš„typeï¼Œè¿™ç§éç›‘ç£çš„æ–¹æ³•åœ¨ç¼ºå°‘æ ‡æ³¨æ•°æ®çš„å‚ç›´é¢†åŸŸå…·æœ‰ä¸€å®šçš„å®ç”¨æ€§ã€‚æœ¬æ–‡çš„æ€è·¯ä¸»è¦æ˜¯é€šè¿‡æ–‡ç« ä¸­çš„entity mentionè·ŸçŸ¥è¯†åº“è¿›è¡Œè¿æ¥ï¼Œé€šè¿‡æ–‡ç« çš„ä¸Šä¸‹æ–‡å­¦ä¹ entity mentionçš„åˆ†å¸ƒå¼è¡¨è¾¾ï¼ŒåŒæ—¶é€šè¿‡å­¦ä¹ çŸ¥è¯†åº“ä¸­å®ä½“å’Œç±»å‹çš„åˆ†å¸ƒå¼è¡¨è¾¾ã€‚æœ€åå°†è¿™äº›è¡¨è¾¾é€å…¥ä¸€ä¸ªå±‚æ¬¡èšç±»ç®—æ³•ï¼Œentity mentionå¾—åˆ°çš„embeddingå’Œç›¸ä¼¼çš„çŸ¥è¯†åº“ç¬¦å·embeddingä¼šèšåˆ°åŒä¸€ä¸ªèšç±»ä¸‹ã€‚è¿›è€Œé€šè¿‡éç›‘ç£çš„æ–¹æ³•å¯¹entity mentionæ‰“ä¸Štypeçš„æ ‡ç­¾ã€‚å®éªŒè¯æ˜æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥è·Ÿç›‘ç£å­¦ä¹ èµ·åˆ°ç±»ä¼¼çš„æ•ˆæœã€‚</p>
<h1 id="ClusType-Effective-Entity-Recognition-and-Typing-by-Relation-Phrase-Based-Clustering"><a href="#ClusType-Effective-Entity-Recognition-and-Typing-by-Relation-Phrase-Based-Clustering" class="headerlink" title="ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering"></a><a href="http://nlp.cs.rpi.edu/paper/entitytyping.pdf" target="_blank" rel="external">ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Xiang Ren, Ahmed El-Kishky, Chi Wang, Fangbo Tao, Clare R. Voss, Heng Ji, Jiawei Han</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>University of Illinois at Urbana-Champaign,<br>Microsoft Research, Redmond,<br>Rensselaer Polytechnic Institute,<br>Army Research Laboratory, Adelphi</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Entity Recognition and Typing,<br>Relation Phrase Clustering</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>KDD, 2015</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>è¿œç¨‹ç›‘ç£æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸçš„å®ä½“æŠ½å–æ–¹é¢å­˜åœ¨é¢†åŸŸæ‰©å±•æ€§å·®ã€å®ä½“æ­§ä¹‰é—®é¢˜ä»¥åŠä¸Šä¸‹æ–‡ç¨€ç¼ºä¸‰å¤§é—®é¢˜ï¼Œæœ¬æ–‡ä¸»è¦ç ”ç©¶å¦‚ä½•æ”¹è¿›è¿™ä¸‰ä¸ªé—®é¢˜ã€‚</p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>é’ˆå¯¹ä¸Šè¿°çš„ä¸‰ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å„è‡ªå¯¹åº”çš„è§£å†³æ€è·¯ï¼šåªä½¿ç”¨æµ…å±‚çš„åˆ†ææ–¹æ³•ä¾‹å¦‚POSç­‰è§£å†³é¢†åŸŸç‹¬ç«‹æ€§é—®é¢˜ï¼›å¯¹entity mention(token span in the text document which refers to a real-world entity)åº”ç”¨è¯å½¢å’Œä¸Šä¸‹æ–‡è”åˆå»ºæ¨¡æ¥è§£å†³æ­§ä¹‰é—®é¢˜ï¼›æŒ–æ˜relation phraseå’Œentity mentionçš„å…±ç°æƒ…å†µï¼Œåˆ©ç”¨relation phraseå‰åå®ä½“ï¼ˆä¸»è¯­å’Œå®¾è¯­ï¼‰çš„ç±»åˆ«æ¥æ‰¾åˆ°ç›¸åŒçš„å…³ç³»ï¼Œè¿›è€Œè¾…åŠ©å®ä½“ç±»å‹çš„æ¨æ–­ã€‚åŸºäºä¸Šè¿°çš„æ€è·¯ï¼Œæœ¬æ–‡æå‡ºäº†ClusTypeçš„æ–¹æ³•ã€‚</p>
<p>ClusTypeçš„é—®é¢˜å®šä¹‰å¦‚ä¸‹ï¼šç»™å®šä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„æ–‡æ¡£é›†åˆï¼Œä¸€ä¸ªå®ä½“ç±»å‹é›†åˆä»¥åŠä¸€ä¸ªçŸ¥è¯†åº“ï¼Œä¸»è¦å®Œæˆä¸‰ä¸ªä»»åŠ¡ï¼šç¬¬ä¸€ï¼Œä»æ–‡æ¡£é›†åˆä¸­æŠ½å–å‡ºå€™é€‰çš„entity mentioné›†åˆï¼›ç¬¬äºŒï¼Œå°†ä¸€éƒ¨åˆ†entity mentioné“¾æ¥åˆ°çŸ¥è¯†åº“ï¼Œä½œä¸ºç§å­entity mentioné›†åˆï¼›ç¬¬ä¸‰ï¼Œå¯¹äºå‰©ä½™æœªå®ŒæˆçŸ¥è¯†é“¾æ¥çš„entity mentioné›†åˆï¼Œé¢„æµ‹æ¯ä¸€ä¸ªentity mentionçš„å¯¹åº”å®ä½“ç±»åˆ«ã€‚</p>
<p>æ ¹æ®ä»»åŠ¡çš„å®šä¹‰ï¼Œæ•´ä¸ªæ¡†æ¶ä¹Ÿåˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«è§£å†³è¿™ä¸‰ä¸ªä»»åŠ¡ã€‚</p>
<p>æœ¬æ–‡æ–¹æ¡ˆçš„å…·ä½“æ€è·¯å¦‚ä¸‹ï¼š</p>
<p>1ã€æ„å»ºå…³ç³»å›¾</p>
<p>å…³ç³»å›¾çš„åŸºæœ¬æ ·å¼å¦‚ä¸‹ï¼š  </p>
<p><img src="media/graph.png" alt="graph"></p>
<p>å›¾å½“ä¸­çš„èŠ‚ç‚¹ä¸»è¦åˆ†ä¸ºä¸‰ç§ï¼šentity mention, surface name, relation phrase.<br>å›¾ä¸­çš„è¾¹çš„ç±»å‹ä¹Ÿæœ‰ä¸‰ç§ï¼šentity mentionå’Œsurface nameçš„å…³ç³»ã€surface nameå’Œrelation phraseåœ¨è¯­æ–™ä¸­çš„å…±ç°æƒ…å†µã€entity mentionå’Œentity mentionçš„å…³ç³»ï¼Œè¡¨ç°entity mentionä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ã€‚è¿™ä¸‰ä¸ªå…³ç³»å‡æ˜¯é€šè¿‡é‚»æ¥çŸ©é˜µçš„å½¢å¼è¡¨ç¤ºã€‚<br>å…³äºä¸‰ç§è¦ç´ çš„ç¡®å®šï¼Œrelation phraseçš„ç¡®å®šä¸»è¦å‚è€ƒå¼€æ”¾åŸŸæŠ½å–çš„æ–¹æ³•ï¼Œentity mentionçš„ç¡®å®šæ–¹æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼šé¦–å…ˆæ‰¾åˆ°å›ºå®šé•¿åº¦çš„ä¸€ä¸ªé¢‘ç¹è¯ä¸²é›†ï¼›ä¸ºé›†åˆä¸­æ¯ä¸€ä¸ªè¯ä¸²è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„å¾—åˆ†ï¼Œå¾—åˆ†è¶Šé«˜è¯æ˜è¶Šéœ€è¦åˆå¹¶ï¼›åœ¨åˆå¹¶çš„è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨è´ªå¿ƒç®—æ³•ï¼Œä»å¾—åˆ†æœ€é«˜å¼€å§‹åˆå¹¶ï¼Œç›´åˆ°æ‰€æœ‰å¾—åˆ†å‡ä½äºæŸä¸€é˜ˆå€¼ã€‚</p>
<p>2ã€ç§å­é›†åˆçš„ç”Ÿæˆ</p>
<p>è¿™é‡Œåˆ©ç”¨äº†dbpedia-spotlightå·¥å…·è¿›è¡Œentity mentionåˆ°çŸ¥è¯†åº“çš„æ˜ å°„ï¼Œåªé€‰å–ç½®ä¿¡åº¦å¾—åˆ†é«˜äº0.8çš„ä½œä¸ºæœ‰æ•ˆè¾“å‡ºã€‚</p>
<p>3ã€å®ä½“ç±»å‹æ¨æ–­<br>ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š<br><img src="media/function.png" alt="function"><br>å…¬å¼å…±åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š<br>ç¬¬ä¸€éƒ¨åˆ†éµå¾ªå®ä½“å…³ç³»å…±ç°å‡è®¾ï¼šå¦‚æœä¸€ä¸ªsurface nameç»å¸¸åœ¨relation phraseå‰åå‡ºç°ï¼Œé‚£ä¹ˆå®ƒçš„ç±»å‹åº”è¯¥åŒrelation phraseå‰åå®ä½“çš„ç±»å‹ç›¸å…³ã€‚  </p>
<p>ç¬¬äºŒéƒ¨åˆ†éµå¾ªä¸¤ä¸ªå‡è®¾ã€‚<br>å‡è®¾ä¸€ï¼šå¦‚æœä¸¤ä¸ªrelation phraseç›¸ä¼¼ï¼Œé‚£ä¹ˆä»–ä»¬å‰åå®ä½“çš„ç±»å‹ä¹Ÿåº”è¯¥ç›¸ä¼¼ï¼›<br>å‡è®¾äºŒï¼šåˆ¤æ–­ä¸¤ä¸ªrelation phraseç›¸ä¼¼çš„ç‰¹å¾ä¸ºè¯å½¢ã€ä¸Šä¸‹æ–‡å’Œå…¶å‰åå®ä½“çš„ç±»å‹ã€‚<br>å› æ­¤ï¼Œç¬¬äºŒéƒ¨åˆ†çš„ä½œç”¨åœ¨äºæ ¹æ®ä¸¤ä¸ªå‡è®¾å»ºæ¨¡ä¸€ä¸ªåŸºäºjoint non-negative matrix factorizationçš„multi-view clustering.</p>
<p>ç¬¬ä¸‰éƒ¨åˆ†å°±æ˜¯å»ºæ¨¡entity mentionå¯¹åº”å®ä½“ç±»åˆ«ã€entity mentionä¹‹é—´çš„å…³ç³»ä»¥åŠå¼•å…¥ç§å­é›†åˆçš„ç›‘ç£ï¼Œåˆ©ç”¨ä¸€ä¸ªentity mentionçš„surface nameå’Œrelation phraseå¯¹åº”çš„å…³ç³»ç±»åˆ«æ¨æ–­å…³ç³»ç±»å‹ï¼ŒåŒæ—¶è€ƒè™‘åˆ°ç›¸ä¼¼entity mentionçš„ä¸€è‡´æ€§ä»¥åŠå¯¹äºç§å­é›†åˆçš„é¢„æµ‹è¯¯å·®å‡½æ•°ã€‚</p>
<h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>æœ¬æ–‡ä¸»è¦å€Ÿé‰´ä¸¤æ–¹é¢çš„å·¥ä½œï¼Œä¸€éƒ¨åˆ†æ˜¯è¿œè·ç¦»ç›‘ç£çš„æ–¹æ³•ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯å¼€æ”¾å…³ç³»æŠ½å–ã€‚<br>è¿œè·ç¦»ç›‘ç£çš„å·¥ä½œä¸»è¦æœ‰ï¼š<br>1ã€N. Nakashole, T. Tylenda, and G. Weikum. Fine-grained semantic typing of emerging entities. In ACL, 2013.<br>2ã€T. Lin, O. Etzioni, et al. No noun phrase left behind: de- tecting and typing unlinkable entities. In EMNLP, 2012.<br>3ã€X. Ling and D. S. Weld. Fine-grained entity recognition. In AAAI, 2012.<br>å¼€æ”¾å…³ç³»æŠ½å–çš„å·¥ä½œä¸»è¦æœ‰ï¼š<br>1ã€A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP, 2011.</p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡é€šè¿‡å¯¹äºè¿œç¨‹ç›‘ç£æ–¹æ³•çš„ç¼ºé™·åˆ†æï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…³ç³»çŸ­è¯­çš„å®ä½“è¯†åˆ«æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªé¢†åŸŸæ— å…³çš„ç”Ÿæˆrelation phraseå’Œentity mentionã€‚é€šè¿‡å°†å…³ç³»çŸ­è¯­çš„èšç±»å’Œå®ä½“ç±»å‹çš„è¯†åˆ«è”åˆå»ºæ¨¡ï¼Œå¯ä»¥åœ¨è§£å†³å®ä½“æ­§ä¹‰å’Œä¸Šä¸‹æ–‡é—®é¢˜ä¸Šå‘æŒ¥å¾ˆå¤§çš„ä½œç”¨ï¼Œè€Œä¸”å¯ä»¥æ ¹æ®entity mentionçš„surface nameå’Œrelation phraseé¢„æµ‹å…³ç³»ç±»å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œå°†å®ä½“è¯†åˆ«å’Œå…³ç³»è¯†åˆ«è¿›è¡Œè”åˆå»ºæ¨¡å¯ä»¥èµ·åˆ°ä¸€ä¸ªç›¸äº’ä¿ƒè¿›çš„ä½œç”¨ï¼Œè€Œä¸”å¯ä»¥å¾ˆå¥½çš„é¿å…åœ¨è¿™ä¸¤ä¸ªä»»åŠ¡å½“ä¸­å¼•å…¥æ·±åº¦è¯­æ³•åˆ†æçš„å·¥å…·å¦‚ä¾å­˜ã€å¥æ³•åˆ†æç­‰ï¼Œå‡å°‘è¯¯å·®ç§¯ç´¯å’Œé¢†åŸŸä¾èµ–æ€§ã€‚æœªæ¥ä¸¤ç§ä»»åŠ¡ç»“åˆä¾æ—§æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç ”ç©¶æ–¹å‘å’Œçƒ­ç‚¹ã€‚</p>
<h1 id="Bootstrapped-Text-level-Named-Entity-Recognition-for-Literature"><a href="#Bootstrapped-Text-level-Named-Entity-Recognition-for-Literature" class="headerlink" title="Bootstrapped Text-level Named Entity Recognition for Literature"></a><a href="http://people.eng.unimelb.edu.au/tbaldwin/pubs/acl2016-ner.pdf" target="_blank" rel="external">Bootstrapped Text-level Named Entity Recognition for Literature</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Julian Brookeï¼ŒTimothy Baldwinï¼ŒAdam Hammond</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>English and Comparative Literature San Diego State University<br>Computing and Information Systems The University of Melbourne</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>NERï¼ŒBrown clusteringï¼ŒText-level context classifier</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ACL2016</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>åœ¨æ— æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹Literatureåšå‘½åå®ä½“è¯†åˆ«</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ¨¡å‹ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼š<br>1ã€Corpus preparation and segmentation<br>ä½¿ç”¨GutenTag toolå¯¹è¯­æ–™åšåŸºæœ¬çš„åç§°åˆ‡åˆ†<br>2ã€Brown clustering<br>åœ¨é¢„å…ˆåˆ‡åˆ†å¥½çš„é¢„æ–™ä¸ŠåšBrown clusteringã€‚æ ¹æ®Brown clusteringçš„èšç±»ä¸­çš„æ¯ä¸ªç±»çš„rankå€¼ï¼Œå°†èšç±»ç»“æœåˆ†æˆä¸‰ä¸ªç±»åˆ«ï¼ˆPERSONï¼ŒLOCATIONï¼Œcatch- all categoryï¼‰å¹¶å°†å…¶ä½œä¸ºBootstrapçš„ç§å­è¿›è¡Œè®­ç»ƒã€‚<br>3ã€Text-level context classifier<br>ä¸ºäº†è§£å†³Brown clusteringèšç±»ç»“æœå¯èƒ½å‡ºç°çš„ä¸€äº›confusionï¼Œå¼•å…¥äº†Text-level context classifierçš„æ€æƒ³ã€‚æ„å»ºåç§°ç‰¹å¾å‘é‡ï¼Œå°†ç§å­é›†æ•°æ®æ”¾åˆ°LRæ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°åˆ†ç±»æ¨¡å‹ã€‚<br>4ã€Improved phrase classification<br>ä¸ºè§£å†³æ¨¡å‹å¯¹çŸ­è¯­åè¯åˆ†ç±»ä¸å‡†ç¡®é—®é¢˜ï¼Œå¼•å…¥äº†æ”¹è¿›çš„çŸ­è¯­åç§°åˆ†ç±»æ–¹æ³•ï¼Œåœ¨LRæ¨¡å‹å¾—åˆ°çš„p(t|r)å€¼çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¯¹å…¶ä¼˜åŒ–å¾—åˆ°ä¿®æ­£çš„pâ€™(t|r) ï¼Œä¿®æ­£æ–¹æ³•å¦‚ä¸‹ï¼š<br> <img src="media/imag1.png" alt="imag1"></p>
<h2 id="èµ„æº"><a href="#èµ„æº" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€datasetï¼š<a href="https://www.gutenberg.org" target="_blank" rel="external">https://www.gutenberg.org</a><br>2ã€GutenTag toolï¼š<a href="http://www.projectgutentag.org" target="_blank" rel="external">http://www.projectgutentag.org</a>   </p>
<h2 id="ç›¸å…³å·¥ä½œ-1"><a href="#ç›¸å…³å·¥ä½œ-1" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>åœ¨Literatureä¸ŠåšNERä»»åŠ¡çš„å·¥ä½œåŒ…æ‹¬ï¼š<br>1ã€(He et al., 2013)character speech identification<br>2ã€(Bamman et al., 2014)analysis of characterization<br>3ã€(Vala et al., 2015)character identification<br>4ã€(Vala et al. 2015)character identification deal the multiple aliases of the same character problem</p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œä½¿ç”¨äº†æ— ç›‘ç£å­¦ä¹ æ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸ(fiction)çŸ¥è¯†åšNERï¼Œå¹¶å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚ä½†æ˜¯æœ¬æ–‡æ–¹æ³•ä¸»è¦ç ”ç©¶ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„NERï¼Œå› æ­¤æœ¬æ–¹æ³•ä½¿ç”¨åœ¨è·¨é¢†åŸŸè·¨è¯­è¨€çš„NERè¯†åˆ«ä»»åŠ¡ä¸­å¹¶ä¸èƒ½è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœï¼Œæ–¹æ³•å…·æœ‰ä¸€å®šçš„å±€é™æ€§ã€‚</p>
<h1 id="Recognizing-Named-Entities-in-Tweets"><a href="#Recognizing-Named-Entities-in-Tweets" class="headerlink" title="Recognizing Named Entities in Tweets"></a><a href="http://people.dbmi.columbia.edu/~szhang/P11-1037.pdf" target="_blank" rel="external">Recognizing Named Entities in Tweets</a></h1><h2 id="ä½œè€…-3"><a href="#ä½œè€…-3" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Xiaohua Liu, Shaodian Zhang, Furu Wei, Ming Zhou</p>
<h2 id="å•ä½-3"><a href="#å•ä½-3" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Harbin Institute of Technology,<br>Shanghai Jiao Tong University,<br>Microsoft Research Asia</p>
<h2 id="å…³é”®è¯-3"><a href="#å…³é”®è¯-3" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Named Entity Recognition, Semi-Supervised Learning</p>
<h2 id="æ–‡ç« æ¥æº-3"><a href="#æ–‡ç« æ¥æº-3" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ACL, 2011</p>
<h2 id="é—®é¢˜-3"><a href="#é—®é¢˜-3" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>å¦‚ä½•å»ºç«‹ä¸€ç§åŠç›‘ç£å­¦ä¹ çš„æ¨¡å‹å¯¹ä½¿ç”¨éæ­£å¼è¯­è¨€çš„tweetè¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Ÿ</p>
<h2 id="æ¨¡å‹-3"><a href="#æ¨¡å‹-3" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>ç°æœ‰çš„åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€NERå·¥å…·è§£å†³éæ­£å¼è¯­è¨€å ä¸»å¯¼çš„tweetæ—¶å¸¸å¸¸ä¼šå¤±æ•ˆï¼Œå¾—ä¸åˆ°ä»¤äººæ»¡æ„çš„ç»“æœï¼Œè€Œtwitterä½œä¸ºä¸€ç§ä¸»æµçš„ç¤¾äº¤åª’ä½“ï¼Œæœ‰ç€ä¸°å¯Œçš„è¯­æ–™å’Œéå¸¸é«˜çš„ç ”ç©¶ä»·å€¼ã€‚æœ¬æ–‡ä»¥tweetä¸ºç ”ç©¶å¯¹è±¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºbootstrappingçš„åŠç›‘ç£å­¦ä¹ æ–¹æ¡ˆã€‚</p>
<p>tweetçš„NERä»»åŠ¡åŒ…æ‹¬å››ç±»å®ä½“ï¼šPersonã€Locationã€Organizationå’ŒProductï¼Œæ ‡æ³¨æ–¹æ³•ç”¨BILOUæ ‡æ³¨æ³•ï¼Œè€Œæ²¡æœ‰ç”¨ç»å…¸çš„IOBæ ‡æ³¨æ³•ã€‚</p>
<p>æœ¬æ–‡æ–¹æ¡ˆçš„å…·ä½“æ€è·¯å¦‚ä¸‹ï¼š</p>
<p><img src="media/knn-crf.png" alt="knn-crf"></p>
<p>1ã€KNNåˆ†ç±»å™¨</p>
<p>å°†tweetä¸­çš„æ¯ä¸ªè¯ç”¨è¯è¢‹æ¨¡å‹è¡¨ç¤ºï¼Œè¾“å…¥åˆ°KNNä¸­å¾—åˆ°ä¸€ä¸ªåˆ†ç±»æ ‡ç­¾ï¼Œè¿™ä¸ªæ ‡ç­¾ä½œä¸ºCRFæ ‡æ³¨æ—¶çš„è¾“å…¥ã€‚</p>
<p>2ã€CRFæ ‡æ³¨å™¨</p>
<p>NERæ˜¯ä¸€ä¸ªå…¸å‹çš„åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼ŒCRFæ˜¯è§£å†³åºåˆ—æ ‡æ³¨é—®é¢˜çš„ä¸€ä¸ªå…¸å‹æ–¹æ³•ã€‚</p>
<p>3ã€è®­ç»ƒè¿‡ç¨‹ï¼š</p>
<p>ï¼ˆ1ï¼‰å…ˆæ ¹æ®å·²æœ‰æ ‡æ³¨æ•°æ®ï¼Œè®­ç»ƒå¥½åˆå§‹çš„KNNå’ŒCRFæ¨¡å‹ã€‚<br>ï¼ˆ2ï¼‰è·å¾—æœªæ ‡æ³¨çš„tweetï¼Œæ¯æ¡tweetä¸­çš„æ¯ä¸ªè¯éƒ½ç»è¿‡KNNåˆ†ç±»å™¨ï¼Œå¾—åˆ°ä¸€ä¸ªåˆ†ç±»æ ‡ç­¾å’Œç›¸åº”çš„æ¦‚ç‡ï¼Œå¦‚æœè¿™ä¸ªæ¦‚ç‡å¤§äºé¢„è®¾é˜ˆå€¼ï¼Œåˆ™æ›´æ–°è¿™ä¸ªæ ‡ç­¾ç»™è¯¥è¯ã€‚æ•´ä¸ªtweetç»è¿‡KNNä¹‹åï¼Œä½œä¸ºç‰¹å¾è¾“å…¥åˆ°CRFæ¨¡å‹ä¸­è¿›è¡Œé¢„æµ‹ï¼Œå¦‚æœé¢„æµ‹å‡ºçš„ç»“æœæ¦‚ç‡å¤§äºé¢„è®¾é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºè¯¥æ ‡æ³¨ç»“æœå¯é ï¼ŒåŠ å…¥å¯é ç»“æœé›†ä¸­ã€‚<br>ï¼ˆ3ï¼‰å½“å¯é ç»“æœé›†çš„æ•°é‡è¾¾åˆ°N=1000æ—¶ï¼Œåˆ™é‡æ–°è®­ç»ƒKNNå’ŒCRFæ¨¡å‹ï¼Œå¹¶ä¸”æ¸…ç©ºå¯é ç»“æœé›†ï¼Œç»§ç»­ï¼ˆ2ï¼‰çš„è¿‡ç¨‹ã€‚</p>
<h2 id="ç›¸å…³å·¥ä½œ-2"><a href="#ç›¸å…³å·¥ä½œ-2" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>åŸºäºbootstrappingåšNERä»»åŠ¡çš„å·¥ä½œè¿˜åŒ…æ‹¬ï¼š</p>
<p>1ã€Instance weighting for domain adaptation in nlp, 2007<br>2ã€Domain adaption bootstrapping for named entity recognition, 2009</p>
<h2 id="ç®€è¯„-3"><a href="#ç®€è¯„-3" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡æ˜¯æ¯”è¾ƒæ—©çš„æ–‡ç« äº†ï¼Œç®—æ˜¯æ¯”è¾ƒæ—©åœ°æ¢ç´¢tweetæ–‡æœ¬æŒ–æ˜ã€‚bootstrappingæ˜¯ä¸€ç§ç»å…¸çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä»å¤§é‡çš„éæ ‡æ³¨æ–‡æœ¬ä¸­è¿›è¡Œå­¦ä¹ å’Œè¡¥å……ï¼Œæ¥æé«˜è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡ã€‚tweetæ˜¯ä¸€ç§éæ­£å¼è¯­è¨€çš„æ–‡æœ¬ï¼Œç°æœ‰çš„NLPå·¥å…·åŸºæœ¬ä¸Šéƒ½ä¸å¥½ç”¨ï¼ŒåŒ…æ‹¬å¾®åšã€è®ºå›çš„æ–‡æœ¬éƒ½é¢ä¸´è¿™æ ·çš„é—®é¢˜ï¼Œè€Œä¸”è¿™æ ·çš„æ–‡æœ¬å æ®ç€æ›´å¤§çš„æ¯”é‡ï¼Œéå¸¸æœ‰å¿…è¦å¯¹ç±»ä¼¼çš„æ–‡æœ¬è¿›è¡ŒNLPå·¥å…·çš„ç ”ç©¶ï¼Œå¤§æ¦‚æƒ³äº†ä¸¤ç§æ€è·¯ï¼Œè¦ä¹ˆä¸“é—¨åœ°æ¥ç ”ç©¶ä¸€å¥—é€‚åˆè¿™ç§éæ­£å¼æ–‡æœ¬çš„å·¥å…·ï¼Œè¦ä¹ˆæƒ³åŠæ³•å°†è¿™æ ·çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ­£å¼çš„è¯­è¨€ï¼Œç”¨ç°æœ‰çš„å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚ç°åœ¨å¾ˆç«çš„chatbotå¯¹è¯ç†è§£ä¹Ÿé¢ä¸´è¿™æ ·çš„é—®é¢˜ï¼Œå¤§å®¶åœ¨å’Œbotå¯¹è¯çš„æ—¶å€™è¯´çš„è¯ä¹Ÿæ˜¯ç±»ä¼¼çš„éæ­£å¼è¯­è¨€ï¼Œå¦‚ä½•å‡†ç¡®ç†è§£å’Œåˆ†æè¿™ç±»è¯ï¼Œå¯¹äºchatbotèƒ½å¦çœŸçš„è¢«åº”ç”¨è‡³å…³é‡è¦ã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>NERçš„åº”ç”¨åœºæ™¯éå¸¸å¹¿æ³›ï¼ŒåŸºäºç›‘ç£å­¦ä¹ çš„è®­ç»ƒæ–¹æ³•æ˜¯æœ€ç®€å•ã€æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å¸¸å¸¸ä¼šé‡åˆ°è®­ç»ƒæ•°æ®éš¾ä»¥è·å¾—çš„å°´å°¬å¢ƒåœ°ï¼Œé‚£ä¹ˆåŠç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ çš„ç ”ç©¶æ­£æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå€¼å¾—å…³æ³¨ï¼æ„Ÿè°¢@é«˜æ¡“ @éŸ©å…¶ç› @min279 @zhangjun å››ä½ç«¥é‹çš„è¾›å‹¤å·¥ä½œã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•è¨€&quot;&gt;&lt;a href=&quot;#å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;å¼•è¨€&quot;&gt;&lt;/a&gt;å¼•è¨€&lt;/h1&gt;&lt;p&gt;å‘½åå®ä½“è¯†åˆ«æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä¸€ä¸ªéå¸¸åŸºç¡€çš„å·¥ä½œï¼Œæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å…³é”®çš„ä¸€ä¸ªç¯èŠ‚ã€‚ç›‘ç£å­¦ä¹ æ˜¯è§£å†³å‘½åå®ä½“è¯†åˆ«çš„ä¸€ä¸ªåŸºæœ¬æ‰‹æ®µï¼Œä½†æ ‡æ³¨æ•°æ®çš„è·å–æˆ
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æœ¬å‘¨å€¼å¾—è¯»(2016.11.28-2016.12.02)</title>
    <link href="http://rsarxiv.github.io/2016/12/04/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-11-28-2016-12-02/"/>
    <id>http://rsarxiv.github.io/2016/12/04/æœ¬å‘¨å€¼å¾—è¯»-2016-11-28-2016-12-02/</id>
    <published>2016-12-04T18:06:33.000Z</published>
    <updated>2016-12-04T18:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ä¸€å‘¨å€¼å¾—è¯»"><a href="#ä¸€å‘¨å€¼å¾—è¯»" class="headerlink" title="ä¸€å‘¨å€¼å¾—è¯»"></a>ä¸€å‘¨å€¼å¾—è¯»</h1><h2 id="A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation"><a href="#A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-Generation" class="headerlink" title="A Simple, Fast Diverse Decoding Algorithm for Neural Generation "></a><a href="http://t.cn/Rfj8F3k" target="_blank" rel="external">A Simple, Fast Diverse Decoding Algorithm for Neural Generation </a></h2><p>ã€beam searchã€‘åœ¨ç”¨seq2seqåšä¸€äº›nlpä»»åŠ¡çš„æ—¶å€™ï¼Œè§£ç å™¨è´Ÿè´£å°†ç»“æœä¸€ä¸ªä¸ªåœ°è§£å‡ºæ¥ã€‚è§£ç å‡ºçš„ç»“æœåº”è¯¥å…·æœ‰å¤šæ ·æ€§çš„ç‰¹ç‚¹ï¼Œå°¤å…¶æ˜¯åœ¨chatbotåº”ç”¨ä¸­ä½“ç°æ›´ä¸ºçªå‡ºã€‚ä¼ ç»Ÿçš„beam searchç®—æ³•åœ¨è§£ç æ—¶å¸¸å¸¸ä¼šè§£å‡ºä¸€äº›éå¸¸å®‰å…¨ä½†æ˜¯æ²¡æœ‰å®é™…æ„ä¹‰çš„responseï¼Œç±»ä¼¼äºâ€œå‘µå‘µå‘µâ€ï¼Œâ€œæˆ‘è®¤ä¸ºæ˜¯è¿™æ ·çš„â€è¿™é‡Œçš„è¯ã€‚æœ¬æ–‡çš„å·¥ä½œé’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›ç‰ˆçš„beam searchç®—æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªæƒ©ç½šå› å­æ¥å½±å“æ’åºç»“æœï¼Œä»è€Œä½¿å¾—è§£ç å‡ºçš„ç»“æœæ›´åŠ å¤šæ ·æ€§ã€‚å»ºè®®ç ”ç©¶seq2seqæˆ–è€…å°è¯•ç”¨å®ƒæ¥è§£å†³ä¸€äº›é—®é¢˜çš„ç«¥é‹å¯ä»¥ç²¾è¯»æ­¤æ–‡ã€‚æœ¬æ–‡æ¥è‡ªJiwei Liã€‚</p>
<h2 id="Dialogue-Learning-With-Human-In-The-Loop"><a href="#Dialogue-Learning-With-Human-In-The-Loop" class="headerlink" title="Dialogue Learning With Human-In-The-Loop "></a><a href="http://t.cn/Rf8XOcr" target="_blank" rel="external">Dialogue Learning With Human-In-The-Loop </a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘ã€åœ¨çº¿å­¦ä¹ ã€‘åœ¨çº¿å­¦ä¹ æ˜¯chatbotåœ¨ä¸äººäº¤äº’çš„è¿‡ç¨‹ä¸­è‡ªåŠ¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œå¿«é€Ÿè€Œä¸”æœ‰æ•ˆã€‚æœ¬æ–‡ç»™å‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åœ¨çº¿äº¤äº’å­¦ä¹ æ–¹æ¡ˆï¼Œä½œè€…æ˜¯Jiwei Liã€‚å»ºè®®ç ”ç©¶æˆ–è€…åšchatbotåº”ç”¨çš„ç«¥é‹å¯ä»¥å¥½å¥½ç ”è¯»æ­¤æ–‡ã€‚</p>
<h2 id="Visual-Dialog"><a href="#Visual-Dialog" class="headerlink" title="Visual Dialog "></a><a href="http://t.cn/RfH7BWW" target="_blank" rel="external">Visual Dialog </a></h2><p>ã€å¤šæ¨¡æ€å¯¹è¯ã€‘å¤šæ¨¡æ€é—®ç­”ï¼ˆVQAï¼‰æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½ç©çš„ä»»åŠ¡ï¼Œæœ¬æ–‡åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†ä¸€ä¸ªæ›´åŠ å¤æ‚è€Œä¸”æœ‰æ„æ€çš„ä»»åŠ¡ï¼Œå³ç»™å®šä¸€å¼ å›¾åƒï¼Œç»™å‡ºè‹¥å¹²ä¸ªé—®å’Œç­”çš„å†å²å¯¹è¯ï¼Œæå‡ºä¸€ä¸ªæ–°é—®é¢˜ï¼Œè¦æ±‚ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚é—®é¢˜ä¸ä»…ä»…éœ€è¦ç†è§£å›¾ç‰‡ï¼Œè€Œä¸”éœ€è¦ç†è§£å†å²å¯¹è¯ã€‚æ–°çš„ä»»åŠ¡æ„å‘³ç€æ–°çš„å‘ï¼Œæ–‡ä¸­ç»™å‡ºäº†ä¸€äº›å¸¸è§NNæ¨¡å‹ä½œä¸ºbaselineï¼Œæ„Ÿå…´è¶£çš„ç«¥é‹å¯ä»¥å…¥å‘ã€‚</p>
<h2 id="Neural-Machine-Translation-with-Latent-Semantic-of-Image-and-Text"><a href="#Neural-Machine-Translation-with-Latent-Semantic-of-Image-and-Text" class="headerlink" title="Neural Machine Translation with Latent Semantic of Image and Text "></a><a href="http://t.cn/RfjRQMP" target="_blank" rel="external">Neural Machine Translation with Latent Semantic of Image and Text </a></h2><p>ã€Visual NMTã€‘å°±åœ¨NMTè¢«è®¨è®ºåœ°å¦‚ç«å¦‚è¼çš„æ—¶å€™ï¼Œè¿˜æœ‰ä¸€éƒ¨åˆ†å·¥ä½œæ˜¯ç»“åˆå¤šæ¨¡æ€ï¼ˆå›¾ç‰‡ï¼‰æ¥åšæœºå™¨ç¿»è¯‘ï¼Œå› ä¸ºäººç±»è·å–ä¿¡æ¯ä¸ä»…ä»…å¯ä»¥é€šè¿‡æ–‡å­—ï¼Œå›¾ç‰‡ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„å­¦ä¹ èµ„æºã€‚</p>
<h2 id="Scalable-Bayesian-Learning-of-Recurrent-Neural-Networks-for-Language-Modeling"><a href="#Scalable-Bayesian-Learning-of-Recurrent-Neural-Networks-for-Language-Modeling" class="headerlink" title="Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling "></a><a href="http://t.cn/RfjELTY" target="_blank" rel="external">Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling </a></h2><p>ã€è´å¶æ–¯å­¦ä¹ ã€‘é€šè¿‡BPTTæ¥è®­ç»ƒçš„RNNåœ¨è§£å†³é—®é¢˜ä¸Šä¼šå­˜åœ¨è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œä¸€ä¸ªä¸»è¦åŸå› æ˜¯éšæœºä¼˜åŒ–è®­ç»ƒæ— æ³•ç»™å‡ºæ¨¡å‹æƒé‡çš„æ¦‚ç‡ä¼°è®¡ï¼Œæœ¬æ–‡é€šè¿‡æœ€è¿‘stochastic gradient Markov Chain Monte Carloçš„ç ”ç©¶æ¥è¯•ç€å­¦ä¹ æ¨¡å‹æƒé‡çš„æ¦‚ç‡ã€‚è¯­è¨€æ¨¡å‹çš„å®éªŒç»“æœå’Œå…¶ä»–çš„ç›¸å…³å®éªŒç»“æœè¡¨æ˜æœ¬æ–‡æ‰€ç”¨çš„æ–¹æ³•ç¡®å®æœ‰æ•ˆã€‚</p>
<h2 id="Learning-Python-Code-Suggestion-with-a-Sparse-Pointer-Network"><a href="#Learning-Python-Code-Suggestion-with-a-Sparse-Pointer-Network" class="headerlink" title="Learning Python Code Suggestion with a Sparse Pointer Network "></a><a href="http://t.cn/RfjEjY5" target="_blank" rel="external">Learning Python Code Suggestion with a Sparse Pointer Network </a></h2><p>ã€ä»£ç è¡¥å…¨ã€‘æœ¬æ–‡ç ”ç©¶çš„é—®é¢˜éå¸¸æœ‰æ„æ€ï¼Œå°±æ˜¯å¤§å®¶å¸¸è§çš„IDEä»£ç è¡¥å…¨åŠŸèƒ½ã€‚ç°æœ‰çš„IDEå¯¹é™æ€ç¼–ç¨‹è¯­è¨€æ”¯æŒçš„æ¯”è¾ƒå¥½ï¼Œå¯¹äºåŠ¨æ€ç¼–ç¨‹è¯­è¨€æ”¯æŒçš„ä¸€èˆ¬ï¼Œè€Œä¸”ä¸€èˆ¬éƒ½æ˜¯è¡¥å…¨æŸä¸ªå‡½æ•°æˆ–è€…æ–¹æ³•ä¹‹ç±»çš„ï¼Œè€Œä¸èƒ½ç»™å‡ºæ›´å¤æ‚çš„ä»£ç ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œæ„é€ äº†ä¸€ä¸ªå¤§å‹çš„python codeæ•°æ®é›†ï¼Œå¹¶ä¸”ç”¨äº†æ¯”è¾ƒæµè¡Œçš„Pointer Networkæ¨¡å‹æ¥åšç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œå–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚ä»£ç è¡¥å…¨åœ¨å®é™…åº”ç”¨ä¸­éå¸¸æœ‰ç”¨ï¼Œä½†æƒ³åšåˆ°å¾ˆå¤æ‚ã€å¾ˆæ™ºèƒ½çš„è¡¥å…¨è¿˜æœ‰å¾ˆé•¿çš„è·¯ã€‚ä¸è¿‡è¿™ä¸ªtopicè¿˜æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„ä¸œè¥¿ã€‚</p>
<h2 id="Joint-Copying-and-Restricted-Generation-for-Paraphrase"><a href="#Joint-Copying-and-Restricted-Generation-for-Paraphrase" class="headerlink" title="Joint Copying and Restricted Generation for Paraphrase "></a><a href="http://t.cn/RfHAsim" target="_blank" rel="external">Joint Copying and Restricted Generation for Paraphrase </a></h2><p>ã€NLGã€‘æœ¬æ–‡çš„æ€è·¯ä¸Pointer Networkæˆ–è€…Copynetç±»ä¼¼ï¼Œåœ¨ç”¨seq2seqåšè‡ªç„¶è¯­è¨€ç”Ÿæˆæ—¶ï¼Œå¢åŠ ä¸€ä¸ªåˆ¤æ–­çš„ç¯èŠ‚ï¼Œæ¥å†³å®šæ¥ä¸‹æ¥çš„è¿™ä¸ªè¯æ˜¯ä»sourceæ¥copyè¿˜æ˜¯ç”¨decoderæ¥rewriteã€‚</p>
<h2 id="Context-aware-Natural-Language-Generation-with-Recurrent-Neural-Networks"><a href="#Context-aware-Natural-Language-Generation-with-Recurrent-Neural-Networks" class="headerlink" title="Context-aware Natural Language Generation with Recurrent Neural Networks "></a><a href="http://t.cn/RfEClfC" target="_blank" rel="external">Context-aware Natural Language Generation with Recurrent Neural Networks </a></h2><p>ã€NLGã€‘è®ºæ–‡çš„æ–¹æ³•ã€æ¨¡å‹æ²¡æœ‰å¤ªå¤šçš„å€¼å¾—è¯´çš„åœ°æ–¹ï¼Œå€’æ˜¯åº”ç”¨çš„ç‚¹éå¸¸æœ‰æ„æ€ï¼Œæ ¹æ®å•†å“çš„ä¸Šä¸‹æ–‡æ¥ä¼ªé€ è¯„è®ºï¼Œäººå·¥è¯„åˆ¤æ—¶æœ‰50%ä»¥ä¸Šçš„ä¼ªé€ è¯„è®ºéƒ½é€šè¿‡äº†ï¼Œ90%ä»¥ä¸Šéª—è¿‡äº†ç°æœ‰çš„è¯†åˆ«ç®—æ³•ã€‚æœ‰ç‚¹é“é«˜ä¸€å°ºé­”é«˜ä¸€ä¸ˆçš„æ„Ÿè§‰ï¼Œå¦‚æœè¿™ç¯‡paperçš„ç»“æœç¡®å®è¿™ä¹ˆç‰›çš„è¯ï¼Œç¡®å®å¾ˆæœ‰æ„æ€ï¼Œå€¼å¾—ç ”ç©¶ä¸€ä¸‹ã€‚</p>
<h2 id="MS-MARCO-A-Human-Generated-MAchine-Reading-COmprehension-Dataset"><a href="#MS-MARCO-A-Human-Generated-MAchine-Reading-COmprehension-Dataset" class="headerlink" title="MS MARCO: A Human Generated MAchine Reading COmprehension Dataset "></a><a href="http://t.cn/RfH2eXu" target="_blank" rel="external">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset </a></h2><p>ã€æœºå™¨é˜…è¯»ç†è§£ã€‘ã€æ•°æ®ç¦åˆ©ã€‘å¾®è½¯æ”¾å‡ºäº†ä¸€ä¸ª100kè§„æ¨¡çš„æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œæ•°æ®æ¥æºäºçœŸå®çš„Bingæœç´¢queryã€‚æ•°æ®åŒ…æ‹¬ï¼šqueryã€10ä¸ªç›¸å…³çš„passageå’Œqueryå¯¹åº”çš„answerã€‚</p>
<h2 id="NewsQA-A-Machine-Comprehension-Dataset"><a href="#NewsQA-A-Machine-Comprehension-Dataset" class="headerlink" title="NewsQA: A Machine Comprehension Dataset "></a><a href="http://t.cn/Rf8MPnf" target="_blank" rel="external">NewsQA: A Machine Comprehension Dataset </a></h2><p>ã€æœºå™¨é˜…è¯»ç†è§£ã€‘ã€æ•°æ®ç¦åˆ©ã€‘Maluubaå…¬å¸æ”¾å‡ºä¸€ä¸ªæ–°çš„æœºå™¨é˜…è¯»ç†è§£çš„æ•°æ®é›†ï¼Œè§„æ¨¡åœ¨100kå·¦å³ï¼Œæ•°æ®æ¥æºä¸ºCNNæ–°é—»ã€‚é€šè¿‡ç”¨å¤šä¸ªä¹‹å‰è¡¨ç°æ¯”è¾ƒå¥½çš„NNæ¨¡å‹å’Œäººå·¥ç»“æœå¯¹æ¯”ï¼Œå‘ç°F1æŒ‡æ ‡å­˜åœ¨25.3%çš„å·®è·ï¼Œè¯´æ˜æœ¬æ•°æ®é›†éœ€è¦æ›´å¥½çš„æ¨¡å‹æ¥è¿›è¡Œç ”ç©¶ã€‚æ•°æ®é›†å·²å…¬å¼€ï¼Œåœ°å€æ˜¯ï¼š<a href="http://datasets.maluuba.com/NewsQA" target="_blank" rel="external">http://datasets.maluuba.com/NewsQA</a></p>
<h1 id="ä¸€å‘¨èµ„æº"><a href="#ä¸€å‘¨èµ„æº" class="headerlink" title="ä¸€å‘¨èµ„æº"></a>ä¸€å‘¨èµ„æº</h1><h2 id="ä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šé€šè®¯"><a href="#ä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šé€šè®¯" class="headerlink" title="ä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šé€šè®¯"></a><a href="http://t.cn/Rf8Yvwn" target="_blank" rel="external">ä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šé€šè®¯</a></h2><p>ã€Šä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šé€šè®¯ã€‹ï¼Œæœ¬æœŸä¸ºå­¦ä¼šä¼˜ç§€åšå£«è®ºæ–‡ä¸“åˆŠ</p>
<h2 id="C-wrapper"><a href="#C-wrapper" class="headerlink" title="C++ wrapper"></a><a href="http://t.cn/RfEIAdZ" target="_blank" rel="external">C++ wrapper</a></h2><p>TensorFlowä½¿ç”¨swigä½œä¸ºC++ wrapperï¼Œæœ€è¿‘Googleåˆæ¨å‡ºäº†pyclifï¼Œå®£ç§°â€œitâ€™s much cleaner and easierâ€ </p>
<h2 id="æ™ºèƒ½æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†"><a href="#æ™ºèƒ½æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†" class="headerlink" title="æ™ºèƒ½æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†"></a><a href="http://t.cn/Rfm4hJb" target="_blank" rel="external">æ™ºèƒ½æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†</a></h2><p>ä»Šå¤©ADLå‰æ²¿è®²ä¹ ç­ã€Šæ™ºèƒ½æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‹Zhengdong Luçš„æŠ¥å‘Šï¼Œé¢˜ç›®Recent Progress on Deep Learning for NLPã€‚</p>
<h2 id="è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ·±åº¦å­¦ä¹ æ´»è·ƒé¢†åŸŸçš„è¯¾ç¨‹è®²ä¹‰"><a href="#è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ·±åº¦å­¦ä¹ æ´»è·ƒé¢†åŸŸçš„è¯¾ç¨‹è®²ä¹‰" class="headerlink" title="è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ·±åº¦å­¦ä¹ æ´»è·ƒé¢†åŸŸçš„è¯¾ç¨‹è®²ä¹‰"></a><a href="http://www.zishu010.com/z/newdetail/9404521.html" target="_blank" rel="external">è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ·±åº¦å­¦ä¹ æ´»è·ƒé¢†åŸŸçš„è¯¾ç¨‹è®²ä¹‰</a></h2><p>æœ¬æ–‡æ˜¯çº½çº¦å¤§å­¦åŠ©ç†æ•™æˆ Sam Bowman å…³äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ·±åº¦å­¦ä¹ æ´»è·ƒé¢†åŸŸçš„è¯¾ç¨‹è®²ä¹‰PPTã€‚å¯¹æ·±åº¦å­¦ä¹ NLPé¢†åŸŸæœ€è¿‘è¾ƒä¸ºæ´»è·ƒçš„ç ”ç©¶è¿›è¡Œäº†ç»¼è¿°ï¼Œå…¶ä¸­åŒ…æ‹¬Attention æ¨¡å‹ã€ç»“æ„åŒ–è®°å¿†ã€è¯æ°´å¹³ä»¥ä¸Šçš„æ— ç›‘ç£å­¦ä¹ ç­‰ç­‰ã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;a href=&quot;#ä¸€å‘¨å€¼å¾—è¯»&quot; class=&quot;headerlink&quot; title=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;/a&gt;ä¸€å‘¨å€¼å¾—è¯»&lt;/h1&gt;&lt;h2 id=&quot;A-Simple-Fast-Diverse-Decoding-Algorithm-for-Neural-G
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬åå…­æœŸ</title>
    <link href="http://rsarxiv.github.io/2016/12/03/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%85%AD%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/12/03/PaperWeekly-ç¬¬åå…­æœŸ/</id>
    <published>2016-12-03T18:08:45.000Z</published>
    <updated>2016-12-03T18:36:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h1><p>æœ¬æœŸPaperWeeklyå°†å¸¦ç€å¤§å®¶æ¥çœ‹ä¸€ä¸‹ICLR 2017çš„å…­ç¯‡paperï¼Œå…¶ä¸­åŒ…æ‹¬å½“ä¸‹éå¸¸ç«çƒ­çš„GANåœ¨NLPä¸­çš„åº”ç”¨ï¼Œå¼€æ”¾åŸŸèŠå¤©æœºå™¨äººå¦‚ä½•ç”Ÿæˆæ›´é•¿æ›´ä¸°å¯Œçš„å›ç­”ï¼Œå¦‚ä½•ç”¨å¼ºåŒ–å­¦ä¹ æ¥æ„å»ºæ ‘ç»“æ„çš„ç¥ç»ç½‘ç»œå’Œå±‚æ¬¡åŒ–çš„è®°å¿†ç½‘ç»œç­‰å†…å®¹ã€‚å…­ç¯‡paperåˆ†åˆ«æ˜¯ï¼š</p>
<p>1ã€A SELF-ATTENTIVE SENTENCE EMBEDDING<br>2ã€Adversarial Training Methods for Semi-Supervised Text Classification<br>3ã€GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS<br>4ã€Hierarchical Memory Networks<br>5ã€Mode Regularized Generative Adversarial Networks<br>6ã€Learning to compose words into sentences with reinforcement learning</p>
<h1 id="A-SELF-ATTENTIVE-SENTENCE-EMBEDDING"><a href="#A-SELF-ATTENTIVE-SENTENCE-EMBEDDING" class="headerlink" title="A SELF-ATTENTIVE SENTENCE EMBEDDING"></a><a href="http://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="external">A SELF-ATTENTIVE SENTENCE EMBEDDING</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou &amp; Yoshua Bengio</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>IBM Watson<br>UniversitÂ´e de MontrÂ´eal</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>self-attention, sentence embedding, author profiling, sentiment classification, textual entailment</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>æœ¬æ–‡æå‡ºä¸€ç§åœ¨æ²¡æœ‰é¢å¤–è¾“å…¥çš„æƒ…å†µä¸‹å¦‚ä½•åˆ©ç”¨attentionæ¥æé«˜æ¨¡å‹è¡¨ç°çš„å¥å­è¡¨ç¤ºæ–¹æ³•ã€‚</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æœ¬æ–‡æå‡ºçš„æ¨¡å‹ç»“æ„åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œ</p>
<ol>
<li>BLSTM<br>è¿™éƒ¨åˆ†é‡‡ç”¨åŒå‘LSTMå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œå¤„ç†ï¼Œæœ€åå¾—åˆ°BLSTMçš„æ‰€æœ‰éšå±‚çŠ¶æ€Hã€‚</li>
<li>Self-attention mechanism<br>åŒattentionæœºåˆ¶ç±»ä¼¼ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä¸€ä¸ªæƒé‡å‘é‡aï¼Œç„¶åé€šè¿‡å¯¹éšå±‚çŠ¶æ€HåŠ æƒæ±‚å’Œå¾—åˆ°å¥å­çš„è¡¨ç¤ºå‘é‡ã€‚è¿™ä¸ªè¿‡ç¨‹å¦‚ä¸‹å…¬å¼æ‰€ç¤ºï¼š<br><img src="media/equation1.png" alt="equation1"><br>ä½†æ˜¯å®é™…ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¯èƒ½ä¼šå¯¹ä¸€ä¸ªå¥å­è¯­ä¹‰çš„å¤šä¸ªæ–¹é¢æ„Ÿå…´è¶£ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„å…¬å¼ï¼Œè·å¾—å¤šä¸ªæƒé‡å‘é‡ç»„æˆçš„çŸ©é˜µAã€‚<br><img src="media/equation2.png" alt="equation2"><br>ç„¶åæ¯ä¸€ä¸ªæƒé‡å‘é‡aéƒ½å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¥å­è¡¨ç¤ºå‘é‡vï¼Œæ‰€æœ‰å¥å­è¡¨ç¤ºå‘é‡ç»„åˆåœ¨ä¸€èµ·å°±å¯ä»¥è·å¾—å¥å­è¡¨ç¤ºçŸ©é˜µMã€‚<br><img src="media/equation3.png" alt="equation3"><br>æœ¬æ–‡çš„æ¨¡å‹åœ¨author profiling, sentiment classificationå’Œtextual entailmentä¸‰ä¸ªä»»åŠ¡ä¸Šè¿›è¡ŒéªŒè¯ï¼Œéƒ½å–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚</li>
</ol>
<h2 id="èµ„æº"><a href="#èµ„æº" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€[Yelp]<br>(<a href="https://www.yelp.com/dataset" target="_blank" rel="external">https://www.yelp.com/dataset</a> challenge)<br>2ã€ [SNLI]<br>(<a href="http://nlp.stanford.edu/projects/snli/" target="_blank" rel="external">http://nlp.stanford.edu/projects/snli/</a>)</p>
<h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>A large annotated<br>corpus for learning natural language inference</p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡æå‡ºçš„self-attentionæ–¹æ³•ç”¨ä¸€ä¸ªmatrixè¡¨ç¤ºä¸€ä¸ªå¥å­ï¼Œå¹¶ä¸”matrixä¸­çš„æ¯ä¸€ä¸ªvectoréƒ½æ˜¯å¥å­è¯­ä¹‰æŸä¸€æ–¹é¢çš„è¡¨ç¤ºï¼Œå¢å¼ºäº†sentence embeddingçš„å¯è§£é‡Šæ€§ã€‚</p>
<h1 id="Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification"><a href="#Adversarial-Training-Methods-for-Semi-Supervised-Text-Classification" class="headerlink" title="Adversarial Training Methods for Semi-Supervised Text Classification"></a><a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="external">Adversarial Training Methods for Semi-Supervised Text Classification</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Takeru Miyato, Andrew M. Dai, Ian Goodfellow</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Google Brain, Kyoto Universityå’ŒOpenAI</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Adversarial training, text classification, semi-supervised learning</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>Adversarial trainingå’Œvirtual adversarial trainingéƒ½éœ€è¦å¯¹è¾“å…¥çš„æ•°å­—å½¢å¼åšå°çš„perturbationï¼Œä¸é€‚ç”¨äºé«˜ç»´ç¨€ç–è¾“å…¥ï¼Œæ¯”å¦‚one-hot word representationsã€‚æ–‡ç« æ‰©å±•å›¾åƒé¢†åŸŸæµè¡Œçš„è¿™ä¸¤ç§æ–¹æ³•åˆ°æ–‡æœ¬é¢†åŸŸï¼Œå¯¹word embeddingè¿›è¡Œperturbationæ¥ä½œä¸ºLSTMçš„è¾“å…¥ï¼Œå–ä»£åŸæœ¬çš„è¾“å…¥å‘é‡ã€‚å¯ä»¥æŠŠè¿™ä¸¤ç§æ–¹æ³•çœ‹åšæ˜¯æ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œä¸ºè¾“å…¥åŠ å…¥å™ªå£°ï¼Œå¯ä»¥ç”¨æ¥å®ç°semi-supervisedçš„ä»»åŠ¡ã€‚</p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>ä»¥adversarial trainingä¸ºä¾‹ï¼Œæ–‡ç« å¯¹word embeddingsè¿›è¡Œadversarial perturbationï¼Œè€Œä¸æ˜¯ç›´æ¥åº”ç”¨åœ¨è¾“å…¥ä¸Šã€‚å‡è®¾normalizedä¹‹åçš„è¾“å…¥åºåˆ—ä¸ºsï¼Œç»™å®šsï¼Œyçš„æ¡ä»¶æ¦‚ç‡ä¸ºp(y|s;theta)ï¼Œå…¶ä¸­thetaä¸ºæ¨¡å‹å‚æ•°ï¼Œåˆ™sä¸Šçš„adversarial perturbation r_advä¸ºï¼š<br><img src="media/16-1-1.png" alt="16-1"></p>
<p>åº”ç”¨åœ¨LSTMä¸Šï¼Œå¦‚ä¸‹å›¾(b)æ‰€ç¤ºã€‚å®šä¹‰å…¶adversarial losså¦‚ä¸‹ï¼š</p>
<p><img src="media/adversarial1.png" alt="adversaria"></p>
<p><img src="media/16-2.png" alt="16-2"></p>
<p>å…¶ä¸­Nä¸ºlabeledçš„ä¾‹å­çš„æ•°ç›®ã€‚é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™æ¥è¿›è¡Œtrainingã€‚</p>
<p>æ–‡ç« ä¹Ÿæä¾›äº†virtual adversarial trainingçš„æ–¹æ³•ã€‚</p>
<h2 id="èµ„æº-1"><a href="#èµ„æº-1" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="external">IMDB</a><br>2ã€<a href="http://riejohnson.com/cnn_data.html" target="_blank" rel="external">Elec</a><br>3ã€<a href="http://snap.stanford.edu/data/web-Amazon.html" target="_blank" rel="external">Rotten Tomatoes</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-1"><a href="#ç›¸å…³å·¥ä½œ-1" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ä¸»è¦åˆ—ä¸‰ç¯‡workï¼š<br>1ã€2015å¹´NIPS, SA-LSTMã€‚Semi-supervised sequence learning<br>2ã€2015å¹´NIPSï¼ŒOne-hot CNNã€‚Semi-supervised convolutional neural networks for text categorization via region<br>embedding<br>3ã€2016å¹´ICMLï¼ŒOne-hot bi-LSTMã€‚Supervised and semi-supervised text categorization using LSTM for region<br>embeddings</p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>ä½œè€…å°†å›¾åƒé¢†åŸŸçš„adversarial trainingåº”ç”¨åœ¨äº†æ–‡æœ¬é¢†åŸŸï¼Œæ”¹å–„äº†word embeddingã€‚ä¼ ç»Ÿçš„word embeddingè¢«è¯­æ³•ç»“æ„å½±å“ï¼Œå³ä½¿ä¸¤ä¸ªå®Œå…¨ç›¸åçš„è¯ï¼ˆæ¯”å¦‚â€goodâ€å’Œâ€badâ€ï¼‰åœ¨è¡¨ç¤ºå½¢å¼ä¸Šä¹Ÿæ˜¯ç›¸è¿‘çš„ï¼Œæ²¡æœ‰è¡¨ç¤ºå‡ºè¯æœ¬èº«çš„æ„æ€ã€‚Adversarial trainingä½¿å¾—æœ‰ç›¸è¿‘è¯­æ³•ç»“æ„ä½†æ˜¯ä¸åŒæ„ä¹‰çš„è¯èƒ½å¤Ÿè¢«åˆ†å¼€ï¼Œå¯ä»¥ç”¨æ¥åšæƒ…æ„Ÿåˆ†ç±»å’Œsequence modelç­‰ã€‚</p>
<h1 id="GENERATING-LONG-AND-DIVERSE-RESPONSES-WITH-NEURAL-CONVERSATION-MODELS"><a href="#GENERATING-LONG-AND-DIVERSE-RESPONSES-WITH-NEURAL-CONVERSATION-MODELS" class="headerlink" title="GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS"></a><a href="http://openreview.net/pdf?id=HJDdiT9gl" target="_blank" rel="external">GENERATING LONG AND DIVERSE RESPONSES WITH NEURAL CONVERSATION MODELS</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray Kurzweil1</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Google Research, Google Brain</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Long and Diverse Responses</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>å¼€æ”¾åŸŸèŠå¤©æœºå™¨äººå¦‚ä½•ç”Ÿæˆæ›´é•¿ä¸”è¾ƒä¸ºä¸°å¯Œçš„å›ç­”ï¼Ÿ</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æœ¬æ–‡æ¨¡å‹æ˜¯åŸºäºç»å…¸çš„seq2seq+attentionæ¡†æ¶ï¼Œåœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œäº†è‹¥å¹²ä¿®æ”¹ï¼Œå¾—åˆ°äº†æ»¡æ„çš„æ•ˆæœã€‚ä¸åŒäºä¹‹å‰æ¨¡å‹çš„åœ°æ–¹æœ‰ä¸¤ç‚¹ï¼š</p>
<p>1ã€encoderä¸ä»…ä»…åŒ…æ‹¬æ•´ä¸ªsourceï¼Œè¿˜åŒ…æ‹¬ä¸€éƒ¨åˆ†targetï¼Œè¿™æ ·attentionä¸ä»…ä»…è€ƒè™‘äº†sourceï¼Œè€Œä¸”è€ƒè™‘äº†éƒ¨åˆ†targetã€‚</p>
<p><img src="media/16-3.png" alt="16-3"></p>
<p>ç»å…¸çš„seq2seq+attentionåœ¨decodingéƒ¨åˆ†ä¼šå°†sourceä¸­çš„æ¯ä¸ªtokenéƒ½è€ƒè™‘åˆ°attentionä¸­æ¥ï¼Œä¹‹å‰æœ‰ä¸€ç§åšæ³•æ˜¯å°†æ•´ä¸ªtargetéƒ¨åˆ†ä¹ŸåŠ å…¥åˆ°attentionä¸­ï¼Œæ•ˆæœä¸Šè™½ç„¶æœ‰ä¸€å®šçš„æå‡ï¼Œä½†éšç€æ•°æ®è§„æ¨¡åœ°å¢åŠ ï¼Œå†…å­˜ä»£ä»·å¤ªå¤§ã€‚æœ¬æ–‡æ­£æ˜¯é’ˆå¯¹è¿™ä¸€ä¸ªé—®é¢˜ï¼Œæå‡ºäº†æ‰€è°“çš„â€œglimpseâ€æ¨¡å‹ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåœ¨encoderéƒ¨åˆ†åŠ å…¥äº†targetçš„å‰å‡ ä¸ªtokenï¼Œç›¸å½“äºæ˜¯ä¸Šé¢ä¸¤ç§æ–¹æ¡ˆçš„ä¸€ç§æŠ˜ä¸­ã€‚</p>
<p>2ã€æå‡ºäº†ä¸€ç§åŸºäºsamplingçš„beam search decodingæ–¹æ¡ˆã€‚</p>
<p>ç»å…¸çš„beam searchåœ¨decodingéƒ¨åˆ†ï¼Œæ˜¯åŸºäºMAPï¼ˆæœ€å¤§åéªŒæ¦‚ç‡ï¼‰è¿›è¡Œè´ªå©ªè§£ç çš„ï¼Œè¿™ç§æ–¹æ¡ˆç”Ÿæˆçš„responseså…·æœ‰ç®€çŸ­ã€æ— ä¿¡æ¯é‡ä»¥åŠé«˜é¢‘çš„ç‰¹ç‚¹ï¼Œé€šä¿—åœ°è®²ä¼šç”Ÿæˆå¾ˆå¤šçš„ç±»ä¼¼â€œå‘µå‘µâ€çš„è¯ï¼Œæ²¡æœ‰å¤ªå¤šè¥å…»å’Œä»·å€¼ã€‚(Jiwei Li,2015)åœ¨è§£å†³è¿™ä¸ªé—®é¢˜æ—¶ï¼Œåœ¨decodingéƒ¨åˆ†é€šè¿‡MMIï¼ˆäº’ä¿¡æ¯ï¼‰å¯¹N-bestç»“æœè¿›è¡Œé‡æ’åºï¼Œè¿™ç§æ–¹æ³•å¯¹äºç”ŸæˆçŸ­æ–‡æœ¬æ•ˆæœæ˜¾è‘—ï¼Œä½†å¯¹äºç”Ÿæˆé•¿æ–‡æœ¬æ•ˆæœä¸ä½³ã€‚å› ä¸ºï¼ŒåŸºäºMAPçš„beam searchå¤©ç„¶å­˜åœ¨è¿™æ ·çš„é—®é¢˜ï¼ŒN-bestå’Œé‡æ’åºéƒ½è§£å†³ä¸äº†æ ¹æœ¬æ€§çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºsamplingçš„beam searchè§£ç æ–¹æ¡ˆï¼Œsamplingå³åœ¨æ¯ä¸€æ­¥è§£ç æ—¶éƒ½sampleå‡ºDä¸ªtokenä½œä¸ºå€™é€‰ï¼Œæœç´¢å®Œæ¯•æˆ–è¾¾åˆ°é¢„è®¾çš„é•¿åº¦ä¹‹åï¼Œç”ŸæˆBä¸ªå€™é€‰responsesï¼Œç„¶åè¿›è¡Œé‡æ’åºã€‚</p>
<p>æœ¬æ–‡çš„å¦å¤–ä¸€å¤§äº®ç‚¹æ˜¯ç”¨äº†å¤§é‡çš„å¯¹è¯æ•°æ®ï¼Œç”¨äº†å¾ˆå¤§è§„æ¨¡å‚æ•°çš„æ¨¡å‹è¿›è¡Œäº†å®éªŒã€‚å®éªŒè¯„ä»·æ ‡å‡†ï¼Œåœ¨è‡ªåŠ¨è¯„ä»·è¿™éƒ¨åˆ†ï¼Œè®¾è®¡äº†ä¸€ä¸ªNé€‰1çš„å®éªŒï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ï¼Œå°†æ­£ç¡®è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºæ··åœ¨ä¸€èµ·ï¼Œæ¨¡å‹éœ€è¦ä»ä¸­é€‰æ‹©æ­£ç¡®çš„è¾“å‡ºï¼Œç”¨é€‰æ‹©å‡†ç¡®ç‡æ¥ä½œä¸ºè‡ªåŠ¨è¯„ä»·æŒ‡æ ‡ã€‚æœ¬æ–‡æ²¡æœ‰ç”¨åˆ°ç»å…¸çš„BLEUæŒ‡æ ‡ï¼Œå› ä¸ºè¿™ä¸ªæŒ‡æ ‡ç¡®å®ä¸é€‚åˆè¯„ä»·å¯¹è¯çš„ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†æ›´æœ‰è¯´æœåŠ›ï¼Œæœ¬æ–‡ç”¨äººå·¥å¯¹ç»“æœè¿›è¡Œè¯„ä»·ã€‚</p>
<h2 id="èµ„æº-2"><a href="#èµ„æº-2" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>æœ¬æ–‡ç”¨åˆ°çš„å¯¹è¯æ•°æ®ï¼š<br>1ã€<a href="https://redd.it/3bxlg7" target="_blank" rel="external">Reddit Data</a><br>2ã€<a href="http://opus.lingfil.uu.se/OpenSubtitles.php" target="_blank" rel="external">2009 Open Subtitles data</a><br>3ã€<a href="https://data.stackexchange.com/" target="_blank" rel="external">Stack Exchange data</a><br>4ã€æœ¬æ–‡ä½œè€…ä»WebæŠ½å–çš„å¯¹è¯æ•°æ®ï¼ˆå¾…å…¬å¼€ï¼‰</p>
<h2 id="ç›¸å…³å·¥ä½œ-2"><a href="#ç›¸å…³å·¥ä½œ-2" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ç”¨seq2seqæ–¹æ³•ç ”ç©¶ç”Ÿæˆå¯¹è¯çš„è´¨é‡ï¼ˆåŒ…æ‹¬é•¿åº¦ã€å¤šæ ·æ€§ï¼‰çš„å·¥ä½œå¹¶ä¸å¤šï¼Œå…·æœ‰ä»£è¡¨æ€§çš„æœ‰ä¸‹é¢ä¸¤ä¸ªå·¥ä½œï¼š<br>1ã€Wu,2016 æå‡ºäº†ç”¨length-normalizationçš„æ–¹æ¡ˆæ¥ç”Ÿæˆæ›´é•¿çš„å¯¹è¯<br>2ã€Jiwei Li,2015 æå‡ºäº†åœ¨è§£ç é˜¶æ®µç”¨MMIï¼ˆäº’ä¿¡æ¯ï¼‰å¯¹N-bestç»“æœè¿›è¡Œé‡æ’åºï¼Œæ—¨åœ¨è·å¾—ä¿¡æ¯é‡æ›´å¤§çš„å¯¹è¯ã€‚</p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡æ¨¡å‹éƒ¨åˆ†å¹¶æ²¡æœ‰å¤ªå¤šçš„åˆ›æ–°ï¼Œå› ä¸ºæ˜¯å·¥ä¸šéƒ¨é—¨çš„paperï¼Œæ‰€ä»¥æ›´å¤šçš„æ˜¯è€ƒè™‘å®ç”¨æ€§ï¼Œå³èƒ½å¦åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šåº”ç”¨è¯¥æ¨¡å‹ï¼Œé›†ä¸­ä½“ç°åœ¨glimpseæ¨¡å‹ä¸Šã€‚ä¸ºäº†ç”Ÿæˆæ›´åŠ é•¿ã€æ›´åŠ å¤šæ ·æ€§çš„å¯¹è¯ï¼Œåœ¨åŸæœ‰beam search + é‡æ’åºçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†samplingæœºåˆ¶ï¼Œç»™ç”Ÿæˆè¿‡ç¨‹å¢åŠ äº†æ›´å¤šçš„å¯èƒ½æ€§ï¼Œä¹Ÿæ˜¯å·¥ç¨‹ä¸Šçš„trickã€‚å¯¹è¯æ•ˆæœçš„è¯„ä»·æ˜¯ä¸€ä»¶å¾ˆéš¾çš„äº‹æƒ…ï¼Œäººç±»å¸Œæœ›botå¯ä»¥ç”Ÿæˆç±»äººçš„å¯¹è¯ï¼Œå›å¤çš„é•¿åº¦å¯ä»¥å®šé‡æè¿°ï¼Œä½†å¤šæ ·æ€§ã€ç”ŸåŠ¨æ€§ã€æ‹ŸäººåŒ–ç­‰ç­‰éƒ½éš¾ä»¥å®šé‡æè¿°ï¼Œæ‰€ä»¥åœ¨æ¢ç´¢ç”Ÿæˆå¯¹è¯çš„è¿™ä¸ªæ–¹å‘ä¸Šè¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚</p>
<h1 id="Hierarchical-Memory-Networks"><a href="#Hierarchical-Memory-Networks" class="headerlink" title="Hierarchical Memory Networks"></a><a href="https://arxiv.org/pdf/1605.07427v1.pdf" target="_blank" rel="external">Hierarchical Memory Networks</a></h1><h2 id="ä½œè€…-3"><a href="#ä½œè€…-3" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio</p>
<h2 id="å•ä½-3"><a href="#å•ä½-3" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>1ã€UniversitÃ© de MontrÃ©al, Canada.<br>2ã€Twitter Cortex, USA.<br>3ã€IBM Watson Research Center, USA.<br>4ã€CIFAR, Canada.  </p>
<h2 id="å…³é”®è¯-3"><a href="#å…³é”®è¯-3" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Hierarchical Memory Networksï¼ŒMaximum Inner Product Search (MIPS)</p>
<h2 id="æ–‡ç« æ¥æº-3"><a href="#æ–‡ç« æ¥æº-3" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017</p>
<h2 id="é—®é¢˜-3"><a href="#é—®é¢˜-3" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>è®°å¿†ç½‘ç»œä¸»è¦åŒ…æ‹¬hard attentionå’Œsoft attenionä¸¤ç§ï¼Œç„¶è€Œhardä¸èƒ½ç”¨äºåå‘ä¼ æ’­ç®—æ³•è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ‰€ä»¥åªèƒ½ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼›softæ‰€æ¶‰åŠçš„è®¡ç®—å‚æ•°åˆå¾ˆå¤§ï¼Œåªé€‚åˆäºå°‘é‡Memoryã€‚æœ¬æ–‡æå‡ºHierarchical Memory Networks(HMN)æ¨¡å‹ï¼Œç®—æ˜¯softå’Œhardçš„ä¸€ä¸ªæ··åˆæ¨¡å‹ï¼Œè®¡ç®—é‡å‡å°‘ä¸”è®­ç»ƒæ›´åŠ å®¹æ˜“ï¼Œ<br>å®éªŒç»“æœä¹Ÿå¾ˆå¥½ã€‚</p>
<h2 id="æ¨¡å‹-3"><a href="#æ¨¡å‹-3" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>soft attentionæ˜¯å¯¹æ‰€æœ‰çš„memoryéƒ½è¦è¿›è¡Œattentionçš„è®¡ç®—ï¼Œå¯¹å…¨é›†è®¡ç®—ä½¿è®¡ç®—é‡å¾ˆå¤§ã€‚HMNåˆ©ç”¨å±‚æ¬¡åŒ–ç»“æ„ä½¿å¾—attentionçš„é›†åˆç¼©å°ï¼Œåˆ©ç”¨MaximumInner Product Search(MIPS)çš„æ–¹æ³•ä»å…¨é›†ä¸­è·å¾—ä¸€ä¸ªæœ€ä¼˜å­é›†ï¼Œåœ¨å­é›†ä¸Šé¢å»åšattentionå°±å¤§å¤§é™ä½è®¡ç®—é‡ã€‚è¿™æ ·çš„æ–¹å¼åˆå’Œhard attentioné¢„æµ‹å…³æ³¨ç‚¹çš„æ–¹æ³•æœ‰äº›ç±»ä¼¼ï¼Œå°†æ³¨æ„åŠ›æ”¾åœ¨æœ€ç›¸å…³çš„é‚£éƒ¨åˆ†ï¼Œè¿™ä¸ªçš„åšæ³•ä¹Ÿæ›´æ¥è¿‘äºäººçš„æ³¨æ„åŠ›æ€ç»´ã€‚  æ–‡ç« çš„æ ¸å¿ƒéƒ¨åˆ†åœ¨äºå¦‚ä½•è·å–ä¸queryæœ€ç›¸è¿‘çš„å­é›†ã€‚</p>
<p>ä¸»å®éªŒä¸»è¦åŒ…æ‹¬ä¸¤ä¸ª:<br>1ã€Exact K-MIPSï¼šè®¡ç®—å¤æ‚åº¦ä¾ç„¶å’Œsoft attentionå·®ä¸å¤šã€‚<br>2ã€Approximate K-MIPSï¼šåˆ©ç”¨Maximum Cosine Similarity Search(MCSS)çš„æ–¹æ³•ä»£æ›¿MIPSçš„æ–¹æ³•ï¼Œç‰ºç‰²ä¸€äº›ç²¾ç¡®åº¦ï¼Œé™ä½å¤æ‚åº¦å’ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚  </p>
<p>MIPSæœ‰ä¸‰ç§æ–¹æ³•ï¼Œåˆ†åˆ«æ˜¯åŸºäºhash,åŸºäºtree,åŸºäºclusteringï¼ŒåŸºäºä¸Šè¿°ä¸‰ç§æ–¹æ³•æ–‡ä¸­åˆåšäº†å‡ ç»„ç»„å¯¹æ¯”å®éªŒï¼Œæœ€åå®éªŒç»“æœæ˜¾ç¤ºåŸºäºclusteringçš„æ•ˆæœæ˜¯æœ€å¥½çš„ã€‚</p>
<p>æ–‡ç« å¾—åˆ°çš„å®éªŒç»“æœå¦‚ä¸‹ï¼š<br><img src="media/HMN_Result.png" alt="HMN_Result"></p>
<h2 id="èµ„æº-ï¼ˆå¯é€‰ï¼‰"><a href="#èµ„æº-ï¼ˆå¯é€‰ï¼‰" class="headerlink" title="èµ„æº ï¼ˆå¯é€‰ï¼‰"></a>èµ„æº ï¼ˆå¯é€‰ï¼‰</h2><p>1ã€<a href="https://www.dropbox.com/s/tohrsllcfy7rch4/SimpleQuestions_v2.tgz" target="_blank" rel="external">The SimpleQuestions dataset</a>(ä½¿ç”¨çš„æ˜¯Large-scale simple question answering with memory networksæ–‡ç« ä¸­çš„æ•°æ®é›†)<br>2ã€<a href="https://research.facebook.com/research/babi/" target="_blank" rel="external">babi</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-3"><a href="#ç›¸å…³å·¥ä½œ-3" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>1ã€arXiv 2014, soft attention,ã€ŠNeural turing machinesã€‹<br>2ã€CoRR 2015, hard attention,ã€ŠReinforcement learning neural turing machineã€‹<br>3ã€ICLR 2015, memory network,ã€ŠMemory networksã€‹<br>4ã€arXiv 2015,ã€ŠEnd-to-end memory networksã€‹,å¼•å…¥åŠç›‘ç£è®°å¿†ç½‘ç»œå¯ä»¥è‡ªå­¦æ‰€éœ€è¦çš„factsã€‚<br>5ã€CoRR 2016, DMN, ã€ŠDynamic memory networks for visual and textual question<br>answeringã€‹,å¢åŠ äº†ä¸€ä¸ªepisodic memory ä½¿å¾—å¯ä»¥åŠ¨æ€æ›´æ–°memoryé‡Œé¢çš„å†…å®¹ã€‚</p>
<h2 id="ç®€è¯„-3"><a href="#ç®€è¯„-3" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æ–‡ç« çš„åˆ›æ–°ä¸»è¦åœ¨äºä¿®æ”¹äº†ä¸¤ä¸ªæ¨¡å—ï¼šMemoryå’ŒReaderã€‚<br>1ã€å°†memoryçš„ç»“æ„ä»a flat of arrayå˜æˆäº†hierarchical memory structureã€‚å°†memoryåˆ†æˆè‹¥å¹²groups,è¿™äº›groupsåˆå¯ä»¥åœ¨è¿›è¡Œæ›´é«˜çº§åˆ«çš„ç»„åˆã€‚<br>2ã€readeræ˜¯ä»MIPSé€‰å‡ºçš„å­é›†ä¸­ä½¿ç”¨soft attentionã€‚MIPSä»memoryä¸­é€‰å‡ºä¸€<br>ä¸ªgroupå­é›†ä½œä¸ºæœ€ç›¸å…³çš„å­é›†ã€‚</p>
<h1 id="Mode-Regularized-Generative-Adversarial-Networks"><a href="#Mode-Regularized-Generative-Adversarial-Networks" class="headerlink" title="Mode Regularized Generative Adversarial Networks "></a><a href="http://openreview.net/pdf?id=HJKkY35le" target="_blank" rel="external">Mode Regularized Generative Adversarial Networks </a></h1><h2 id="ä½œè€…-4"><a href="#ä½œè€…-4" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Tong Che; Yanran Li</p>
<h2 id="å•ä½-4"><a href="#å•ä½-4" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Montreal Institute for Learning Algorithms;<br>Department of Computing, The Hong Kong Polytechnic University</p>
<h2 id="å…³é”®è¯-4"><a href="#å…³é”®è¯-4" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>GAN, Regularizers</p>
<h2 id="æ–‡ç« æ¥æº-4"><a href="#æ–‡ç« æ¥æº-4" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017</p>
<h2 id="é—®é¢˜-4"><a href="#é—®é¢˜-4" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>æœ¬æ–‡é’ˆå¯¹çš„é—®é¢˜æ˜¯ï¼š1ã€GAN çš„è®­ç»ƒè¿‡ç¨‹å¾ˆä¸ç¨³å®š 2ã€GAN ç”Ÿæˆçš„æ ·æœ¬å±€é™äºè®­ç»ƒæ ·æœ¬ä¸­çš„å¤§ model ä¸Šï¼Œä¸èƒ½å¹³è¡¡æ•°æ®çš„åˆ†å¸ƒï¼ˆmissing model problemï¼‰ã€‚<br>ä¸¤ä¸ªé—®é¢˜äº’ç›¸å½±å“ï¼Œå¯¼è‡´è®­ç»ƒç»“æœä¸å¥½ã€‚</p>
<h2 id="æ¨¡å‹-4"><a href="#æ¨¡å‹-4" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>é’ˆå¯¹ä¸Šé¢çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§ regularizers å»æ§åˆ¶ GAN çš„è®­ç»ƒè¿‡ç¨‹ã€‚<br>ç¬¬ä¸€ä¸ª regularizer ä¹Ÿè¢«ä½œè€…ç§°ä¸º Regularized-GANã€‚ä½œè€…è®¤ä¸ºå¯ä»¥ä» generator å…¥æ‰‹ï¼Œç»™ generator å¢åŠ  regularizerï¼Œä½¿å¾—å…¶å…·æœ‰æ›´å¥½çš„ gradient ï¼Œè¿™æ · G å’Œ D éƒ½èƒ½ç¨³å®šè®­ç»ƒã€‚<br>å…·ä½“çš„æ–¹æ³•æ˜¯å¢åŠ ä¸€ä¸ª encoder E(x) : X â†’ Z.å³æŠŠåŸå…ˆçš„ noise vector z æ”¹ä¸º z = encoder(X) ï¼Œå³ç„¶åå† G(encoder(X))ã€‚å¦‚ä¸‹å›¾ï¼š<br><img src="media/16-5.png" alt="16-5"></p>
<p>è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ã€‚ç¬¬ä¸€ï¼ŒåŸå§‹çš„æ¨¡å‹å¾ˆå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„æƒ…å†µï¼Œå› ä¸º discriminator D ç‰¹åˆ«å®¹æ˜“åŒºåˆ†çœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®å¯¼è‡´ generator å°±å¾—ä¸åˆ° D çš„æ¢¯åº¦ã€‚ä½œè€…çš„æ¨¡å‹å¤šäº†ä¸€ä¸ª reconstruction çš„éƒ¨åˆ†ï¼Œè¿™æ ·ç”Ÿæˆå‡ºæ¥æ•°æ®ä¸å†é‚£æ ·å®¹æ˜“è¢« D è¯†åˆ«å‡ºæ¥ã€‚æ‰€ä»¥ D å’Œ G å°±éƒ½èƒ½ä¸€ç›´æœ‰ gradient å»è®­ç»ƒï¼Œä»è€Œæé«˜ç¨³å®šæ€§ã€‚ç¬¬äºŒï¼Œå¯¹äº x ï¼ŒG(E(x)) ä¼šå°½é‡å»ç”Ÿæˆ x åŸæœ¬æ‰€å±çš„ç±»ï¼Œä»è€Œä¸€å®šç¨‹åº¦è§£å†³äº† missing model problemã€‚<br>ç¬¬äºŒä¸ª regularizer åŸºäºç¬¬ä¸€ä¸ª regularizer æ—¨åœ¨æ”¹è¿›è®­ç»ƒçš„æ–¹æ³•ï¼Œä¹Ÿè¢«ä½œè€…ç§°ä¸º manifold-diffusion GANã€‚åˆ†ä¸ºä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥ manifold step è®­ç»ƒ discriminator D1 ï¼Œç›®çš„æ˜¯å‡å°‘ G(Enc(X)) å’Œ X çš„çš„å·®åˆ«ï¼›ç¬¬äºŒæ­¥ diffusion å°±æ˜¯è®­ç»ƒ D2 è®© G(Enc(X)) å’Œ G(z) åˆ†å¸ƒçš„è·ç¦»æ¥è¿‘ã€‚å¦‚ä¸‹å›¾ï¼š</p>
<p><img src="media/16-6.png" alt="16-6"></p>
<p>æœ€åï¼Œä½œè€…æŠŠ GAN çš„ç½‘ç»œè®­ç»ƒåå¡Œçš„æƒ…å†µè€ƒè™‘è¿›å»ï¼Œæå‡ºäº†æ–°çš„ evaluation metricã€‚</p>
<h2 id="ç›¸å…³å·¥ä½œ-4"><a href="#ç›¸å…³å·¥ä½œ-4" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>æœ¬ç¯‡æ–‡ç« çš„ä½œè€…æå«£ç„¶å†™è¿‡ä¸€ç¯‡éå¸¸æ£’çš„<a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;mid=2650325352&amp;idx=1&amp;sn=90fb15cee44fa7175a804418259d352e&amp;mpshare=1&amp;scene=1&amp;srcid=0829ixEhnGChNKAl5kgz6b9V#rd" target="_blank" rel="external">ç»¼è¿°</a> ,åœ¨è¿™é‡Œå°±ä¸ç´¯èµ˜é˜è¿°äº†ã€‚</p>
<h2 id="ç®€è¯„-4"><a href="#ç®€è¯„-4" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>å½“ä¸‹ GAN çš„ç ”ç©¶éå¸¸ç«çˆ†ï¼Œå‡ºç°äº†è®¸è®¸å¤šå¤šå¯¹ GAN çš„æ”¹è¿›ï¼Œæœ¬ç¯‡æ–‡ç« çš„æå‡ºçš„ä¸¤ç§ regularizers éå¸¸æœ‰æ•ˆçš„æé«˜äº† GAN çš„ç¨³å®šæ€§ï¼ˆå…¶ä¸­ regularizer çš„æ€æƒ³ä¹Ÿå—åˆ°äº†ç›‘ç£å­¦ä¹ çš„å¯å‘ï¼‰ï¼Œå€¼å¾—å¯¹ GAN æ„Ÿå…´è¶£çš„åŒå­¦ç ”è¯»ã€‚</p>
<h2 id="å®Œæˆäººä¿¡æ¯"><a href="#å®Œæˆäººä¿¡æ¯" class="headerlink" title="å®Œæˆäººä¿¡æ¯"></a>å®Œæˆäººä¿¡æ¯</h2><p>professorshui@gmail.com</p>
<h1 id="Learning-to-compose-words-into-sentences-with-reinforcement-learning"><a href="#Learning-to-compose-words-into-sentences-with-reinforcement-learning" class="headerlink" title="Learning to compose words into sentences with reinforcement learning"></a><a href="https://openreview.net/forum?id=Skvgqgqxe" target="_blank" rel="external">Learning to compose words into sentences with reinforcement learning</a></h1><h2 id="ä½œè€…-5"><a href="#ä½œè€…-5" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling</p>
<h2 id="å•ä½-5"><a href="#å•ä½-5" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Google</p>
<h2 id="å…³é”®è¯-5"><a href="#å…³é”®è¯-5" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Tree-LSTM, Reinforcement Learning</p>
<h2 id="æ–‡ç« æ¥æº-5"><a href="#æ–‡ç« æ¥æº-5" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017</p>
<h2 id="é—®é¢˜-5"><a href="#é—®é¢˜-5" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥æ„å»ºæ ‘ç»“æ„çš„ç¥ç»ç½‘ç»œTree-LSTMï¼Œå­¦ä¹ è‡ªç„¶è¯­è¨€çš„å¥å­è¡¨ç¤º</p>
<h2 id="æ¨¡å‹-5"><a href="#æ¨¡å‹-5" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ¨¡å‹åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šTree-LSTMå’Œå¼ºåŒ–å­¦ä¹ æ¨¡å‹<br>åº”ç”¨Tree-LSTM(å¯ä»¥é€šè¿‡LSTMçš„å¿˜è®°é—¨æœºåˆ¶ï¼Œè·³è¿‡æ•´æ£µå¯¹ç»“æœå½±å“ä¸å¤§çš„å­æ ‘)ï¼Œå¹¶ç»“åˆ{SHIFTï¼ŒREDUCE}æ“ä½œï¼ŒSHIFTæ“ä½œå¯¹åº”å°†ä¸€ä¸ªèŠ‚ç‚¹å‹å…¥æ ˆï¼ŒREDUCEå¯¹åº”å°†ä¸¤ä¸ªå…ƒç´ ç»„åˆï¼Œä»è€Œå»ºç«‹æ ‘ç»“æ„</p>
<p>å¼ºåŒ–å­¦ä¹ ç”¨æ¥å¯»æ‰¾æœ€ä½³çš„èŠ‚ç‚¹ç»„åˆæƒ…å†µï¼ŒRLæ¨¡å‹ä¸­çš„çŠ¶æ€så³å½“å‰æ„å»ºçš„æ ‘ç»“æ„ï¼Œaä¸º{SHIFTï¼ŒREDUCE}æ“ä½œï¼Œrewardå¯¹åº”ä¸åŒdownstream<br> task(ä¾‹ï¼šè‹¥æ˜¯ç”¨è¯¥å¥å­è¡¨ç¤ºè¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œåˆ™rå¯¹åº”ä»ç­–ç•¥ç½‘ç»œä¸­é‡‡æ ·å¾—åˆ°å¥å­è¡¨ç¤ºçš„åˆ†ç±»å‡†ç¡®æ€§çš„æ¦‚ç‡)</p>
<h2 id="èµ„æº-3"><a href="#èµ„æº-3" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>ä½œè€…å°†è¯¥å·¥ä½œè¿›è¡Œäº†å››ç»„å®éªŒï¼Œæƒ…æ„Ÿåˆ†ç±»ï¼Œè¯­ä¹‰ç›¸å…³æ€§åˆ¤æ–­ï¼Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œå¥å­ç”Ÿæˆ<br>åˆ†åˆ«åº”ç”¨Stanford Sentiment Treebankï¼ŒSentences Involving Compositional Knowledge corpusï¼ŒStanford Natural Language Inference corpusï¼ŒIMDB movie review corpus</p>
<h2 id="ç›¸å…³å·¥ä½œ-5"><a href="#ç›¸å…³å·¥ä½œ-5" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ä¸Socherç­‰äººä¹‹å‰æå‡ºçš„Recursive NN,MV-RNN,RNTNï¼ŒTree-LSTMç­‰å·¥ä½œä¸€è„‰ç›¸æ‰¿ï¼Œæœ¬æ–‡åˆåŠ å…¥äº†RLæ–¹å¼æ„å»ºæ ‘å½¢ç»“æ„</p>
<h2 id="ç®€è¯„-5"><a href="#ç®€è¯„-5" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>å°†å¼ºåŒ–å­¦ä¹ å¼•å…¥å¥å­è¡¨ç¤ºå­¦ä¹ ä¹‹ä¸­ï¼Œå­¦ä¹ æ„å»ºæ ‘çš„ä¸åŒæ–¹å¼ï¼Œä»å·¦å‘å³ï¼Œä»å³å‘å·¦ï¼ŒåŒå‘ï¼Œæœ‰ç›‘ç£ã€åŠç›‘ç£ã€é¢„å…ˆæ— ç»“æ„ç­‰æ–¹å¼å»æ„å»ºæ ‘ç»“æ„ï¼Œä½†æ˜¯è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œåœ¨å‡ ä¸ªä»»åŠ¡ä¸Šæ•ˆæœæå‡ä¸æ˜¯ç‰¹åˆ«æ˜æ˜¾ã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>GANæ˜¯å½“ä¸‹çš„ç ”ç©¶çƒ­ç‚¹ä¹‹ä¸€ï¼Œåœ¨å›¾åƒé¢†åŸŸä¸­ç ”ç©¶è¾ƒå¤šï¼Œæœ¬æœŸçš„ä¸¤ç¯‡paperæ¢è®¨äº†GANåœ¨NLPä¸­çš„åº”ç”¨ï¼Œå€¼å¾—å…³æ³¨å’ŒæœŸå¾…ã€‚æœ€åæ„Ÿè°¢@destinwangã€@gcyydxfã€@chunhualiuã€@tonyaã€@suhuiå’Œ@zhangjunå…­ä½ç«¥é‹çš„è¾›å‹¤å·¥ä½œã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•è¨€&quot;&gt;&lt;a href=&quot;#å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;å¼•è¨€&quot;&gt;&lt;/a&gt;å¼•è¨€&lt;/h1&gt;&lt;p&gt;æœ¬æœŸPaperWeeklyå°†å¸¦ç€å¤§å®¶æ¥çœ‹ä¸€ä¸‹ICLR 2017çš„å…­ç¯‡paperï¼Œå…¶ä¸­åŒ…æ‹¬å½“ä¸‹éå¸¸ç«çƒ­çš„GANåœ¨NLPä¸­çš„åº”ç”¨ï¼Œå¼€æ”¾åŸŸèŠå¤©æœºå™¨
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>ä¸­æ–‡åˆ†è¯å·¥å…·æµ‹è¯„</title>
    <link href="http://rsarxiv.github.io/2016/11/29/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E6%B5%8B%E8%AF%84/"/>
    <id>http://rsarxiv.github.io/2016/11/29/ä¸­æ–‡åˆ†è¯å·¥å…·æµ‹è¯„/</id>
    <published>2016-11-29T17:49:50.000Z</published>
    <updated>2016-11-29T19:07:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h1><p>åˆ†è¯å¯¹äºç ”ç©¶å’Œåº”ç”¨ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„ç«¥é‹æ¥è¯´ï¼Œéƒ½æ˜¯ä¸€ä¸ªéå¸¸éå¸¸åŸºç¡€çš„éƒ¨ä»¶ï¼Œåˆ†è¯çš„è´¨é‡ç›´æ¥å½±å“åˆ°åç»­è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ã€å¥æ³•åˆ†æç­‰éƒ¨ä»¶çš„å‡†ç¡®æ€§ã€‚ä½œä¸ºä¸€ä¸ªåŸºç¡€éƒ¨ä»¶ï¼Œå­¦æœ¯ç•Œå¯¹åˆ†è¯çš„ç ”ç©¶å·²ç»éå¸¸ä¹…äº†ï¼Œå¸‚é¢ä¸Šæµè¡Œçš„å‡ å¤§å¼€æºåˆ†è¯å·¥å…·ä¹Ÿè¢«å·¥ä¸šç•Œçš„å„å¤§å…¬å¸åº”ç”¨å¾ˆå¤šå¹´äº†ã€‚æœ€è¿‘ï¼Œä¸­æ–‡åˆ†è¯éšç€ä¸€ç¯‡åšæ–‡çš„å‘è¡¨è¢«æ¨åˆ°äº†é£å£æµªå°–ï¼Œå¼•å‘ä¼—å¤šå¤§ç‰›åœ¨å¾®åšã€å¾®ä¿¡ç¾¤é‡Œçš„æ¿€çƒˆè®¨è®ºã€‚æœ¬æ–‡å¹¶ä¸æƒ³å¯¹è¿™ç¯‡åšæ–‡è¿›è¡Œè¿‡å¤šè¯„è®ºï¼Œåªæ˜¯æƒ³ç”¨å…¬å¼€çš„æ•°æ®é›†å¯¹å„å¤§åˆ†è¯å·¥å…·è¿›è¡Œä¸€ä¸ªå®¢è§‚åœ°æµ‹è¯„ï¼Œä»¥ä¾›å¤§å®¶åœ¨é€‰æ‹©å·¥å…·æ—¶æœ‰æ‰€ä¾æ®ã€‚</p>
<h1 id="ä¸­æ–‡åˆ†è¯å·¥å…·"><a href="#ä¸­æ–‡åˆ†è¯å·¥å…·" class="headerlink" title="ä¸­æ–‡åˆ†è¯å·¥å…·"></a>ä¸­æ–‡åˆ†è¯å·¥å…·</h1><p>æœ¬æ–‡é€‰æ‹©äº†4ä¸ªå¸¸è§çš„åˆ†è¯å·¥å…·ï¼Œåˆ†åˆ«æ˜¯ï¼šå“ˆå·¥å¤§LTPã€ä¸­ç§‘é™¢è®¡ç®—æ‰€NLPIRã€æ¸…åå¤§å­¦THULACå’Œjiebaï¼Œä¸ºäº†å¯¹æ¯”åˆ†è¯é€Ÿåº¦ï¼Œé€‰æ‹©äº†è¿™å››ä¸ªå·¥å…·çš„c++ç‰ˆæœ¬è¿›è¡Œè¯„æµ‹ã€‚</p>
<p>1ã€LTP <a href="https://github.com/HIT-SCIR/ltp" target="_blank" rel="external">https://github.com/HIT-SCIR/ltp</a><br>2ã€NLPIR <a href="https://github.com/NLPIR-team/NLPIR" target="_blank" rel="external">https://github.com/NLPIR-team/NLPIR</a><br>3ã€THULAC <a href="https://github.com/thunlp/THULAC" target="_blank" rel="external">https://github.com/thunlp/THULAC</a><br>4ã€jieba <a href="https://github.com/yanyiwu/cppjieba" target="_blank" rel="external">https://github.com/yanyiwu/cppjieba</a></p>
<h1 id="æµ‹è¯•æ•°æ®é›†"><a href="#æµ‹è¯•æ•°æ®é›†" class="headerlink" title="æµ‹è¯•æ•°æ®é›†"></a>æµ‹è¯•æ•°æ®é›†</h1><p>1ã€SIGHAN Bakeoff 2005 MSR, 560KB  <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/</a><br>2ã€SIGHAN Bakeoff 2005 PKU, 510KB  <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/</a><br>3ã€äººæ°‘æ—¥æŠ¥ 2014, 65MB  <a href="https://pan.baidu.com/s/1hq3KKXe" target="_blank" rel="external">https://pan.baidu.com/s/1hq3KKXe</a></p>
<p>å‰ä¸¤ä¸ªæ•°æ®é›†æ˜¯SIGHANäº2005å¹´ç»„ç»‡çš„ä¸­æ–‡åˆ†è¯æ¯”èµ›æ‰€ç”¨çš„æ•°æ®é›†ï¼Œä¹Ÿæ˜¯å­¦æœ¯ç•Œæµ‹è¯•åˆ†è¯å·¥å…·çš„æ ‡å‡†æ•°æ®é›†ï¼Œæœ¬æ–‡ç”¨äºæµ‹è¯•å„å¤§åˆ†è¯å·¥å…·çš„å‡†ç¡®æ€§ï¼Œè€Œæœ€åä¸€ä¸ªæ•°æ®é›†è§„æ¨¡è¾ƒå¤§ï¼Œç”¨äºæµ‹è¯•åˆ†è¯é€Ÿåº¦ã€‚</p>
<h1 id="æµ‹è¯•æ–¹æ³•"><a href="#æµ‹è¯•æ–¹æ³•" class="headerlink" title="æµ‹è¯•æ–¹æ³•"></a>æµ‹è¯•æ–¹æ³•</h1><p>ç”¨SIGHAN Bakeoff 2005æ¯”èµ›ä¸­æ‰€è‡ªå¸¦çš„scoreè„šæœ¬ã€test goldæ•°æ®å’Œtraining wordsæ•°æ®å¯¹4ä¸ªå·¥å…·è¿›è¡Œå‡†ç¡®æ€§æµ‹è¯•ï¼Œå…·ä½“ä½¿ç”¨æ–¹æ³•å¯å‚è€ƒï¼š<a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip" target="_blank" rel="external">http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip</a> ä¸­çš„readmeæ–‡ä»¶ã€‚</p>
<h1 id="æµ‹è¯•ç¡¬ä»¶"><a href="#æµ‹è¯•ç¡¬ä»¶" class="headerlink" title="æµ‹è¯•ç¡¬ä»¶"></a>æµ‹è¯•ç¡¬ä»¶</h1><p>Intel Core i7-6700 CPU@3.40GHz*8</p>
<h1 id="æµ‹è¯•ç»“æœ"><a href="#æµ‹è¯•ç»“æœ" class="headerlink" title="æµ‹è¯•ç»“æœ"></a>æµ‹è¯•ç»“æœ</h1><p>1ã€MSRæµ‹è¯•ç»“æœ<br><img src="media/1.png" alt="1"></p>
<p>2ã€PKUæµ‹è¯•ç»“æœ<br><img src="media/2.png" alt="2"></p>
<p>3ã€äººæ°‘æ—¥æŠ¥æµ‹è¯•ç»“æœ<br><img src="media/3.png" alt="3"></p>
<h1 id="æµ‹è¯•ç»“è®º"><a href="#æµ‹è¯•ç»“è®º" class="headerlink" title="æµ‹è¯•ç»“è®º"></a>æµ‹è¯•ç»“è®º</h1><p>1ã€ä¸€ä¸ªå¥½çš„åˆ†è¯å·¥å…·ä¸åº”è¯¥åªèƒ½åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°ä¸é”™çš„æŒ‡æ ‡ï¼Œè€Œåº”è¯¥åœ¨å„ä¸ªæ•°æ®é›†éƒ½æœ‰å¾ˆä¸é”™çš„è¡¨ç°ã€‚ä»è¿™ä¸€ç‚¹æ¥çœ‹ï¼Œthulacå’Œltpéƒ½è¡¨ç°éå¸¸ä¸é”™ã€‚</p>
<p>2ã€å› ä¸ºåˆ†è¯æ˜¯ä¸ªåŸºç¡€éƒ¨ä»¶ï¼Œåˆ†è¯é€Ÿåº¦å¯¹äºä¸€ä¸ªåˆ†è¯å·¥å…·æ¥è¯´ä¹Ÿè‡³å…³é‡è¦ã€‚ä»è¿™ä¸€ç‚¹æ¥çœ‹ï¼Œthulacå’Œjiebaè¡¨ç°çš„ä¸é”™ã€‚</p>
<p>3ã€å¤§å®¶éƒ½çŸ¥é“ï¼ŒåŸºæœ¬çš„åˆ†è¯ä¾èµ–æ¨¡å‹ï¼Œä½†çœŸæ­£æƒ³ç”¨åˆ†è¯å·¥å…·æ¥è§£å†³åº”ç”¨å±‚é¢ä¸Šçš„é—®é¢˜ï¼Œéƒ½éœ€è¦å€ŸåŠ©äºè¯åº“ï¼Œæœ¬æ–‡æµ‹è¯•çš„4ä¸ªå·¥å…·å‡æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰è¯åº“ã€‚</p>
<p>4ã€ç‰¹åˆ«éœ€è¦å¼ºè°ƒçš„ä¸€ç‚¹æ˜¯ï¼Œå“ˆå·¥å¤§çš„ltpæ”¯æŒåˆ†è¯æ¨¡å‹çš„åœ¨çº¿è®­ç»ƒï¼Œå³åœ¨ç³»ç»Ÿè‡ªå¸¦æ¨¡å‹çš„åŸºç¡€ä¸Šå¯ä»¥ä¸æ–­åœ°å¢åŠ è®­ç»ƒæ•°æ®ï¼Œæ¥å¾—åˆ°æ›´åŠ ä¸°å¯Œã€æ›´åŠ ä¸ªæ€§åŒ–çš„åˆ†è¯æ¨¡å‹ã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>äº‰è®ºæ˜¯ä¸€ä¸ªå¥½çš„äº‹æƒ…ï¼Œå°¤å…¶æ˜¯ä¸åŒèƒŒæ™¯çš„äººç«™åœ¨ä¸åŒçš„è§’åº¦å¯¹åŒä¸€ä¸ªäº‹æƒ…è¿›è¡Œäº‰è®ºï¼Œå¸¸å¸¸ä¼šç¢°æ’å‡ºçŸ¥è¯†çš„ç«èŠ±ï¼Œå¯¹äºè¿™ä¸ªé¢†åŸŸçš„å‘å±•æœ‰æ›´å¥½åœ°æ¨åŠ¨ä½œç”¨ã€‚å¸Œæœ›ç±»ä¼¼çš„äº‰è®ºå¯ä»¥å¤šä¸€äº›ï¼Œè®©åˆšåˆšå…¥é—¨çš„æˆ–è€…å‡†å¤‡å…¥é—¨çš„ç«¥é‹å¯ä»¥æ›´åŠ å®¢è§‚åœ°çœ‹åˆ°ä¸€ä¸ªé¢†åŸŸçš„å‘å±•ç°çŠ¶ï¼Œè€Œä¸æ˜¯ç›²ç›®åœ°è¢«ä¸€äº›çƒ­é—¨çš„è¯è’™è”½åŒçœ¼ï¼Œå¤±å»åˆ¤æ–­ã€‚å¯¹äºåˆ†è¯æ¥è¯´ï¼Œæœ€è¿‘å‡ å¹´å¤§çƒ­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸ä¼šæ¯”ä¹‹å‰ä¼ ç»Ÿçš„crfæ¨¡å‹æœ‰å¤šå¤§æ€§èƒ½ä¸Šçš„çªç ´ï¼Œæ‰€ä»¥å¤§å®¶åº”è¯¥ç†æ€§åœ°çœ‹å¾…æ·±åº¦å­¦ä¹ ä»¥åŠäººå·¥æ™ºèƒ½ï¼Œæ§å¾—è¶Šé«˜å¯èƒ½æ‘”å¾—è¶Šæƒ¨ã€‚</p>
<h1 id="å‚è€ƒæ–‡çŒ®"><a href="#å‚è€ƒæ–‡çŒ®" class="headerlink" title="å‚è€ƒæ–‡çŒ®"></a>å‚è€ƒæ–‡çŒ®</h1><p>1ã€Zhongguo Li, Maosong Sun. Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics, vol. 35, no. 4, pp. 505-512, 2009.<br>2ã€Meishan Zhang, Yue Zhang, Guohong Fu. Transition-Based Neural Word Segmentation<br><a href="http://www.aclweb.org/anthology/P/P16/P16-1040.pdf" target="_blank" rel="external">http://www.aclweb.org/anthology/P/P16/P16-1040.pdf</a><br>3ã€Meishan Zhang, Zhilong Dengï¼ŒWanxiang Che, and Ting Liu. Combining Statistical Model and Dictionary for Domain Adaption of Chinese Word Segmentation. Journal of Chinese Information Processing. 2012, 26 (2) : 8-12 (in Chinese)<br>4ã€Wanxiang Che, Zhenghua Li, and Ting Liu. LTP: A Chinese Language Technology Platform. In Proceedings of the Coling 2010:Demonstrations. 2010.08, pp13-16, Beijing, China.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•è¨€&quot;&gt;&lt;a href=&quot;#å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;å¼•è¨€&quot;&gt;&lt;/a&gt;å¼•è¨€&lt;/h1&gt;&lt;p&gt;åˆ†è¯å¯¹äºç ”ç©¶å’Œåº”ç”¨ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„ç«¥é‹æ¥è¯´ï¼Œéƒ½æ˜¯ä¸€ä¸ªéå¸¸éå¸¸åŸºç¡€çš„éƒ¨ä»¶ï¼Œåˆ†è¯çš„è´¨é‡ç›´æ¥å½±å“åˆ°åç»­è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ã€å¥æ³•åˆ†æç­‰éƒ¨ä»¶çš„å‡†
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æœ¬å‘¨å€¼å¾—è¯»(2016.11.21-2016.11.25)</title>
    <link href="http://rsarxiv.github.io/2016/11/26/%E6%9C%AC%E5%91%A8%E5%80%BC%E5%BE%97%E8%AF%BB-2016-11-21-2016-11-25/"/>
    <id>http://rsarxiv.github.io/2016/11/26/æœ¬å‘¨å€¼å¾—è¯»-2016-11-21-2016-11-25/</id>
    <published>2016-11-27T06:12:34.000Z</published>
    <updated>2016-11-27T06:31:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ä¸€å‘¨å€¼å¾—è¯»"><a href="#ä¸€å‘¨å€¼å¾—è¯»" class="headerlink" title="ä¸€å‘¨å€¼å¾—è¯»"></a>ä¸€å‘¨å€¼å¾—è¯»</h1><h2 id="Generative-Deep-Neural-Networks-for-Dialogue-A-Short-Review"><a href="#Generative-Deep-Neural-Networks-for-Dialogue-A-Short-Review" class="headerlink" title="Generative Deep Neural Networks for Dialogue: A Short Review"></a><a href="http://t.cn/RfX2bms" target="_blank" rel="external">Generative Deep Neural Networks for Dialogue: A Short Review</a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘æœ¬æ–‡å¯¹seq2seqæ–¹æ³•åœ¨å¯¹è¯ç³»ç»Ÿä¸­çš„åº”ç”¨åšäº†ä¸€ä¸ªç®€çŸ­çš„å¯¹æ¯”å’Œç»¼è¿°ï¼Œä¸»è¦æ˜¯é’ˆå¯¹å‡ ä½ä½œè€…æå‡ºçš„ä¸‰ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šHREDã€VHREDå’ŒMrRNNï¼Œå®éªŒæ•°æ®ç”¨äº†Ubuntu Dialogue Corpuså’ŒTwitter Corpusã€‚ä¸ç®¡æ˜¯ç”¨seq2seqç”Ÿæˆä¹Ÿå¥½ï¼Œè¿˜æ˜¯å¥—ç”¨æ¨¡æ¿ä¹Ÿç½¢ï¼Œå¯¹è¯ç³»ç»Ÿçš„éš¾ç‚¹ä»æ˜¯ä¸Šä¸‹æ–‡çš„ç†è§£å’Œå¦‚ä½•è¾“å‡ºä¸€äº›é«˜è´¨é‡çš„å¯¹è¯ï¼Œæœ‰äº›åº”ç”¨åœºæ™¯å¯¹responseçš„è¦æ±‚æ²¡é‚£ä¹ˆé«˜ï¼Œåªè¦å¯ä»¥è¾¾åˆ°ä¸€å®šå®é™…æ•ˆæœå³å¯ï¼Œè€Œæœ‰çš„åˆ™éœ€è¦ç”Ÿæˆæ›´åŠ æ¥è¿‘äººç±»çš„å¯¹è¯ã€‚æœ¬æ–‡é€‚åˆç ”ç©¶æ·±åº¦seq2seqçš„ç«¥é‹ä»¥åŠæƒ³çœ‹çœ‹å„ç§seq2seqæ•ˆæœå¦‚ä½•çš„ç«¥é‹æ¥è¯»ã€‚æœ¬æ–‡æ€»ç»“çš„ä¸‰ä¸ªæ¨¡å‹åŸæ–‡é“¾æ¥ï¼š</p>
<p>(a) MrRNN: <a href="https://arxiv.org/pdf/1606.00776.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1606.00776.pdf</a><br>(b) VHRED: <a href="https://arxiv.org/pdf/1605.06069v3.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1605.06069v3.pdf</a><br>(c) HRED: <a href="https://arxiv.org/pdf/1507.04808v3.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1507.04808v3.pdf</a></p>
<h2 id="Coherent-Dialogue-with-Attention-based-Language-Models"><a href="#Coherent-Dialogue-with-Attention-based-Language-Models" class="headerlink" title="Coherent Dialogue with Attention-based Language Models"></a><a href="http://t.cn/Rfag1Jx" target="_blank" rel="external">Coherent Dialogue with Attention-based Language Models</a></h2><p>ã€å¯¹è¯ç³»ç»Ÿã€‘è€ƒè™‘å¹¶ç†è§£ä¸Šä¸‹æ–‡æ˜¯Chatbotçš„ä¸€å¤§éš¾ç‚¹ï¼Œä¹Ÿæ˜¯ç›®å‰ç»å¤§å¤šæ•°chatbotä¸æ™ºèƒ½çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€çš„attentionæ¨¡å‹ï¼Œåœ¨ç†è§£ç”¨æˆ·è¯·æ±‚çš„æ—¶å€™ï¼ŒåŠ¨æ€åœ°è€ƒè™‘å†å²ä¿¡æ¯ã€‚æœ¬æ–‡ç”¨åˆ°äº†ä¸¤ä¸ªå¼€æ”¾æ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯MovieTripleså’ŒUbuntu Troubleshoot datasetã€‚å»ºè®®å¯¹chatbotæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç²¾è¯»æ­¤æ–‡ã€‚</p>
<h2 id="Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks"><a href="#Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks" class="headerlink" title="Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks"></a><a href="http://t.cn/RfXLHIP" target="_blank" rel="external">Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks</a></h2><p>ã€è¯¾ç¨‹å­¦ä¹ ã€‘Curriculum Learningæ˜¯ä¸€ç±»æ¨¡æ‹Ÿå°å­©å­å­¦ä¹ è¿‡ç¨‹çš„å­¦ä¹ ç®—æ³•ï¼Œç®€å•åœ°è¯´æ˜¯æŒ‡åœ¨è®­ç»ƒæ¨¡å‹æ˜¯ä»ç®€å•çš„æ ·æœ¬å¼€å§‹ï¼Œé€æ¸å¢åŠ å­¦ä¹ æ ·æœ¬çš„éš¾åº¦ã€‚æœ¬æ–‡ä»¥æƒ…æ„Ÿåˆ†æä¸ºç ”ç©¶å¯¹è±¡ï¼Œå¯¹Curriculum Learningå¦‚ä½•æå‡LSTMæ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šçš„æ•ˆæœè¿›è¡Œäº†å®éªŒç ”ç©¶ï¼Œå¹¶ç»™å‡ºäº†å¯è§†åŒ–çš„ç»“æœã€‚æœ¬æ–‡é€‚åˆç ”ç©¶Curriculum Learningçš„ç«¥é‹ä»¥åŠåœ¨è®­ç»ƒæ¨¡å‹ä¸­æƒ³å°è¯•ä¸‹Curriculum Learningæ€è·¯çš„ç«¥é‹ç ”è¯»ã€‚</p>
<h2 id="Variable-Computation-in-Recurrent-Neural-Networks"><a href="#Variable-Computation-in-Recurrent-Neural-Networks" class="headerlink" title="Variable Computation in Recurrent Neural Networks"></a><a href="http://t.cn/RfXUmyN" target="_blank" rel="external">Variable Computation in Recurrent Neural Networks</a></h2><p>ã€RNNç ”ç©¶ã€‘RNNåœ¨è§£å†³åºåˆ—å»ºæ¨¡é—®é¢˜æœ‰ç€å¤©ç„¶çš„ä¼˜åŠ¿ï¼Œä½†æœ‰äº›åºåˆ—æ•°æ®å­˜åœ¨å‘¨æœŸæ€§çš„å˜åŒ–ï¼Œæˆ–è€…çŸ­æ—¶é—´å†…å˜åŒ–å¹¶ä¸æ˜æ˜¾ï¼Œæ¯”å¦‚è§†é¢‘æ•°æ®ï¼Œå› æ­¤å›ºå®šä¸å˜çš„RNNè®­ç»ƒæ–¹æ¡ˆä¼šæµªè´¹è®¡ç®—èµ„æºï¼Œæœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§RNNå˜è®¡ç®—è®­ç»ƒæ–¹æ¡ˆï¼Œå³åœ¨è®¡ç®—ä¸‹ä¸€ä¸ªtime stepçš„hidden stateæ—¶ï¼Œä¸éœ€è¦ä¸Šä¸€ä¸ªtime stepæ‰€æœ‰çš„ç»´åº¦ï¼Œåªå–ä¸€éƒ¨åˆ†æ¥è®¡ç®—ï¼Œå…¶ä»–çš„ç»´åº¦å¤åˆ¶è¿‡æ¥å³å¯ã€‚è¿™ç¯‡å·¥ä½œçš„ç›¸å…³çš„å‰äººç ”ç©¶åŒ…æ‹¬ï¼š2014å¹´çš„A Clockwork RNNï¼Œé“¾æ¥å¦‚ä¸‹ï¼š<a href="https://arxiv.org/pdf/1402.3511.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1402.3511.pdf</a></p>
<h2 id="Learning-to-Distill-The-Essence-Vector-Modeling-Framework"><a href="#Learning-to-Distill-The-Essence-Vector-Modeling-Framework" class="headerlink" title="Learning to Distill: The Essence Vector Modeling Framework"></a><a href="http://t.cn/RfoWk0K" target="_blank" rel="external">Learning to Distill: The Essence Vector Modeling Framework</a></h2><p>ã€è¡¨ç¤ºå­¦ä¹ ã€‘æœ¬æ–‡ç ”ç©¶çš„å†…å®¹åŒ…æ‹¬ä¸¤ä¸ªç‚¹ï¼Œä¸€ä¸ªæ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œä¸€ä¸ªæ˜¯æ–‡æ¡£è¡¨ç¤ºã€‚è¯è¡¨ç¤ºã€å¥å­è¡¨ç¤ºéƒ½æœ‰æ¯”è¾ƒå¤šçš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®é™…åº”ç”¨ä¸­æ–‡æ¡£çº§åˆ«çš„è¡¨ç¤ºéå¸¸é‡è¦ï¼Œæ¯”å¦‚æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•å¯¹æ–‡æ¡£ä»¥åŠèƒŒåæ‰€è•´è—çš„èƒŒæ™¯çŸ¥è¯†è¿›è¡Œä½ç»´è¡¨ç¤ºã€‚è‡ªç„¶è¯­è¨€éšç€å…ƒç´ çº§åˆ«åœ°æå‡ï¼ˆä»å­—ã€è¯ã€çŸ­è¯­ã€å¥å­åˆ°æ–‡æ¡£ï¼‰ï¼Œç ”ç©¶çš„éš¾åº¦éšä¹‹å¢åŠ ï¼Œå®ç”¨ç¨‹åº¦éšä¹‹å‡å°‘ã€‚å»ºè®®æƒ³ä»æ— ç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æ‰€çªç ´ä»¥åŠæƒ³è¯•è¯•æ–‡æ¡£è¡¨ç¤ºçš„ç«¥é‹å¯ä»¥æ¥è¯»æœ¬æ–‡ã€‚</p>
<h2 id="Unsupervised-Learning-of-Sentence-Representations-using-Convolutional-Neural-Networks"><a href="#Unsupervised-Learning-of-Sentence-Representations-using-Convolutional-Neural-Networks" class="headerlink" title="Unsupervised Learning of Sentence Representations using Convolutional Neural Networks"></a><a href="http://t.cn/Rf9VNdk" target="_blank" rel="external">Unsupervised Learning of Sentence Representations using Convolutional Neural Networks</a></h2><p>ã€å¥å­è¡¨ç¤ºã€‘æœ¬æ–‡çš„è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„CNN-LSTM auto-encoderï¼Œä½œä¸ºä¸€ç§æ— ç›‘ç£çš„å¥å­å­¦ä¹ æ¨¡å‹ã€‚</p>
<h2 id="Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers"><a href="#Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers" class="headerlink" title="Emergent Logical Structure in Vector Representations of Neural Readers"></a><a href="http://t.cn/Rf9Vwi7" target="_blank" rel="external">Emergent Logical Structure in Vector Representations of Neural Readers</a></h2><p>ã€é—®ç­”ç³»ç»Ÿã€‘é’ˆå¯¹æœ€è¿‘æå‡ºçš„å„ç§å„æ ·çš„attention based reader models,æœ¬æ–‡ä½œè€…åšäº†ä¸€ä¸ªæ¯”è¾ƒå…¨é¢çš„æ€»ç»“å’Œåˆ†æï¼Œå¹¶ä¸”é€šè¿‡æ•°å­¦åˆ†æå’Œå®éªŒå±•ç¤ºäº†æ¨¡å‹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚PaperWeeklyç¬¬åå››æœŸçš„æ–‡ç« æœ‰ç›¸å…³çš„paper noteå¯ä»¥å‚è€ƒ<a href="http://rsarxiv.github.io/2016/11/19/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%9C%9F/">åœ°å€</a></p>
<h1 id="å…¬ç›Šå¹¿å‘Š"><a href="#å…¬ç›Šå¹¿å‘Š" class="headerlink" title="å…¬ç›Šå¹¿å‘Š"></a>å…¬ç›Šå¹¿å‘Š</h1><p>ç¾å›½å›½ç«‹å«ç”Ÿç ”ç©¶é™¢æ‹›åšå£«åï¼Œç ”ç©¶é¢†åŸŸåŒ…æ‹¬ï¼šNLPã€text miningå’Œmachine learningï¼Œæ„Ÿå…´è¶£çš„ç«¥é‹å¯ä»¥çœ‹è¿‡æ¥ï¼Œè¯¦æƒ…è¯·æˆ³<a href="https://www.stat.washington.edu/jobs/archive/2013/may/05.20.13_NIH_E_B_NLP_Post_Doc_Ad_PDF_May_20_2013.pdf" target="_blank" rel="external">è¿™é‡Œ</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;a href=&quot;#ä¸€å‘¨å€¼å¾—è¯»&quot; class=&quot;headerlink&quot; title=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;/a&gt;ä¸€å‘¨å€¼å¾—è¯»&lt;/h1&gt;&lt;h2 id=&quot;Generative-Deep-Neural-Networks-for-Dialogue-A-Short-
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬åäº”æœŸ</title>
    <link href="http://rsarxiv.github.io/2016/11/26/PaperWeekly-%E7%AC%AC%E5%8D%81%E4%BA%94%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/26/PaperWeekly-ç¬¬åäº”æœŸ/</id>
    <published>2016-11-26T18:37:08.000Z</published>
    <updated>2016-11-26T18:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h1><p>NMTæ˜¯çƒ­é—¨ç ”ç©¶é¢†åŸŸä¹‹ä¸€ï¼Œå°¤å…¶æ˜¯Googleå’Œç™¾åº¦éƒ½æ¨å‡ºäº†è‡ªå·±çš„NMTç¿»è¯‘ç³»ç»Ÿï¼Œåœ¨å·¥ä¸šç•Œã€å­¦æœ¯ç•Œå’Œç¿»è¯‘ç•Œéƒ½å¼•èµ·äº†è½©ç„¶å¤§æ³¢ï¼Œä¸€æ—¶é—´å¯¹NMTæŠ€æœ¯çš„ç ”ç©¶å’Œè®¨è®ºè¾¾åˆ°äº†é¡¶å³°ã€‚Attentionæ¨¡å‹åœ¨NLPä¸­æœ€æ—©çš„ä½¿ç”¨æ­£æ˜¯åœ¨NMTé¢†åŸŸå‡ºç°çš„ï¼ŒåŒ…æ‹¬æ¨ªæ‰«å¾ˆå¤šé¢†åŸŸçš„seq2seq+attentionè§£å†³æ–¹æ¡ˆï¼Œéƒ½æ˜¯åœ¨NMTæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œç›¸åº”çš„ä¸€äº›å°æ”¹åŠ¨è€Œæˆçš„ã€‚æ‰€ä»¥ï¼Œæœ¬æœŸPaperWeeklyå¸¦å¤§å®¶çœ‹ä¸€çœ‹æœ€è¿‘ä¸¤å¹´Attentionæ¨¡å‹åœ¨NMTé¢†åŸŸä¸­çš„ç ”ç©¶è¿›å±•ï¼Œæœ¬æ–‡åŒ…æ‹¬ä»¥ä¸‹paperï¼š</p>
<p>1ã€Neural Machine Translation by Jointly Learning to Align and Translate, 2015<br>2ã€Effective approaches to attention-based neural machine translation, 2015<br>3ã€Modeling Coverage for Neural Machine Translation,  2016<br>4ã€Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation, 2016<br>5ã€Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation, 2016</p>
<h1 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate" class="headerlink" title="Neural Machine Translation by Jointly Learning to Align and Translate"></a><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio</p>
<h2 id="å•ä½"><a href="#å•ä½" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Jacobs University Bremen, Germany<br><br>Universite Ì de Montre Ìal</p>
<h2 id="å…³é”®è¯"><a href="#å…³é”®è¯" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>NMT, attention</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2015</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>è¿™ç¯‡è®ºæ–‡é¦–æ¬¡æå‡ºåœ¨NMTä¸­ä½¿ç”¨attentionçš„æœºåˆ¶ï¼Œå¯ä»¥ä½¿æ¨¡å‹è‡ªåŠ¨ç¡®å®šæºå¥å­ä¸­å’Œç›®æ ‡è¯è¯­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œç›¸æ¯”äºåŸºæœ¬çš„encoder-decoderæ–¹æ³•æé«˜äº†ç¿»è¯‘æ•ˆæœã€‚</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>è¯¥è®ºæ–‡ä½¿ç”¨çš„åŸºæœ¬æ¨¡å‹æ˜¯ä¸€ä¸ªåŒå‘RNNçš„encoder-decoderçš„ç»“æ„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¹‹å‰ï¼Œencoderéƒ¨åˆ†éƒ½æ˜¯ç›´æ¥æŠŠè¾“å…¥å¥å­encodeæˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡cï¼Œç„¶ådecoderå†æ ¹æ®è¯¥å‘é‡æ¥äº§ç”Ÿç¿»è¯‘ã€‚ä½†æ˜¯ç”±äºå¥å­é•¿åº¦ä¸å®šï¼Œè¿™ç§åšæ³•å¯¹é•¿å¥å­çš„æ•ˆæœä¸ç†æƒ³ã€‚<br><img src="media/model.png" alt="mode"></p>
<p>ä¸Šå›¾æ˜¯è¿™ç¯‡è®ºæ–‡æå‡ºçš„æ¨¡å‹ç»“æ„ï¼Œä½œè€…é¦–æ¬¡æå‡ºäº†åœ¨decoderä¸­åŠ å…¥ä¸€ç§attentionçš„æœºåˆ¶ã€‚ç›´è§‚ä¸Šç†è§£ï¼Œå°±æ˜¯decoderå¯ä»¥å†³å®šæ›´å¤šåœ°æ³¨æ„åŸå¥å­ä¸­çš„æŸäº›éƒ¨åˆ†ï¼Œä»è€Œä¸å¿…æŠŠåŸå¥å­ä¸­çš„æ‰€æœ‰ä¿¡æ¯éƒ½encodeæˆä¸€ä¸ªå›ºå®šçš„å‘é‡ã€‚å…·ä½“æ¥è®²ï¼Œä¸Šä¸‹æ–‡å‘é‡ciç”±ä¸‹å¼è®¡ç®—å¾—å‡ºï¼š<br><img src="media/ci.png" alt="ci"></p>
<p>å…¶ä¸­ï¼Œ<br><img src="media/aij.png" alt="aij"></p>
<p>å…¶ä¸­ï¼Œ<br><img src="media/eij.png" alt="eij"></p>
<p>ä¸Šå¼ä¸­çš„aä¾¿æ˜¯alignment modelï¼Œå¯ä»¥ç”¨æ¥ä¼°è®¡ä½ç½®jé™„è¿‘çš„è¾“å…¥å’Œä½ç½®içš„è¾“å‡ºä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚æœ¬è®ºæ–‡ä¸­çš„alignment modelæ˜¯ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®ƒå’Œæ¨¡å‹ä¸­çš„å…¶å®ƒéƒ¨åˆ†ä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚</p>
<h2 id="èµ„æº"><a href="#èµ„æº" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€è‹±æ³•ç¿»è¯‘æ•°æ®é›† <a href="http://www.statmt.org/wmt14/translation-task.html" target="_blank" rel="external">ACL WMT â€™14</a></p>
<p>2ã€ä¸€ä¸ªåŸºæœ¬çš„RNN encoder-decoderæ¨¡å‹çš„å®ç° <a href="https://github.com/lisa-groundhog/GroundHog." target="_blank" rel="external">GroundHog</a></p>
<h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>1ã€2013å¹´ï¼Œä¸€ä¸ªç±»ä¼¼çš„aligningçš„æ–¹æ³•è¢«æå‡ºç”¨äºæ‰‹å†™ä½“ç”Ÿæˆã€‚è®ºæ–‡ï¼šGraves(2013) Generating sequences with recurrent neural networks<br>2ã€2014å¹´ï¼Œseq2seqçš„ç¥ç»ç½‘ç»œæ¨¡å‹ç”¨äºæœºå™¨ç¿»è¯‘ã€‚è®ºæ–‡ï¼šSutskever(2014) Sequence to sequence learning with neural networks</p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬è®ºæ–‡åˆ›æ–°æ€§åœ°åœ¨NMTä¸­æå‡ºäº†attentionçš„æœºåˆ¶ï¼Œå¯ä»¥ä½¿æ¨¡å‹åœ¨æ¯ä¸€æ­¥æ³¨æ„åˆ°æºå¥å­ä¸­ä¸åŒçš„éƒ¨åˆ†ï¼Œä»è€Œæé«˜äº†NMTçš„æ•ˆæœï¼Œè¯¥æ•ˆæœçš„æå‡å¯¹äºé•¿å¥å­çš„ç¿»è¯‘å°¤å…¶æ˜æ˜¾ã€‚</p>
<h1 id="Effective-approaches-to-attention-based-neural-machine-translation"><a href="#Effective-approaches-to-attention-based-neural-machine-translation" class="headerlink" title="Effective approaches to attention-based neural machine translation"></a><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="external">Effective approaches to attention-based neural machine translation</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Minh-Thang Luong, Hieu Pham, Christopher D. Manning</p>
<h2 id="å•ä½-1"><a href="#å•ä½-1" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Computer Science Department, Stanford University</p>
<h2 id="å…³é”®è¯-1"><a href="#å…³é”®è¯-1" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>NMT;Global Attention;Local Attention</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>EMNLP 2015</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>Attentionæœºåˆ¶å¼•å…¥æå¤§æå‡äº†NMTçš„ç¿»è¯‘è´¨é‡ï¼Œä½†å¯¹äºAttentionå®ç°æ¶æ„çš„è®¨è®ºè¿˜å¾ˆå°‘ï¼Œå°¤å…¶æ˜¯å…¨å±€Attentionçš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚æœ¬æ–‡å°±æ˜¯è®¨è®ºå„ç§ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬Global Attention, Local Attentionï¼ŒInput-feedingæ–¹æ³•ç­‰ã€‚</p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>Global Attenionï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡c_tæ—¶ï¼Œè€ƒè™‘åŸæ–‡ç¼–ç è¿‡ç¨‹ä¸­çš„æ‰€æœ‰éšçŠ¶æ€ã€‚<br>  <img src="media/1GlobalAttention.png" alt="1GlobalAttention"></p>
<p>Local Attentionï¼Œå¯¹äºæ¯ä¸ªæ­£åœ¨ç”Ÿæˆçš„è¯‘è¯ï¼Œé¢„æµ‹ä¸€ä¸ªåŸæ–‡å¯¹é½çš„ä½ç½®ï¼Œåªè€ƒè™‘è¯¥ä½ç½®å‰åä¸€ä¸ªçª—å£èŒƒå›´å†…çš„åŸæ–‡ç¼–ç éšçŠ¶æ€ã€‚  </p>
<p><img src="media/2LocalAttention.png" alt="2LocalAttention"></p>
<p>Input-feedingï¼Œç”¨ä¸€ä¸ªé¢å¤–çš„å‘é‡ï¼Œæ¥è®°ä½å“ªäº›è¯æ˜¯å·²ç»ç¿»è¯‘è¿‡çš„ï¼Œå³è€ƒè™‘äº†coverageçš„é—®é¢˜ã€‚  </p>
<p><img src="media/3Input-Feeding.png" alt="3Input-Feeding"></p>
<h2 id="èµ„æº-1"><a href="#èµ„æº-1" class="headerlink" title="èµ„æº"></a>èµ„æº</h2><p>1ã€è®­ç»ƒæ•°æ®ï¼šWMT14 (4.5Må¥å¯¹ï¼Œ116M è‹±æ–‡è¯ï¼Œ110Må¾·æ–‡è¯)<br>2ã€å¼€å‘é›†ï¼šnewstest2013 (3000å¥)<br>3ã€æµ‹è¯•é›†ï¼šnewstest2014(2737å¥)å’Œnewstest2015(2169å¥)<br>4ã€ä»£ç å’Œæ¨¡å‹å…±äº«åœ¨ï¼š<a href="http://nlp.stanford.edu/projects/nmt/" target="_blank" rel="external">http://nlp.stanford.edu/projects/nmt/</a></p>
<h2 id="ç›¸å…³å·¥ä½œ-1"><a href="#ç›¸å…³å·¥ä½œ-1" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ä¸»è¦æ˜¯followäº†(Bahdanau et al., 2015; Jean et al., 2015)çš„å·¥ä½œï¼Œå¯¹Attentionçš„æœºåˆ¶è¿›è¡Œäº†æ¢è®¨å’Œæ”¹è¿›ã€‚  </p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>English-Germançš„å®éªŒç»“æœï¼Œè¾ƒä¸ç”¨attentionçš„æ–¹æ³•æå‡äº†5ä¸ªå¤šç‚¹BLEUï¼Œå……åˆ†è¯æ˜äº†attentionçš„æœ‰æ•ˆæ€§ã€‚<br>å®éªŒç»“æœçš„è¡¨æ ¼è¯¦ç»†åˆ—å‡ºäº†å„ç§æ”¹è¿›æ–¹æ³•å¸¦æ¥çš„æ”¶ç›Šï¼Œè·Ÿè¿›è€…ä¸å¦¨ä»”ç»†çœ‹çœ‹ï¼ˆä»¥åŠç¬¬5èŠ‚çš„åˆ†æï¼‰ï¼Œå¯ä»¥å¾ˆå¿«äº†è§£å„ç§æŠ˜è…¾çš„æ–¹å‘ã€‚</p>
<p><img src="media/4ExperimentResult.png" alt="4ExperimentResult"></p>
<h2 id="å®Œæˆäººä¿¡æ¯"><a href="#å®Œæˆäººä¿¡æ¯" class="headerlink" title="å®Œæˆäººä¿¡æ¯"></a>å®Œæˆäººä¿¡æ¯</h2><p>å¾®åš @MyGod9ï¼Œè¯­æ™ºäº‘å¸†åˆ›å§‹äººï¼Œæœºå™¨ç¿»è¯‘è€å…µï¼ŒNMTè¿½éšè€…ï¼Œweiyongpeng@lingosail.com </p>
<h1 id="Modeling-Coverage-for-Neural-Machine-Translation"><a href="#Modeling-Coverage-for-Neural-Machine-Translation" class="headerlink" title="Modeling Coverage for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1601.04811v6.pdf" target="_blank" rel="external">Modeling Coverage for Neural Machine Translation</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li</p>
<h2 id="å•ä½-2"><a href="#å•ä½-2" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>è¯ºäºšæ–¹èˆŸå®éªŒå®¤ï¼Œæ¸…åå¤§å­¦</p>
<h2 id="å…³é”®è¯-2"><a href="#å…³é”®è¯-2" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>NMT</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ACL2016</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>è§£å†³ç»å…¸ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹ä¸­å­˜åœ¨çš„over-translationï¼ˆè¿‡åº¦ç¿»è¯‘ï¼‰å’Œunder-translation(ç¿»è¯‘ä¸è¶³ï¼‰çš„é—®é¢˜ã€‚</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>åœ¨ä¼ ç»ŸNMTæ¨¡å‹ä¸­ï¼ŒåŠ å…¥ç»Ÿè®¡æœºå™¨ç¿»è¯‘ç­–ç•¥ä¸­çš„coverageæ–¹æ³•ï¼Œæ¥è¿½è¸ªã€åˆ¤æ–­åŸå§‹å¥å­æ˜¯å¦è¢«ç¿»è¯‘ï¼Œå¦‚ä¸‹å›¾ã€å…¬å¼æ‰€ç¤ºã€‚<br><img src="media/pic1.png" alt="pi"><br><img src="media/pic2.png" alt="pi"><br><img src="media/pic3.png" alt="pi"><br>å…¶ä¸­ï¼ŒCä¸ºæ–°å¼•å…¥çš„coverageå‘é‡ã€‚</p>
<h2 id="ç›¸å…³å·¥ä½œ-2"><a href="#ç›¸å…³å·¥ä½œ-2" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>å‰åºæ–‡ç« ï¼šNeural Machine Translation by Jointly Learning to Align and Translate</p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¯¥æ–‡æ˜¯åŸºäºNeural Machine Translation by Jointly Learning to Align and Translateä¹‹ä¸Šçš„å·¥ä½œï¼Œå¼•å…¥äº†ç»Ÿè®¡æœºå™¨ç¿»è¯‘ä¸­çš„Coverageæ–¹æ³•æ¥å°è¯•é¿å…NMTä¸­çš„ä¸€äº›é—®é¢˜ã€‚æ ¹æ®æ–‡ç« çš„è¯•éªŒç»“æœï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæå‡ç¿»è¯‘æ•ˆæœã€‚ç”±äºå†™ä½œæ­¤æ–‡æ—¶ç¬”è€…æœªä½œå®éªŒï¼Œå› æ­¤å®é™…æ•ˆæœæœ‰å¾…è¿›ä¸€æ­¥è¡¡é‡ã€‚</p>
<h1 id="Agreement-based-Joint-Training-for-Bidirectional-Attention-based-Neural-Machine-Translation"><a href="#Agreement-based-Joint-Training-for-Bidirectional-Attention-based-Neural-Machine-Translation" class="headerlink" title="Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation"></a><a href="https://arxiv.org/abs/1512.04650" target="_blank" rel="external">Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation</a></h1><h2 id="ä½œè€…-3"><a href="#ä½œè€…-3" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu</p>
<h2 id="å•ä½-3"><a href="#å•ä½-3" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Tsinghua University</p>
<h2 id="å…³é”®è¯-3"><a href="#å…³é”®è¯-3" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>Bidirectional NMT; Attention</p>
<h2 id="æ–‡ç« æ¥æº-3"><a href="#æ–‡ç« æ¥æº-3" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>IJCAI 2016</p>
<h2 id="é—®é¢˜-3"><a href="#é—®é¢˜-3" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ç”±äºè‡ªç„¶è¯­è¨€é”™ç»¼å¤æ‚çš„ç»“æ„ï¼Œå•å‘çš„æ³¨æ„åŠ›æ¨¡å‹åªèƒ½å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶çš„éƒ¨åˆ†regulizationã€‚æ–‡ç« æå‡ºäº†è”åˆè®­ç»ƒåŒå‘çš„æ³¨æ„åŠ›æ¨¡å‹ï¼Œå°½å¯èƒ½ä½¿æ³¨æ„åŠ›åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šä¿æŒä¸€è‡´ã€‚</p>
<h2 id="æ¨¡å‹-3"><a href="#æ¨¡å‹-3" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ¨¡å‹çš„ä¸­å¿ƒæ€æƒ³å°±æ˜¯å¯¹äºç›¸åŒçš„training dataï¼Œä½¿source-to-targetå’Œtarget-to-sourceä¸¤ä¸ªæ¨¡å‹åœ¨alignment matricesä¸Šä¿æŒä¸€è‡´ã€‚è¿™æ ·èƒ½å¤Ÿå»æ‰ä¸€äº›æ³¨æ„åŠ›å™ªå£°ï¼Œä½¿æ³¨æ„åŠ›æ›´åŠ é›†ä¸­ã€å‡†ç¡®ã€‚æ›´ç¡®åˆ‡åœ°è¯´ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç›®æ ‡å‡½æ•°ï¼š</p>
<p><img src="media/1.png" alt="1"></p>
<p>å…¶ä¸­<br><img src="media/2.png" alt="2">è¡¨ç¤ºsource-to-targetåŸºäºæ³¨æ„åŠ›çš„ç¿»è¯‘æ¨¡å‹ï¼Œè€Œ<img src="media/3.png" alt="3">è¡¨ç¤ºtarget-to-sourceçš„æ¨¡å‹ã€‚<img src="media/4.png" alt="4">è¡¨ç¤ºå¯¹äºå¥å­s source-to-targetçš„alignment matrixï¼Œè€Œ<img src="media/5.png" alt="5">è¡¨ç¤ºtarget-to-sourceçš„ã€‚<img src="media/6.png" alt="6">æ˜¯æŸå¤±å‡½æ•°ï¼Œå¯ä»¥è¡¡é‡ä¸¤ä¸ªalignment matrixä¹‹é—´çš„disagreeç¨‹åº¦ã€‚</p>
<p>å¯¹äº<img src="media/6.png" alt="6">,æœ‰å‡ ç§ä¸åŒçš„å®šä¹‰æ–¹æ³•ï¼š<br>1ã€Square of addition(SOA)<br><img src="media/7.png" alt="7"></p>
<p>2ã€Square of subtraction(SOS)<br><img src="media/9.png" alt="9"></p>
<p>3ã€Multiplication(MUL)<br><img src="media/10.png" alt="10"></p>
<h2 id="ç›¸å…³å·¥ä½œ-3"><a href="#ç›¸å…³å·¥ä½œ-3" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ä½œè€…æ–‡ä¸­è¯´çš„æ˜¯bidirectional translationçš„alignment matricesè¦ä¸€è‡´ï¼›è¿˜æœ‰å¦å¤–ä¸€ç¯‡æ–‡ç« â€œAgreement on Target-bidirectional Neural Machine Translationâ€æ˜¯è¯´decodingçš„æ—¶å€™å¯ä»¥æ­£å‘æˆ–è€…åå‘äº§ç”Ÿç›®æ ‡å¥å­ï¼ŒæŠŠè¿™äºŒè€…è¿›è¡Œè”åˆè®­ç»ƒã€‚å¦å¤–ï¼Œæœ€è¿‘ä¹Ÿæœ‰å¾ˆå¤šå…³äºbidirectional trainingæˆ–è€…ç±»ä¼¼æ€æƒ³çš„æ–‡ç« ï¼Œæ¯”å¦‚â€œDual Learning for Machine Translation. Computation and Languageâ€å°†reinforcementçš„æ¦‚å¿µå¼•å…¥äº†bidirectional trainingå½“ä¸­ï¼Œâ€œNeural Machine Translation with Reconstructionâ€ å¸Œæœ›èƒ½ä»target hidden stateæ¢å¤å‡ºsource sentence</p>
<h2 id="ç®€è¯„-3"><a href="#ç®€è¯„-3" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¿™ç¯‡æ–‡ç« èƒœåœ¨idea,å¾ˆå·§å¦™åœ°æƒ³åˆ°äº†è®©æ­£åå‘çš„æ³¨æ„åŠ›ä¸€è‡´æ¥æ”¹è¿›attentionã€‚</p>
<h1 id="Improving-Attention-Modeling-with-Implicit-Distortion-and-Fertility-for-Machine-Translation"><a href="#Improving-Attention-Modeling-with-Implicit-Distortion-and-Fertility-for-Machine-Translation" class="headerlink" title="Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation"></a><a href="https://arxiv.org/abs/1601.03317" target="_blank" rel="external">Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation</a></h1><h2 id="ä½œè€…-4"><a href="#ä½œè€…-4" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Shi Feng, Shujie Liu, Nan Yang, Mu Li, Ming Zhou, Kenny Q.Zhu</p>
<h2 id="å•ä½-4"><a href="#å•ä½-4" class="headerlink" title="å•ä½"></a>å•ä½</h2><p>Shanghai Jiao Tong University, Microsoft Research</p>
<h2 id="å…³é”®è¯-4"><a href="#å…³é”®è¯-4" class="headerlink" title="å…³é”®è¯"></a>å…³é”®è¯</h2><p>NMT, Attention, Fertility, Distortion</p>
<h2 id="æ–‡ç« æ¥æº-4"><a href="#æ–‡ç« æ¥æº-4" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>COLING 2016</p>
<h2 id="é—®é¢˜-4"><a href="#é—®é¢˜-4" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ä½¿ç”¨attentionæœºåˆ¶è§£å†³NMTä¸­è°ƒåºå’Œç¹è¡ç‡çš„é—®é¢˜ã€‚</p>
<h2 id="æ¨¡å‹-4"><a href="#æ¨¡å‹-4" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ¨¡å‹éå¸¸ç®€å•ï¼Œå³åœ¨attentionæœºåˆ¶ä¸­å°†å‰ä¸€æ—¶åˆ»çš„context vector cä½œä¸ºè¾“å…¥ä¼ å…¥å½“å‰æ—¶åˆ»attentionä¸­ï¼ˆå‘½åä¸ºRecAttï¼‰ã€‚å¦‚å›¾ï¼š</p>
<p><img src="media/coling.jpg" alt="coling"></p>
<p>é€šè¿‡è¿™æ ·çš„RecAttæœºåˆ¶ï¼Œattentionéƒ¨åˆ†çš„ç½‘ç»œç›¸å½“äºè®°å¿†äº†ä¹‹å‰æ—¶åˆ»çš„contextã€‚</p>
<h2 id="ç›¸å…³å·¥ä½œ-4"><a href="#ç›¸å…³å·¥ä½œ-4" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>ACL 2016æèˆªè€å¸ˆç»„çš„å·¥ä½œ Modeling Coverage for Neural Machine Translationåˆ©ç”¨äº†attentionæœºåˆ¶æ¥è§£å†³äº†NMTä¸­â€œæ¬ ç¿»è¯‘â€å’Œâ€œè¿‡ç¿»è¯‘â€çš„é—®é¢˜ã€‚</p>
<h2 id="ç®€è¯„-4"><a href="#ç®€è¯„-4" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¯¥æ–‡ç« çš„åˆ›æ–°ä¹‹å¤„åœ¨äºæå‡ºå°†attentionè®¡ç®—å¾—åˆ°çš„context vector cä½œä¸ºattentionçš„è¾“å…¥ï¼Œè¿™æ ·å°±æ˜¯çš„attentionæœºåˆ¶å¸¦æœ‰ä¸€ç§recurrentçš„æ„å‘³ã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>æœ¬æœŸPaperWeeklyç²¾é€‰äº†5ç¯‡Attentionæ¨¡å‹åœ¨NMTä»»åŠ¡ä¸Šçš„ç ”ç©¶å·¥ä½œï¼ŒAttentionæ¨¡å‹çš„å‘å±•ä¸ä»…ä»…æ¨åŠ¨ç€NMTçš„è¿›æ­¥ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å€Ÿé‰´äºå…¶ä»–çš„ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚QAï¼Œæ¯”å¦‚chatbotã€‚æ„Ÿè°¢@MyGod9 @é›¨ç¥ @susie-nmt @æäº‰ @magic282 äº”ä½ç«¥é‹çš„è¾›å‹¤ä»˜å‡ºã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•è¨€&quot;&gt;&lt;a href=&quot;#å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;å¼•è¨€&quot;&gt;&lt;/a&gt;å¼•è¨€&lt;/h1&gt;&lt;p&gt;NMTæ˜¯çƒ­é—¨ç ”ç©¶é¢†åŸŸä¹‹ä¸€ï¼Œå°¤å…¶æ˜¯Googleå’Œç™¾åº¦éƒ½æ¨å‡ºäº†è‡ªå·±çš„NMTç¿»è¯‘ç³»ç»Ÿï¼Œåœ¨å·¥ä¸šç•Œã€å­¦æœ¯ç•Œå’Œç¿»è¯‘ç•Œéƒ½å¼•èµ·äº†è½©ç„¶å¤§æ³¢ï¼Œä¸€æ—¶é—´å¯¹NMTæŠ€æœ¯
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>æ‚‰å°¼ç§‘æŠ€å¤§å­¦åšå£«åæ‹›è˜ä¿¡æ¯</title>
    <link href="http://rsarxiv.github.io/2016/11/19/%E6%82%89%E5%B0%BC%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E5%90%8E%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/"/>
    <id>http://rsarxiv.github.io/2016/11/19/æ‚‰å°¼ç§‘æŠ€å¤§å­¦åšå£«åæ‹›è˜ä¿¡æ¯/</id>
    <published>2016-11-20T04:47:55.000Z</published>
    <updated>2016-11-20T04:49:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, I am recruiting a Postdoc in Machine Learning for two (2) years. In brief, the candidate should:</p>
<p>1 Hold a PhD in machine learning and have a good research track record.</p>
<p>2 Have a genuine interest in mathematical modeling.</p>
<p>3 Good communication skills and is willing to help PhD students resolving their mathematical issues.</p>
<p>4 Excellent in programming and experimentation.</p>
<p>The candidate will work with Dr Richard Xu (Yida.Xu@uts.edu.au), where his teamâ€™s recent research themes include: Bayesian Non-Parametric (BNP), Monte-Carlo inference, Matrix (and Tensor) factorization and Deep Learning. The application areas include both computer vision and document. There is no strict requirement that the candidate must align his/her research exactly to Richardâ€™s existing work, i.e., the candidate can continue to work in his/her established field as long as the group benefit from his/her presence.</p>
<p>Please contact Richard to obtain further information, and remember to check out his website:</p>
<p><a href="http://www-staff.it.uts.edu.au/~ydxu/" target="_blank" rel="external">http://www-staff.it.uts.edu.au/~ydxu/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi, I am recruiting a Postdoc in Machine Learning for two (2) years. In brief, the candidate should:&lt;/p&gt;
&lt;p&gt;1 Hold a PhD in machine learn
    
    </summary>
    
    
      <category term="æ‹›è˜" scheme="http://rsarxiv.github.io/tags/%E6%8B%9B%E8%81%98/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.11.14-2016.11.18</title>
    <link href="http://rsarxiv.github.io/2016/11/19/cs-CL-weekly-2016-11-14-2016-11-18/"/>
    <id>http://rsarxiv.github.io/2016/11/19/cs-CL-weekly-2016-11-14-2016-11-18/</id>
    <published>2016-11-20T04:30:55.000Z</published>
    <updated>2016-11-20T04:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ä¸€å‘¨å€¼å¾—è¯»"><a href="#ä¸€å‘¨å€¼å¾—è¯»" class="headerlink" title="ä¸€å‘¨å€¼å¾—è¯»"></a>ä¸€å‘¨å€¼å¾—è¯»</h1><h2 id="UTCNN-a-Deep-Learning-Model-of-Stance-Classificationon-on-Social-Media-Text"><a href="#UTCNN-a-Deep-Learning-Model-of-Stance-Classificationon-on-Social-Media-Text" class="headerlink" title="UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text"></a><a href="http://t.cn/RfGR4GE" target="_blank" rel="external">UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text</a></h2><p>ã€æ–‡æœ¬åˆ†ç±»ã€‘ã€ç¤¾äº¤ç½‘ç»œã€‘ç¤¾äº¤ç½‘ç»œä¸­è•´è—ç€å¤§é‡çš„éç»“æ„åŒ–çš„æ–‡æœ¬ï¼Œå¯¹ç¤¾äº¤ç½‘ç»œçš„æŒ–æ˜ä¹Ÿæ˜¯ä¸€å—é‡è¦çš„ç ”ç©¶å†…å®¹ã€‚æœ¬æ–‡ç ”ç©¶å†…å®¹ä¸ºç«‹åœºåˆ†ç±»ï¼Œåˆ›æ–°ä¹‹å¤„åœ¨äºåšåˆ†ç±»æ—¶ä¸ä»…ä»…è€ƒè™‘è¯¥æ–‡æœ¬ä¿¡æ¯æœ¬èº«ï¼Œè€Œä¸”è€ƒè™‘äº†ä¸è¯¥æ–‡æœ¬ç›¸å…³çš„è¯„è®ºã€åé¦ˆã€ç”¨æˆ·ä¿¡æ¯ã€è¯é¢˜ç­‰å„ç§æ–‡æœ¬ä¿¡æ¯ã€‚æ¨¡å‹éƒ¨åˆ†æ²¡æœ‰å¤ªå¤šçš„æ–°æ„ï¼Œæ˜¯ç»å…¸çš„CNNã€‚æœ¬æ–‡é€‚åˆåšç¤¾äº¤ç½‘ç»œæ–‡æœ¬æŒ–æ˜çš„ç«¥é‹æ¥è¯»ã€‚</p>
<h2 id="A-Way-out-of-the-Odyssey-Analyzing-and-Combining-Recent-Insights-for-LSTMs"><a href="#A-Way-out-of-the-Odyssey-Analyzing-and-Combining-Recent-Insights-for-LSTMs" class="headerlink" title="A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs"></a><a href="http://t.cn/RfVSmk7" target="_blank" rel="external">A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs</a></h2><p>ã€æ–‡æœ¬åˆ†ç±»ã€‘RNNåŠå…¶æ‰©å±•LSTMåœ¨æ–‡æœ¬åˆ†ç±»ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œæœ¬æ–‡æ¢ç©¶äº†å¤šç§LSTMçš„å°å˜ç§å¯¹åˆ†ç±»ç»“æœçš„å½±å“ï¼Œä¸€äº›å°æ”¹å˜å¯¹ç»“æœè¿˜æ˜¯æœ‰ä¸€å®šå½±å“çš„ã€‚æœ¬æ–‡é€‚åˆå·¥ç¨‹ä¸Šç”¨LSTMè§£å†³æ–‡æœ¬åˆ†ç±»é—®é¢˜çš„ç«¥é‹ç²¾è¯»ã€‚</p>
<h2 id="Linguistically-Regularized-LSTMs-for-Sentiment-Classification"><a href="#Linguistically-Regularized-LSTMs-for-Sentiment-Classification" class="headerlink" title="Linguistically Regularized LSTMs for Sentiment Classification"></a><a href="http://t.cn/Rf56bpv" target="_blank" rel="external">Linguistically Regularized LSTMs for Sentiment Classification</a></h2><p>ã€æƒ…æ„Ÿåˆ†æã€‘æœ¬æ–‡æœ€å¤§çš„äº®ç‚¹åœ¨äºå°†è¯­è¨€å­¦èµ„æºï¼Œæ¯”å¦‚æƒ…æ„Ÿè¯å…¸ï¼Œå¦å®šè¯ï¼Œè¡¨ç¤ºç¨‹åº¦çš„è¯ç­‰ç­‰ä»¥çº¦æŸæ¡ä»¶çš„å½¢å¼èå…¥åˆ°äº†ç°æœ‰çš„å¥å­çº§åˆ«çš„LSTMåˆ†ç±»æ¨¡å‹ä¸­ï¼Œå–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚æ·±åº¦å­¦ä¹ ç«èµ·æ¥ä¹‹åï¼Œå¤§å®¶éƒ½æ¨å´‡æ•°æ®é©±åŠ¨çš„æ¨¡å‹ï¼Œå¸Œæœ›æ‰¾åˆ°ä¸€ç§ç®€å•ç²—ç³™çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œå¿½è§†äº†ç»å…¸çš„è‡ªç„¶è¯­è¨€èµ„æºå’Œè¯­è¨€å­¦çš„çŸ¥è¯†ã€‚ç»å…¸çš„è¿™äº›èµ„æºéƒ½æ˜¯éå¸¸å®è´µçš„ä¸œè¥¿ï¼Œå¦‚ä½•å°†è¿™äº›çŸ¥è¯†èå…¥åˆ°ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œæ˜¯ä¸ªå¾ˆéš¾ä½†å´éå¸¸æœ‰æ„ä¹‰çš„äº‹æƒ…ï¼Œæœ¬æ–‡åœ¨å¥å­çº§åˆ«çš„æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­åšäº†ç›¸å…³çš„æ¢ç´¢ã€‚æ¨èç ”ç©¶æƒ…æ„Ÿåˆ†æçš„ç«¥é‹ç²¾è¯»ã€‚</p>
<h2 id="Googleâ€™s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation"><a href="#Googleâ€™s-Multilingual-Neural-Machine-Translation-System-Enabling-Zero-Shot-Translation" class="headerlink" title="Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"></a><a href="http://t.cn/Rf5I4nw" target="_blank" rel="external">Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a></h2><p>ã€æœºå™¨ç¿»è¯‘ã€‘Google NMTç³»ç»Ÿæ”¯æŒZero-Shotç¿»è¯‘ï¼Œä»è®­ç»ƒé›†å……è¶³çš„A-&gt;Bï¼ŒB-&gt;Cä¸¤ä¸ªç¿»è¯‘æ¨¡å‹ä¸­å¯ä»¥æ¨å‡ºä¸€ä¸ªè´¨é‡ä¸é”™çš„A-&gt;Cæ¨¡å‹ï¼ŒAå’ŒCå¹¶ä¸éœ€è¦å¾ˆå……è¶³çš„è®­ç»ƒé›†ã€‚</p>
<h2 id="Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot"><a href="#Zero-resource-Machine-Translation-by-Multimodal-Encoder-decoder-Network-with-Multimedia-Pivot" class="headerlink" title="Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot"></a><a href="http://t.cn/Rf5IB9P" target="_blank" rel="external">Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot</a></h2><p>ã€å¤šæ¨¡æ€ã€‘ã€æœºå™¨ç¿»è¯‘ã€‘ç°åœ¨çš„äººå·¥æ™ºèƒ½è¿˜è¾¾ä¸åˆ°å¾ˆé«˜çš„æ™ºèƒ½æ°´å¹³ï¼Œä½†æ˜¯å¤„ç†æˆ–è€…ç†è§£ä¸€äº›ç¨å¾®åˆçº§çš„ä¸œè¥¿å¯èƒ½è¿˜è¡Œï¼Œæ¯”å¦‚3å²å­©å­çš„æ•…äº‹ä¹‹ç±»çš„ã€‚ä¹‹å‰èŒç”Ÿä¸€ä¸ªæƒ³æ³•ï¼Œèƒ½ä¸èƒ½é’ˆå¯¹å°å­©å­¦ä¹ ï¼Œæ¯”å¦‚å­¦ä¹ è‹±è¯­ï¼Œä¼ ç»Ÿçš„æ–¹æ³•å¯èƒ½æ˜¯ç»™ä¸€å¥ä¸­æ–‡ï¼Œæ•™ä¸€å¥è‹±è¯­ï¼Œæ„Ÿè§‰ç”¨åˆ°çš„ä¿¡æ¯é‡è¿˜æ˜¯å°‘ï¼Œèƒ½ä¸èƒ½ä¸€è¾¹çœ‹å›¾ï¼Œä¸€è¾¹å­¦ä¹ è‹±è¯­ï¼Œç›¸å½“äºç”¨åˆ°äº†å›¾åƒè¿™ä¸ªä¿¡æ¯ï¼Œå­©å­åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ä¼šå¤šä¸€äº›ä¿¡æ¯ç»´åº¦ï¼Œå­¦ä¹ æ•ˆæœå¯èƒ½ä¼šæ›´å¥½ä¸€äº›ã€‚æœ¬æ–‡æ­£æ˜¯åšäº†è¿™ä¹ˆä¸€ä»¶äº‹æƒ…ï¼Œå€ŸåŠ©å›¾åƒä½œä¸ºæœºå™¨ç¿»è¯‘çš„æ¡¥æ¢ã€‚</p>
<h2 id="Neural-Machine-Translation-with-Pivot-Languages"><a href="#Neural-Machine-Translation-with-Pivot-Languages" class="headerlink" title="Neural Machine Translation with Pivot Languages"></a><a href="http://t.cn/RftFqja" target="_blank" rel="external">Neural Machine Translation with Pivot Languages</a></h2><p>ã€æœºå™¨ç¿»è¯‘ã€‘å¾ˆå¤šè¯­è¨€çš„æœºå™¨ç¿»è¯‘éƒ½é¢ä¸´ç€ä¸€ä¸ªè¯­è¨€å¯¹æ•°æ®é›†åŒ®ä¹çš„é—®é¢˜ï¼Œä¸€ä¸ªæ¯”è¾ƒç›´è§‚çš„æ€è·¯æ˜¯ï¼Œç”¨ä¸€ç§å¸¸è§çš„è¯­è¨€ä½œä¸ºâ€œæ¡¥æ¢â€ï¼Œè¿æ¥èµ·ä¸¤ç§è¯­è¨€å¯¹æ•°æ®é›†åŒ®ä¹çš„è¯­è¨€ã€‚æ˜¨å¤©Googleçš„zero-shotä¹Ÿæ­£æ˜¯è¿™ä¹ˆä¸€ç§æ€è·¯ï¼Œæœ¬æ–‡ä¹Ÿåœ¨è¿™æ–¹é¢è¿›è¡Œäº†ç ”ç©¶å·¥ä½œï¼Œå³æƒ³åšA-&gt;Cçš„ç¿»è¯‘ï¼Œéœ€è¦æ‹¿ä¸€ä¸ªçƒ­é—¨è¯­è¨€Bä½œä¸ºæ¡¥æ¢ï¼Œæ„é€ ä¸€ä¸ªA-&gt;B,B-&gt;Cçš„è”åˆè®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ–‡ç”¨è‹±è¯­ä½œä¸ºBï¼Œç”¨å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­åˆ†åˆ«ä½œä¸ºAå’ŒCè¿›è¡Œäº†ä¸¤ç»„å®éªŒï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§â€œä¸²è¡Œâ€seq2seqçš„æ€è·¯ï¼Œå…¶å®å¯ä»¥å°è¯•ä¸€äº›å…¶ä»–çš„ä»»åŠ¡ï¼Œåšä¸€äº›seq2seq2seqâ€¦çš„æ¨¡å‹å‡ºæ¥ã€‚</p>
<h2 id="Joint-Representation-Learning-of-Text-and-Knowledge-for-Knowledge-Graph-Completion"><a href="#Joint-Representation-Learning-of-Text-and-Knowledge-for-Knowledge-Graph-Completion" class="headerlink" title="Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion"></a><a href="http://t.cn/Rf5J12U" target="_blank" rel="external">Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion</a></h2><p>ã€çŸ¥è¯†è¡¨ç¤ºã€‘â€œè”åˆå­¦ä¹ â€æ˜¯ä¸ªçƒ­é—¨è¯ï¼Œâ€œè”åˆå­¦ä¹ â€å¯ä»¥é¿å…ä¸€äº›è¯­è¨€åˆ†æè¿‡ç¨‹ï¼ˆæ¯”å¦‚ï¼šå¥æ³•ä¾å­˜åˆ†æï¼‰å¸¦æ¥çš„è¯¯å·®ã€‚æœ¬æ–‡åœ¨å­¦ä¹ è¯ã€å®ä½“å’Œå…³ç³»è¡¨ç¤ºæ—¶åŒæ—¶ç”¨åˆ°äº†textå’ŒçŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå¾—åˆ°äº†ä¸é”™çš„æ•ˆæœã€‚</p>
<h2 id="Multi-lingual-Knowledge-Graph-Embeddings-for-Cross-lingual-Knowledge-Alignment"><a href="#Multi-lingual-Knowledge-Graph-Embeddings-for-Cross-lingual-Knowledge-Alignment" class="headerlink" title="Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment"></a><a href="http://t.cn/Rf5XVuD" target="_blank" rel="external">Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment</a></h2><p>ã€çŸ¥è¯†å›¾è°±ã€‘ä¸€ä¸ªæ–°æ¨¡å‹æ¥å¡«å‘ï¼ŒTransç³»åˆ—çš„æ–°æˆå‘˜â€”â€”MTransE </p>
<h2 id="End-to-End-Neural-Sentence-Ordering-Using-Pointer-Network"><a href="#End-to-End-Neural-Sentence-Ordering-Using-Pointer-Network" class="headerlink" title="End-to-End Neural Sentence Ordering Using Pointer Network"></a><a href="http://t.cn/RftkYoF" target="_blank" rel="external">End-to-End Neural Sentence Ordering Using Pointer Network</a></h2><p>ã€å¥å­æ’åºã€‘ã€æ–‡æœ¬æ‘˜è¦ã€‘å¥å­æ’åºæ˜¯ä¸€é¡¹é‡è¦çš„åŸºæœ¬å·¥ä½œï¼Œå°¤å…¶æ˜¯åœ¨åšå•æ–‡æ¡£å’Œå¤šæ–‡æ¡£æ–‡æœ¬æŠ½å–å¼æ‘˜è¦æ—¶æ˜¾å¾—ç‰¹åˆ«é‡è¦ã€‚ç»å…¸çš„æ’åºæ–¹æ³•å‡ ä¹éƒ½æ˜¯è€ƒè™‘å•ä¸ªå¥å­æ‰€åŒ…å«çš„ä¿¡æ¯è¿›è¡Œæ’åºï¼Œå¿½ç•¥äº†å¥å­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ¬æ–‡å·¥ä½œå€Ÿé‰´äº†Pointer Networkçš„æ€è·¯ï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ’åºæ¨¡å‹ï¼Œåœ¨æ’åºæ—¶è€ƒè™‘å¥å­çš„ä¸Šä¸‹æ–‡ã€‚é€šè¿‡ä¸¤ç»„å®éªŒéªŒè¯äº†æœ¬æ–‡ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚æœºå™¨ç¿»è¯‘ä¸­çš„seq2seq+attentionå·²ç»æˆåŠŸçš„åº”ç”¨åœ¨äº†å¾ˆå¤šä»»åŠ¡ä¸Šï¼Œä½†é’ˆå¯¹å…·ä½“ä»»åŠ¡ä¸åŒçš„ç‰¹ç‚¹è¿›è¡Œé’ˆå¯¹æ€§åœ°ä¿®æ­£ä¼šå¸¦æ¥æ¯”è¾ƒç†æƒ³çš„ç»“æœã€‚å»ºè®®ç ”ç©¶æ–‡æœ¬æ‘˜è¦çš„ç«¥é‹è¯»æœ¬æ–‡ã€‚</p>
<h2 id="The-Amazing-Mysteries-of-the-Gutter-Drawing-Inferences-Between-Panels-in-Comic-Book-Narratives"><a href="#The-Amazing-Mysteries-of-the-Gutter-Drawing-Inferences-Between-Panels-in-Comic-Book-Narratives" class="headerlink" title="The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives"></a><a href="http://t.cn/RfVoHOF" target="_blank" rel="external">The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives</a></h2><p>ã€é—®ç­”ç³»ç»Ÿã€‘åŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”å·²ç»æœ‰å¾ˆå¤šæ•°æ®é›†äº†ï¼ŒåŸºäºå›¾åƒçš„é—®ç­”ä¹Ÿæœ‰ä¸€äº›æ•°æ®é›†äº†ã€‚æ¼«ç”»æ˜¯ä¸€ç±»å¤§å®¶å°æ—¶å€™éƒ½å–œæ¬¢çš„è¯»ç‰©ï¼ŒåŒ…å«äº†ä¸°å¯Œçš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®ï¼ˆå¯¹è¯ï¼‰ã€‚æœ¬æ–‡ç»™å‡ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…æ‹¬äº†ä¸°å¯Œçš„å›¾åƒå’Œæ–‡æœ¬ï¼Œè§„æ¨¡åœ¨120ä¸‡ï¼ˆ120GBï¼‰å·¦å³ã€‚æ•°æ®ç»™å‡ºäº†å‡ ä¸ªä»»åŠ¡ï¼ŒåŸºäºå›¾åƒçš„é—®ç­”ä»»åŠ¡ï¼ŒåŸºäºå¯¹è¯æ–‡æœ¬çš„é—®ç­”ä»»åŠ¡å’Œæ–‡æœ¬æ’åºä»»åŠ¡ã€‚å¯¹é—®ç­”æ„Ÿå…´è¶£ï¼Œæƒ³æ‰¾ä¸€äº›æ–°æ•°æ®æ¥åˆ·ä¸€åˆ·æ¦œçš„ç«¥é‹å¯ä»¥çœ‹è¿‡æ¥ã€‚</p>
<h1 id="ä¸€å‘¨èµ„æº"><a href="#ä¸€å‘¨èµ„æº" class="headerlink" title="ä¸€å‘¨èµ„æº"></a>ä¸€å‘¨èµ„æº</h1><h2 id="Highlights-of-EMNLP-2016-Dialogue-deep-learning-and-more"><a href="#Highlights-of-EMNLP-2016-Dialogue-deep-learning-and-more" class="headerlink" title="Highlights of EMNLP 2016: Dialogue, deep learning, and more"></a><a href="http://blog.aylien.com/highlights-emnlp-2016-dialogue-deeplearning-and-more/" target="_blank" rel="external">Highlights of EMNLP 2016: Dialogue, deep learning, and more</a></h2><p>NLPæŠ€æœ¯æœåŠ¡å…¬å¸Aylienå†™çš„EMNLP 2016æ€»ç»“</p>
<h1 id="ä¸€å¥è¯å…¬ç›Šå¹¿å‘Š"><a href="#ä¸€å¥è¯å…¬ç›Šå¹¿å‘Š" class="headerlink" title="ä¸€å¥è¯å…¬ç›Šå¹¿å‘Š"></a>ä¸€å¥è¯å…¬ç›Šå¹¿å‘Š</h1><p>æ‚‰å°¼ç§‘æŠ€å¤§å­¦Dr Richard Xuæ‹›æœºå™¨å­¦ä¹ åšå£«åï¼Œæ„Ÿå…´è¶£çš„ç«¥é‹çœ‹è¿‡æ¥ã€‚å…·ä½“ä¿¡æ¯è¯·ç‚¹é˜…è¯»åŸæ–‡<a href="http://rsarxiv.github.io/2016/11/19/%E6%82%89%E5%B0%BC%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E5%90%8E%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/">æŸ¥çœ‹é“¾æ¥</a>ã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;a href=&quot;#ä¸€å‘¨å€¼å¾—è¯»&quot; class=&quot;headerlink&quot; title=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;/a&gt;ä¸€å‘¨å€¼å¾—è¯»&lt;/h1&gt;&lt;h2 id=&quot;UTCNN-a-Deep-Learning-Model-of-Stance-Classificationo
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PaperWeekly ç¬¬åå››æœŸ</title>
    <link href="http://rsarxiv.github.io/2016/11/19/PaperWeekly-%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%9C%9F/"/>
    <id>http://rsarxiv.github.io/2016/11/19/PaperWeekly-ç¬¬åå››æœŸ/</id>
    <published>2016-11-19T17:58:02.000Z</published>
    <updated>2016-11-19T18:34:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å¼•è¨€"><a href="#å¼•è¨€" class="headerlink" title="å¼•è¨€"></a>å¼•è¨€</h1><p>PaperWeeklyå·²ç»ä»‹ç»è¿‡ä¸å°‘Question Answeringçš„ç›¸å…³å·¥ä½œã€‚ä¸»è¦æœ‰DeepMind Attentive Readerï¼ŒFAIR Memory Networksï¼ŒDanqiâ€™s Stanford Reader, Attention Sum Reader, Gated Attention Sum Reader, Attention Over Attention Reader, etc. è¿™äº›æ¨¡å‹å…³è”æ€§å¾ˆå¤§ï¼Œæˆ–å¤šæˆ–å°‘å­˜åœ¨ç›¸ä¼¼ä¹‹å¤„ã€‚æœ¬æ–‡ç»™å¤§å®¶ä»‹ç»ä¸€ä¸‹Toyota Technological Institute at Chicago (TTIC)åœ¨Question Answeringæ–¹é¢çš„ç›¸å…³å·¥ä½œï¼Œå…±æœ‰3ç¯‡paperï¼š</p>
<p>1ã€Who did What: A Large-Scale Person-Centered Cloze Dataset, 2016<br>2ã€Broad Context Language Modeling as Reading Comprehension, 2016<br>3ã€Emergent Logical Structure in Vector Representations of Neural Readers, 2016</p>
<h1 id="Who-did-What-A-Large-Scale-Person-Centered-Cloze-Dataset"><a href="#Who-did-What-A-Large-Scale-Person-Centered-Cloze-Dataset" class="headerlink" title="Who did What: A Large-Scale Person-Centered Cloze Dataset"></a><a href="https://tticnlp.github.io/who_did_what/" target="_blank" rel="external">Who did What: A Large-Scale Person-Centered Cloze Dataset</a></h1><h2 id="ä½œè€…"><a href="#ä½œè€…" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David McAllester</p>
<h2 id="æ–‡ç« æ¥æº"><a href="#æ–‡ç« æ¥æº" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>EMNLP 2016</p>
<h2 id="é—®é¢˜"><a href="#é—®é¢˜" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>æ–‡ç« æ„å»ºäº†ä¸€ä¸ªæ–°çš„Question Answering datasetï¼Œâ€Who did Whatâ€ã€‚</p>
<p>sample instanceå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚<br><img src="media/example.png" alt="example"></p>
<p>é—®é¢˜çš„å¥å­æ€»æ˜¯æŒ–æ‰äº†ä¸€äº›named entitiesï¼Œç„¶åç»™å‡ºåœ¨æ–‡ä¸­å‡ºç°è¿‡çš„åˆ«çš„named entitiesä½œä¸ºé€‰é¡¹ã€‚è¿™ä¸€ä¸ªdatasetçš„éš¾åº¦è¦é«˜äºä¹‹å‰çš„CNN/DM datasetï¼Œå¯ä»¥ä½œä¸ºåˆ›å»ºæ–°æ¨¡å‹çš„å‚è€ƒæ•°æ®é›†ã€‚</p>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æ„å»ºæ­¤æ•°æ®é›†çš„æ–¹æ³•ä¸CNN/DMä¸åŒï¼Œé—®é¢˜å¹¶ä¸æ˜¯context passgeçš„ä¸€ä¸ªsummaryã€‚é—®é¢˜ä¸contextå‡æ¥è‡ªGigaword Corpusï¼Œä»–ä»¬æ˜¯ä¸¤ç¯‡éå¸¸ç›¸å…³çš„æ–‡ç« ã€‚</p>
<p>å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…ˆæ‰¾åˆ°ä¸€ç¯‡æ–‡ç« ï¼Œä½œä¸ºquestionæ–‡ç« ã€‚ç„¶åæå–å‡ºæ–‡ä¸­ç¬¬ä¸€å¥è¯çš„named entitiesï¼Œåˆ é™¤å…¶ä¸­çš„ä¸€ä¸ªnamed entityä½œä¸ºå°†è¦è¢«é¢„æµ‹çš„ç­”æ¡ˆã€‚ç„¶ååˆ©ç”¨è¿™ä¸€å¥question sentenceï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸€äº›Information Retrievalç³»ç»Ÿä»Gigaword Corpusæ‰¾åˆ°ä¸€ç¯‡ç›¸å…³çš„æ–‡ç« ä½œä¸ºpassageã€‚è¿™ç¯‡æ–‡ç« ä¸questionæ–‡ç« ä¸åŒï¼Œä½†æ˜¯åŒ…å«ç€ä¸question sentenceéå¸¸ç±»ä¼¼çš„ä¿¡æ¯ã€‚</p>
<p>æœ‰äº†passageä¹‹åï¼Œæˆ‘ä»¬å†ä»passageä¸­æ‰¾å‡ºnamed entitiesä½œä¸ºcandidate answersã€‚</p>
<p>ä¸ºäº†ä½¿ä»»åŠ¡éš¾åº¦æ›´å¤§ï¼Œæˆ‘ä»¬ç”¨ä¸€äº›ç®€å•çš„baseline (First person in passage, etc) å°†ä¸€äº›å¾ˆå®¹æ˜“åšå‡ºçš„é—®é¢˜åˆ æ‰ï¼Œåªç•™ä¸‹æ¯”è¾ƒå›°éš¾çš„instancesã€‚è¿™æ ·æ„å»ºçš„æ•°æ®æ¯”CNN/DMä¼šå›°éš¾ä¸å°‘ã€‚</p>
<h2 id="ç®€è¯„"><a href="#ç®€è¯„" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>ç›¸ä¿¡ä½œè€…åˆ›å»ºçš„æ–°æ•°æ®é›†ä¼šç»™Machine comprehensionå¸¦æ¥ä¸€äº›æ–°çš„é—®é¢˜ä¸æŒ‘æˆ˜ï¼Œæ˜¯å¾ˆæœ‰ä»·å€¼çš„èµ„æºã€‚æ–‡ç« é‡‡ç”¨çš„baseline suppresionæ–¹æ³•å¯ä»¥ç”¨æ¯”è¾ƒå°çš„ä»£ä»·åŠ å¤§é—®é¢˜çš„éš¾åº¦ï¼Œå€¼å¾—å‚è€ƒã€‚</p>
<h1 id="Broad-Context-Language-Modeling-as-Reading-Comprehension"><a href="#Broad-Context-Language-Modeling-as-Reading-Comprehension" class="headerlink" title="Broad Context Language Modeling as Reading Comprehension"></a><a href="https://arxiv.org/abs/1610.08431" target="_blank" rel="external">Broad Context Language Modeling as Reading Comprehension</a></h1><h2 id="ä½œè€…-1"><a href="#ä½œè€…-1" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester</p>
<h2 id="æ–‡ç« æ¥æº-1"><a href="#æ–‡ç« æ¥æº-1" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>arXiv</p>
<h2 id="é—®é¢˜-1"><a href="#é—®é¢˜-1" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>ä¸ä¹…å‰å‘å¸ƒçš„<a href="https://arxiv.org/abs/1606.06031" target="_blank" rel="external">LAMBADA dataset</a>ä¸­ï¼Œä½œè€…å°è¯•çš„å„ç§baseline modelséƒ½ç»™å‡ºäº†æ¯”è¾ƒå·®çš„ç»“æœã€‚</p>
<p>æ¯ä¸€ä¸ªLAMBADA instanceå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>
<p><img src="media/LAMBADA.png" alt="LAMBADA"></p>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>åœ¨è§‚å¯Ÿäº†LAMBADA datasetä¹‹åï¼Œæˆ‘ä»¬è®¤ä¸ºå¯ä»¥åˆ©ç”¨Reading comprehension modelsæ¥æå‡å‡†ç¡®ç‡ï¼Œè€Œä¸å¿…ä½¿ç”¨ä¼ ç»Ÿçš„language modelã€‚</p>
<p>ç”±äºstate of the art reading comprehension modelséœ€è¦ç»™å‡ºcandidate answersï¼Œç„¶åä»ä¸­é€‰å‡ºä¸€ä¸ªä½œä¸ºé¢„æµ‹çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬å°±å°†æ‰€æœ‰åœ¨contextä¸­å‡ºç°è¿‡çš„å•è¯éƒ½ä½œä¸ºä¸€ä¸ªcandidate answerã€‚</p>
<p>LAMBADAç»™å‡ºçš„è®­ç»ƒé›†æ˜¯ä¸€äº›å°è¯´çš„æ–‡æœ¬ã€‚ä¸ºäº†ä½¿è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„æ•°æ®ç±»å‹ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªbiased training setã€‚å…·ä½“çš„åšæ³•æ˜¯ï¼Œæˆ‘ä»¬å°†training setåˆ’åˆ†æˆ4-5å¥è¯çš„contextï¼Œç„¶åä¿è¯target wordåœ¨context passageä¸­å‡ºç°ï¼Œåªä¿ç•™è¿™æ ·çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨æ–°æ„å»ºçš„training setä¸Šè®­ç»ƒå„ç§attention based models,å¾—åˆ°äº†æ¯”åŸä½œè€…å¥½å¾—å¤šçš„æµ‹è¯•ç»“æœã€‚</p>
<p><img src="media/zewei-results.png" alt="zewei-results"></p>
<h2 id="ç®€è¯„-1"><a href="#ç®€è¯„-1" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä½œè€…åˆ©ç”¨äº†ç®€å•çš„æ–¹æ³•å’Œæ¨¡å‹å°†LAMBADA datasetçš„å‡†ç¡®ç‡ä»7.3%æé«˜åˆ°45.4%ï¼Œéå¸¸ç®€å•æœ‰æ•ˆã€‚</p>
<h1 id="Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers"><a href="#Emergent-Logical-Structure-in-Vector-Representations-of-Neural-Readers" class="headerlink" title="Emergent Logical Structure in Vector Representations of Neural Readers"></a><a href="http://openreview.net/pdf?id=ryWKREqxx" target="_blank" rel="external">Emergent Logical Structure in Vector Representations of Neural Readers</a></h1><h2 id="ä½œè€…-2"><a href="#ä½œè€…-2" class="headerlink" title="ä½œè€…"></a>ä½œè€…</h2><p>Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester</p>
<h2 id="æ–‡ç« æ¥æº-2"><a href="#æ–‡ç« æ¥æº-2" class="headerlink" title="æ–‡ç« æ¥æº"></a>æ–‡ç« æ¥æº</h2><p>ICLR 2017 Submission</p>
<h2 id="é—®é¢˜-2"><a href="#é—®é¢˜-2" class="headerlink" title="é—®é¢˜"></a>é—®é¢˜</h2><p>æœ€è¿‘æå‡ºçš„å„ç§å„æ ·çš„attention based reader models,æœ¬æ–‡ä½œè€…åšäº†ä¸€ä¸ªæ¯”è¾ƒå…¨é¢çš„æ€»ç»“å’Œåˆ†æï¼Œå¹¶ä¸”é€šè¿‡æ•°å­¦åˆ†æå’Œå®éªŒå±•ç¤ºäº†æ¨¡å‹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p>
<h2 id="æ¨¡å‹-2"><a href="#æ¨¡å‹-2" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p>æœ¬æ–‡ä½œè€…è®¤ä¸ºï¼Œå½“å‰çš„attention based modelså¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼Œaggregation readers(åŒ…æ‹¬attentive readerså’Œstanford readers)ä»¥åŠexplicit reference readers(åŒ…æ‹¬attention sum readerå’Œgated attention sum reader)ã€‚</p>
<p>è¿™ä¸¤ç§readerå¯ä»¥ç”¨å¦‚ä¸‹çš„å…¬å¼è”ç³»åœ¨ä¸€èµ·ã€‚</p>
<p><img src="media/formula1.png" alt="formula1"></p>
<p>è¦æ»¡è¶³ä¸Šè¿°ç­‰å¼ï¼Œåªéœ€è¦æ»¡è¶³ä¸‹é¢çš„å…¬å¼ã€‚</p>
<p><img src="media/formula2.png" alt="formula2"></p>
<p>ä¹Ÿå°±æ˜¯è¯´ï¼Œåªæœ‰æ­£ç¡®ç­”æ¡ˆæ‰€åœ¨çš„hidden vectorå’Œquestion vectorå¾—åˆ°çš„inner productæ‰èƒ½ç»™å‡ºä¸ä¸ºé›¶çš„å¸¸æ•°ã€‚ä»¥ä¸‹å®éªŒç»“è®ºæ”¯æŒäº†è¿™ä¸€å‡è®¾ã€‚</p>
<p><img src="media/experiment.png" alt="experiment"></p>
<p>ç”±äºCNN/DMåœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­ç»è¿‡äº†anonymizationï¼Œä½œè€…è®¤ä¸ºæ­¤inner productå…¶å®å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†ä¸anonymized token IDæœ‰å…³ï¼Œå¦ä¸€éƒ¨åˆ†ä¸IDæ— å…³ã€‚ä¸IDç›¸å…³çš„é‚£ä¸€éƒ¨åˆ†åœ¨inner productåº”è¯¥ç›´æ¥ç»™å‡º0çš„ç­”æ¡ˆã€‚å¦‚ä¸‹è¿°å…¬å¼æ‰€ç¤ºã€‚</p>
<p><img src="media/formula3.png" alt="formula3"></p>
<p>æœ¬æ–‡çš„å¦ä¸€éƒ¨åˆ†å·¥ä½œæ˜¯åœ¨attention readersä¸ŠåŠ å…¥ä¸€äº›linguistic featuresæå‡å„ä¸ªæ•°æ®é›†çš„å‡†ç¡®è¯»ï¼Œè¿™é‡Œä¸ä»”ç»†æè¿°ã€‚</p>
<h2 id="ç®€è¯„-2"><a href="#ç®€è¯„-2" class="headerlink" title="ç®€è¯„"></a>ç®€è¯„</h2><p>æœ¬æ–‡æ˜¯å¯¹äºå„ä¸ªattetion based neural reader modelså¾ˆå¥½çš„æ€»ç»“ï¼Œå®ƒå¾ˆå¥½åœ°è¿æ¥äº†å„ä¸ªä¸åŒçš„modelï¼Œè¯´æ˜äº†ä¸ºä½•çœ‹ä¼¼ä¸åŒçš„modelèƒ½å¤Ÿç»™å‡ºéå¸¸ç±»ä¼¼çš„ç»“æœã€‚</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><p>é—®ç­”ç³»ç»Ÿæ˜¯ä¸€ç±»å¤§çš„é—®é¢˜ï¼Œä¹Ÿæ˜¯ç›®å‰NLPåº”ç”¨çš„ç ”ç©¶çƒ­ç‚¹ä¹‹ä¸€ã€‚æœ¬æ–‡ä½œè€…ä»‹ç»äº†TTICåœ¨QAç ”ç©¶ä¸­çš„ä¸€äº›æˆæœï¼Œå…¶ä¸­ç¬¬äºŒç¯‡æ˜¯æœ¬æ–‡ä½œè€…è¿‘æœŸçš„paperã€‚æ„Ÿè°¢æ¥è‡ªèŠåŠ å“¥å¤§å­¦çš„@ZeweiChuç«¥é‹è¾›å‹¤çš„åŠ³åŠ¨ã€‚</p>
<h1 id="å…¬ç›Šå¹¿å‘Š"><a href="#å…¬ç›Šå¹¿å‘Š" class="headerlink" title="å…¬ç›Šå¹¿å‘Š"></a>å…¬ç›Šå¹¿å‘Š</h1><p>æ¸…åå¤§å­¦è®¡ç®—æœºç³»è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤æ‹›è˜åšå£«å</p>
<h2 id="å°†ä»äº‹çš„ç ”ç©¶æ–¹å‘"><a href="#å°†ä»äº‹çš„ç ”ç©¶æ–¹å‘" class="headerlink" title="å°†ä»äº‹çš„ç ”ç©¶æ–¹å‘"></a>å°†ä»äº‹çš„ç ”ç©¶æ–¹å‘</h2><p>å›´ç»•è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­ä¹‰åˆ†æã€ç»Ÿè®¡æœºå™¨ç¿»è¯‘æˆ–ç¤¾ä¼šè®¡ç®—å¼€å±•æ·±å…¥çš„ç ”ç©¶å·¥ä½œã€‚å®éªŒå®¤å…·ä½“ä¿¡æ¯è§ï¼š<a href="http://nlp.csai.tsinghua.edu.cn" target="_blank" rel="external">http://nlp.csai.tsinghua.edu.cn</a></p>
<h2 id="åº”è˜æ¡ä»¶"><a href="#åº”è˜æ¡ä»¶" class="headerlink" title="åº”è˜æ¡ä»¶"></a>åº”è˜æ¡ä»¶</h2><p>1ã€å…·æœ‰è®¡ç®—æœºç§‘å­¦æŠ€æœ¯æˆ–ç›¸å…³å­¦ç§‘åšå£«å­¦ä½ï¼ˆåšå£«æ¯•ä¸šä¸¤å¹´å†…ï¼‰ï¼›<br>2ã€ç†Ÿæ‚‰è‡ªç„¶è¯­è¨€å¤„ç†æˆ–æœºå™¨å­¦ä¹ çš„åŸºæœ¬ç†è®ºã€æ¨¡å‹ä¸ç®—æ³•ï¼Œæ›¾åœ¨å›½å†…å¤–é‡è¦å­¦æœ¯åˆŠç‰©æˆ–é‡è¦å›½é™…ä¼šè®®ï¼ˆCCF Aç±»ï¼‰ä¸Šå‘è¡¨ï¼ˆå«å·²å½•ç”¨ï¼‰é«˜æ°´å¹³å­¦æœ¯è®ºæ–‡ï¼›<br>3ã€åœ¨å¥æ³•åˆ†æã€è¯­ä¹‰åˆ†ææ–¹é¢æœ‰è¾ƒå¥½ç ”ç©¶åŸºç¡€è€…ä¼˜å…ˆï¼›<br>4ã€å…·æœ‰è¾ƒå¼ºçš„ç¼–ç¨‹èƒ½åŠ›åŠé¡¹ç›®ç ”å‘èƒ½åŠ›ï¼›<br>5ã€è´£ä»»å¿ƒå¼ºï¼Œå…·æœ‰è¾ƒå¥½çš„å›¢é˜Ÿåˆä½œç²¾ç¥å’Œåˆ›æ–°æ„è¯†ï¼Œè‹±è¯­é˜…è¯»åŠå†™ä½œèƒ½åŠ›è¾ƒå¼ºï¼›<br>6ã€ç¬¦åˆæ¸…åå¤§å­¦åšå£«åæ‹›æ”¶æ¡ä»¶ã€‚</p>
<h2 id="å·¥èµ„å¾…é‡"><a href="#å·¥èµ„å¾…é‡" class="headerlink" title="å·¥èµ„å¾…é‡"></a>å·¥èµ„å¾…é‡</h2><p>äº«å—æ¸…åå¤§å­¦åšå£«åå¾…é‡åŠè¯¾é¢˜ç»„æ´¥è´´ã€‚å…¨åŠ›æ”¯æŒç”³è¯·å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ã€å…¨å›½åšå£«åç®¡å§”ä¼šã€åŒ—äº¬å¸‚ã€æ¸…åå¤§å­¦çš„ç›¸å…³ç ”ç©¶è®¡åˆ’ã€‚</p>
<h2 id="ç”³è¯·ææ–™"><a href="#ç”³è¯·ææ–™" class="headerlink" title="ç”³è¯·ææ–™"></a>ç”³è¯·ææ–™</h2><p>1ã€ä¸ªäººç®€å†ã€å­¦ä½è¯ä¹¦åŠæˆç»©å•å¤å°ä»¶ï¼›<br>2ã€æœ€å…·ä»£è¡¨æ€§çš„è®ºæ–‡2ç¯‡ï¼›<br>3ã€åšå£«åæœŸé—´ç ”ç©¶è®¾æƒ³ï¼ˆç®€æ˜æ‰¼è¦ï¼‰ï¼›<br>4ã€å…¶å®ƒä»»ä½•æ”¯æŒææ–™ã€‚</p>
<h2 id="å¯¼å¸ˆåŠè”ç³»æ–¹å¼"><a href="#å¯¼å¸ˆåŠè”ç³»æ–¹å¼" class="headerlink" title="å¯¼å¸ˆåŠè”ç³»æ–¹å¼"></a>å¯¼å¸ˆåŠè”ç³»æ–¹å¼</h2><p>åˆä½œå¯¼å¸ˆï¼šå­™èŒ‚æ¾æ•™æˆã€åˆ˜çŸ¥è¿œåŠ©ç†æ•™æˆ<br>è”ç³»äººï¼šåˆ˜çŸ¥è¿œ<br>ç”µå­é‚®ä»¶ï¼š liuzy@tsinghua.edu.cn</p>
<p>æœ‰æ„è€…è¯·å°†ç”³è¯·ææ–™å‘è‡³ç”µå­é‚®ç®±ï¼Œè¯·åœ¨é‚®ä»¶ä¸»é¢˜ä¸­æ³¨æ˜å§“åå’Œâ€œç”³è¯·åšå£«åâ€ã€‚ææ–™é€šè¿‡åˆé€‰è€…è¿›è¡Œé¢è°ˆï¼ˆé¢è°ˆæ—¶é—´å¦è¡Œé€šçŸ¥ï¼‰ï¼Œç„¶åèµ°æ¸…åå¤§å­¦åšå£«åç”³è¯·ç¨‹åºã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;å¼•è¨€&quot;&gt;&lt;a href=&quot;#å¼•è¨€&quot; class=&quot;headerlink&quot; title=&quot;å¼•è¨€&quot;&gt;&lt;/a&gt;å¼•è¨€&lt;/h1&gt;&lt;p&gt;PaperWeeklyå·²ç»ä»‹ç»è¿‡ä¸å°‘Question Answeringçš„ç›¸å…³å·¥ä½œã€‚ä¸»è¦æœ‰DeepMind Attentive Reader
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>cs.CL weekly 2016.11.07-2016.11.11</title>
    <link href="http://rsarxiv.github.io/2016/11/13/cs-CL-weekly-2016-11-07-2016-11-11/"/>
    <id>http://rsarxiv.github.io/2016/11/13/cs-CL-weekly-2016-11-07-2016-11-11/</id>
    <published>2016-11-13T19:10:19.000Z</published>
    <updated>2016-11-13T19:23:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ä¸€å‘¨å€¼å¾—è¯»"><a href="#ä¸€å‘¨å€¼å¾—è¯»" class="headerlink" title="ä¸€å‘¨å€¼å¾—è¯»"></a>ä¸€å‘¨å€¼å¾—è¯»</h1><h2 id="Learning-Recurrent-Span-Representations-for-Extractive-Question-Answering"><a href="#Learning-Recurrent-Span-Representations-for-Extractive-Question-Answering" class="headerlink" title="Learning Recurrent Span Representations for Extractive Question Answering"></a><a href="https://arxiv.org/pdf/1611.01436v1.pdf" target="_blank" rel="external">Learning Recurrent Span Representations for Extractive Question Answering</a></h2><p>ã€æœºå™¨é˜…è¯»ã€‘ä¸åŒçš„é˜…è¯»ç†è§£æ•°æ®é›†äº§ç”Ÿç­”æ¡ˆçš„æ–¹å¼ä¸åŒï¼Œæœ‰çš„æ˜¯ç»™å®šNä¸ªå€™é€‰ç­”æ¡ˆï¼Œæœ‰çš„æ˜¯è§„å®šä»åŸæ–‡ä¸­çš„entityä¸­è¿›è¡Œé€‰æ‹©ï¼Œæœ‰çš„æ˜¯ä»åŸæ–‡ä¸­çš„ä»»æ„tokenè¿›è¡Œé€‰æ‹©ç­‰ç­‰ã€‚æœ¬æ–‡æ‰€ç”¨çš„æ•°æ®é›†æ˜¯SQuADï¼Œå€™é€‰ç­”æ¡ˆæ˜¯åŸæ–‡ä¸­çš„ä»»æ„å­—ç¬¦ä¸²ï¼Œéš¾åº¦è¾ƒå¤§ï¼Œç­”æ¡ˆå¯èƒ½æ˜¯ä¸€ä¸ªè¯æˆ–è€…å‡ ä¸ªè¯éƒ½æœ‰å¯èƒ½ã€‚æœ¬æ–‡åœ¨å‰äººç ”ç©¶çš„åŸºç¡€ä¸Šæå‡ºäº†ä¸€ç§æ˜¾å¼è¡¨ç¤ºanswer spançš„æ¨¡å‹ï¼Œå–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚</p>
<h2 id="Answering-Complicated-Question-Intents-Expressed-in-Decomposed-Question-Sequences"><a href="#Answering-Complicated-Question-Intents-Expressed-in-Decomposed-Question-Sequences" class="headerlink" title="Answering Complicated Question Intents Expressed in Decomposed Question Sequences"></a><a href="https://arxiv.org/pdf/1611.01242v1.pdf" target="_blank" rel="external">Answering Complicated Question Intents Expressed in Decomposed Question Sequences</a></h2><p>ã€å¤æ‚é—®ç­”ã€‘åŸºäºè¯­ä¹‰åˆ†æçš„é—®ç­”ç³»ç»Ÿæœ€è¿‘æµè¡Œäºè§£å†³é•¿ã€éš¾é—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶çš„å†…å®¹æ˜¯å¦‚ä½•å¤„ç†å¤šä¸ªç›¸äº’å…³è”çš„ç®€å•é—®é¢˜ï¼Ÿï¼ˆå³å°†å¤æ‚é—®é¢˜åˆ†è§£æˆå¤šä¸ªç›¸å…³ç®€ç­”é—®é¢˜ï¼‰å¹¶ç»™å‡ºäº†ä¸€ä¸ªä»»åŠ¡æ•°æ®é›†ã€‚è¿™ä¸ªé—®é¢˜çš„ä¸€å¤§éš¾ç‚¹åœ¨äºç›¸äº’å…³è”çš„é—®é¢˜éœ€è¦å…±æŒ‡æ¶ˆè§£çš„å·¥ä½œã€‚æœ¬æ–‡å°†å•è½®é—®ç­”å¯¹è¯åˆ†è§£æˆå¤šè½®é—®é¢˜è¿‡ç¨‹ï¼Œä¸Šä¸‹æ–‡çš„å¤„ç†éå¸¸é‡è¦ã€‚å»ºè®®ç ”ç©¶èŠå¤©æœºå™¨äººçš„ç«¥é‹æ¥ç²¾è¯»æ­¤æ–‡ã€‚</p>
<h2 id="Unsupervised-Pretraining-for-Sequence-to-Sequence-Learning"><a href="#Unsupervised-Pretraining-for-Sequence-to-Sequence-Learning" class="headerlink" title="Unsupervised Pretraining for Sequence to Sequence Learning"></a><a href="https://arxiv.org/pdf/1611.02683v1.pdf" target="_blank" rel="external">Unsupervised Pretraining for Sequence to Sequence Learning</a></h2><p>ã€seq2seqã€‘ã€æ–‡æœ¬æ‘˜è¦ã€‘seq2seqæ˜¯ä¸€ç§æ•ˆæœéå¸¸ä¸é”™çš„æ¡†æ¶ï¼Œå°¤å…¶æ˜¯è¾“å…¥-è¾“å‡ºæ•°æ®éå¸¸å……åˆ†çš„æ—¶å€™ã€‚ä½†å¾ˆå¤šè¯­è¨€ç¿»è¯‘é—®é¢˜å¹¶ä¸èƒ½æ‹¿åˆ°éå¸¸å¤šçš„è®­ç»ƒæ•°æ®ï¼Œæ•ˆæœå°±ä¼šæ‰“æŠ˜æ‰£ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œåœ¨åŸæœ‰seq2seqæ¡†æ¶ä¸Šæå‡ºäº†ä¸€ç§å°æ”¹åŠ¨ã€‚ encoderå’Œdecoderçš„åˆå§‹å€¼ç”¨è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹æ¥èµ‹å€¼ï¼Œç”¨å¯ä»¥è·å¾—çš„å°‘é‡è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè°ƒä¼˜ï¼Œæœ¬æ–‡æ–¹æ³•çš„æ•ˆæœåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡å’Œabstractiveå¼æ‘˜è¦ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚ä»æœ¬æ–‡ä¸­ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªå¥½çš„åˆå€¼ä¸ä»…å¯ä»¥ä½¿å¾—è®­ç»ƒæ›´å¿«ï¼Œè€Œä¸”å¯ä»¥å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚æœ¬æ–‡é€‚åˆç”¨seq2seqè§£å†³å·¥ç¨‹é—®é¢˜çš„ç«¥é‹è¯»ã€‚</p>
<h2 id="Sentence-Ordering-using-Recurrent-Neural-Networks"><a href="#Sentence-Ordering-using-Recurrent-Neural-Networks" class="headerlink" title="Sentence Ordering using Recurrent Neural Networks"></a><a href="https://arxiv.org/pdf/1611.02654v1.pdf" target="_blank" rel="external">Sentence Ordering using Recurrent Neural Networks</a></h2><p>ã€å¥å­æ’åºã€‘ã€æ–‡æœ¬æ‘˜è¦ã€‘å¥å­æ’åºä»»åŠ¡å¯¹äºç ”ç©¶æ–‡æ¡£çš„è¿è´¯æ€§éå¸¸æœ‰æ„ä¹‰ï¼Œè€Œè¿è´¯æ€§å¯¹äºå¾ˆå¤šä»»åŠ¡éå¸¸é‡è¦ï¼Œæ¯”å¦‚æ–‡æœ¬æ‘˜è¦ã€‚æœ¬æ–‡åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šç”¨äº†æµè¡Œçš„seq2seqæ–¹æ³•ï¼Œå¹¶ä¸”ç»™å‡ºäº†ä¸€ç§å¯è§†åŒ–çš„å¥å­è¡¨ç¤ºæ•ˆæœã€‚å»ºè®®ç ”ç©¶æ‘˜è¦çš„ç«¥é‹è¯»ã€‚</p>
<h2 id="Modeling-Coverage-for-Neural-Machine-Translation"><a href="#Modeling-Coverage-for-Neural-Machine-Translation" class="headerlink" title="Modeling Coverage for Neural Machine Translation"></a><a href="https://arxiv.org/pdf/1601.04811v6.pdf" target="_blank" rel="external">Modeling Coverage for Neural Machine Translation</a></h2><p>ã€æœºå™¨ç¿»è¯‘ã€‘é’ˆå¯¹ç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰è¯‘æ–‡ä¸­ç»å¸¸å‡ºç°çš„é—æ¼ç¿»è¯‘ï¼ˆunder-translationï¼‰å’Œè¿‡åº¦ç¿»è¯‘ï¼ˆover-translationï¼‰é—®é¢˜ï¼Œåä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤é¦–æ¬¡æå‡ºå¯¹è¦†ç›–ç‡ï¼ˆcoverageï¼‰è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ–¹æ³•çš„ä¸»è¦æ€æƒ³æ˜¯ä¸ºæ¯ä¸ªæºç«¯è¯ç»´æŠ¤ä¸€ä¸ªcoverage vectorä»¥è¡¨ç¤ºè¯¥è¯è¢«ç¿»è¯‘ï¼ˆæˆ–è¦†ç›–ï¼‰çš„ç¨‹åº¦ã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­è¯¥è¦†ç›–ç‡ä¿¡æ¯ä¼šä¼ å…¥attention modelï¼Œä»¥ä½¿å®ƒæ›´å…³æ³¨äºæœªè¢«ç¿»è¯‘çš„æºç«¯è¯ï¼Œå®éªŒè¡¨ç¤ºè¯¥æ–¹æ³•èƒ½æ˜¾è‘—å‡å°‘é—æ¼ç¿»è¯‘å’Œè¿‡åº¦ç¿»è¯‘é”™è¯¯æ•°é‡ï¼Œè¯¥å·¥ä½œå‘è¡¨åœ¨ACL 2016ä¸Šã€‚</p>
<h2 id="Efficient-Summarization-with-Read-Again-and-Copy-Mechanism"><a href="#Efficient-Summarization-with-Read-Again-and-Copy-Mechanism" class="headerlink" title="Efficient Summarization with Read-Again and Copy Mechanism"></a><a href="https://arxiv.org/pdf/1611.03382.pdf" target="_blank" rel="external">Efficient Summarization with Read-Again and Copy Mechanism</a></h2><p>ã€æ–‡æœ¬æ‘˜è¦ã€‘æœ¬æ–‡é€‚åˆç ”ç©¶æ–‡æœ¬æ‘˜è¦ï¼Œå°¤å…¶æ˜¯ç”¨seq2seqæ¥è§£å†³å¥å­çº§æ‘˜è¦çš„ç«¥é‹è¿›è¡Œç ”è¯»ã€‚</p>
<h1 id="ä¸€å‘¨èµ„æº"><a href="#ä¸€å‘¨èµ„æº" class="headerlink" title="ä¸€å‘¨èµ„æº"></a>ä¸€å‘¨èµ„æº</h1><h2 id="åˆ˜çŸ¥è¿œè€å¸ˆåœ¨å°†é—¨çš„talk"><a href="#åˆ˜çŸ¥è¿œè€å¸ˆåœ¨å°†é—¨çš„talk" class="headerlink" title="åˆ˜çŸ¥è¿œè€å¸ˆåœ¨å°†é—¨çš„talk"></a><a href="http://nlp.csai.tsinghua.edu.cn/~lzy/index_cn.html" target="_blank" rel="external">åˆ˜çŸ¥è¿œè€å¸ˆåœ¨å°†é—¨çš„talk</a></h2><p>å¯¹â€œè¡¨ç¤ºå­¦ä¹ å’ŒçŸ¥è¯†è·å–â€æ„Ÿå…´è¶£çš„ç«¥é‹å¯ä»¥çœ‹è¿‡æ¥ã€‚</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;a href=&quot;#ä¸€å‘¨å€¼å¾—è¯»&quot; class=&quot;headerlink&quot; title=&quot;ä¸€å‘¨å€¼å¾—è¯»&quot;&gt;&lt;/a&gt;ä¸€å‘¨å€¼å¾—è¯»&lt;/h1&gt;&lt;h2 id=&quot;Learning-Recurrent-Span-Representations-for-Extractiv
    
    </summary>
    
    
      <category term="PaperWeekly" scheme="http://rsarxiv.github.io/tags/PaperWeekly/"/>
    
      <category term="nlp" scheme="http://rsarxiv.github.io/tags/nlp/"/>
    
  </entry>
  
</feed>
